[
  {
    "code": "def settext(self, text, cls='current'):\n        \"\"\"Set the text for this element.\n\n        Arguments:\n            text (str): The text\n            cls (str): The class of the text, defaults to ``current`` (leave this unless you know what you are doing). There may be only one text content element of each class associated with the element.\n        \"\"\"\n        self.replace(TextContent, value=text, cls=cls)",
    "reference": "Set the text for this element.\n\n        Arguments:\n            text (str): The text\n            cls (str): The class of the text, defaults to ``current`` (leave this unless you know what you are doing). There may be only one text content element of each class associated with the element.",
    "generated": "Set the text for this element.\n\n        Arguments:\n            text (str): The text\n            cls (str): The class of the text, defaults to ``current`` (leave this unless you know what you are doing). There may be only one text content element of each class associated with the element."
  },
  {
    "code": "def setdocument(self, doc):\n        \"\"\"Associate a document with this element.\n\n        Arguments:\n            doc (:class:`Document`): A document\n\n        Each element must be associated with a FoLiA document.\n        \"\"\"\n        assert isinstance(doc, Document)\n\n        if not self.doc:\n            self.doc = doc\n            if self.id:\n                if self.id in doc:\n                    raise DuplicateIDError(self.id)\n                else:\n                    self.doc.index[id] = self\n\n        for e in self: #recursive for all children\n            if isinstance(e,AbstractElement): e.setdocument(doc)",
    "reference": "Associate a document with this element.\n\n        Arguments:\n            doc (:class:`Document`): A document\n\n        Each element must be associated with a FoLiA document.",
    "generated": "Associate a document with this element.\n\n        Arguments:\n            doc (:class:`Document`): A document\n        Each element must be associated with a FoLiA document."
  },
  {
    "code": "def addable(Class, parent, set=None, raiseexceptions=True):\n        \"\"\"Tests whether a new element of this class can be added to the parent.\n\n        This method is mostly for internal use.\n        This will use the ``OCCURRENCES`` property, but may be overidden by subclasses for more customised behaviour.\n\n        Parameters:\n            parent (:class:`AbstractElement`): The element that is being added to\n            set (str or None): The set\n            raiseexceptions (bool): Raise an exception if the element can't be added?\n\n        Returns:\n            bool\n\n        Raises:\n            ValueError\n         \"\"\"\n\n\n        if not parent.__class__.accepts(Class, raiseexceptions, parent):\n            return False\n\n        if Class.OCCURRENCES > 0:\n            #check if the parent doesn't have too many already\n            count = parent.count(Class,None,True,[True, AbstractStructureElement]) #never descend into embedded structure annotatioton\n            if count >= Class.OCCURRENCES:\n                if raiseexceptions:\n                    if parent.id:\n                        extra = ' (id=' + parent.id + ')'\n                    else:\n                        extra = ''\n                    raise DuplicateAnnotationError(\"Unable to add another object of type \" + Class.__name__ + \" to \" + parent.__class__.__name__ + \" \" + extra + \". There are already \" + str(count) + \" instances of this class, which is the maximum.\")\n                else:\n                    return False\n\n        if Class.OCCURRENCES_PER_SET > 0 and set and Class.REQUIRED_ATTRIBS and Attrib.CLASS in Class.REQUIRED_ATTRIBS:\n            count = parent.count(Class,set,True, [True, AbstractStructureElement])\n            if count >= Class.OCCURRENCES_PER_SET:\n                if raiseexceptions:\n                    if parent.id:\n                        extra = ' (id=' + parent.id + ')'\n                    else:\n                        extra = ''\n                    raise DuplicateAnnotationError(\"Unable to add another object of set \" + set + \" and type \" + Class.__name__ + \" to \" + parent.__class__.__name__ + \" \" + extra + \". There are already \" + str(count) + \" instances of this class, which is the maximum for the set.\")\n                else:\n                    return False\n\n\n\n        return True",
    "reference": "Tests whether a new element of this class can be added to the parent.\n\n        This method is mostly for internal use.\n        This will use the ``OCCURRENCES`` property, but may be overidden by subclasses for more customised behaviour.\n\n        Parameters:\n            parent (:class:`AbstractElement`): The element that is being added to\n            set (str or None): The set\n            raiseexceptions (bool): Raise an exception if the element can't be added?\n\n        Returns:\n            bool\n\n        Raises:\n            ValueError",
    "generated": "Tests whether a new element of this class can be added to the parent.\n\n        This method is mostly for internal use.\n        This will use the ``OCCURRENCES`` property, but may be overidden by subclasses for more customised behaviour.\n\n        Returns:\n            bool\n\n        Raises:\n            ValueError\n         \"\"\"\n\n        if a new element of this class is being added to\n           "
  },
  {
    "code": "def postappend(self):\n        \"\"\"This method will be called after an element is added to another and does some checks.\n\n        It can do extra checks and if necessary raise exceptions to prevent addition. By default makes sure the right document is associated.\n\n        This method is mostly for internal use.\n        \"\"\"\n\n        #If the element was not associated with a document yet, do so now (and for all unassociated children:\n        if not self.doc and self.parent.doc:\n            self.setdocument(self.parent.doc)\n\n        if self.doc and self.doc.deepvalidation:\n            self.deepvalidation()",
    "reference": "This method will be called after an element is added to another and does some checks.\n\n        It can do extra checks and if necessary raise exceptions to prevent addition. By default makes sure the right document is associated.\n\n        This method is mostly for internal use.",
    "generated": "This method will be called after an element is added to another and does some checks.\n\n        It can do extra checks and if necessary raise exceptions to prevent addition. By default makes sure the right document is associated.\n\n        This method is mostly for internal use."
  },
  {
    "code": "def updatetext(self):\n        \"\"\"Recompute textual value based on the text content of the children. Only supported on elements that are a ``TEXTCONTAINER``\"\"\"\n        if self.TEXTCONTAINER:\n            s = \"\"\n            for child in self:\n                if isinstance(child, AbstractElement):\n                    child.updatetext()\n                    s += child.text()\n                elif isstring(child):\n                    s += child\n            self.data = [s]",
    "reference": "Recompute textual value based on the text content of the children. Only supported on elements that are a ``TEXTCONTAINER``",
    "generated": "Recompute textual value based on the text content of the children. Only supported on elements that are a ``TEXTCONTAINER``\"\"\"\n        if self.TEXTCONTAINER:"
  },
  {
    "code": "def ancestors(self, Class=None):\n        \"\"\"Generator yielding all ancestors of this element, effectively back-tracing its path to the root element. A tuple of multiple classes may be specified.\n\n        Arguments:\n            *Class: The class or classes (:class:`AbstractElement` or subclasses). Not instances!\n\n        Yields:\n            elements (instances derived from :class:`AbstractElement`)\n        \"\"\"\n        e = self\n        while e:\n            if e.parent:\n                e = e.parent\n                if not Class or isinstance(e,Class):\n                    yield e\n                elif isinstance(Class, tuple):\n                    for C in Class:\n                        if isinstance(e,C):\n                            yield e\n            else:\n                break",
    "reference": "Generator yielding all ancestors of this element, effectively back-tracing its path to the root element. A tuple of multiple classes may be specified.\n\n        Arguments:\n            *Class: The class or classes (:class:`AbstractElement` or subclasses). Not instances!\n\n        Yields:\n            elements (instances derived from :class:`AbstractElement`)",
    "generated": "Generator yielding all ancestors of this element, effectively back-tracing its path to the root element. A tuple of multiple classes may be specified.\n\n        Arguments:\n            *Class: The class or classes (:class:`AbstractElement` or subclasses). Not instances!\n\n        Yields:\n            elements (instances derived from :class:`AbstractElement`)"
  },
  {
    "code": "def ancestor(self, *Classes):\n        \"\"\"Find the most immediate ancestor of the specified type, multiple classes may be specified.\n\n        Arguments:\n            *Classes: The possible classes (:class:`AbstractElement` or subclasses) to select from. Not instances!\n\n        Example::\n\n            paragraph = word.ancestor(folia.Paragraph)\n        \"\"\"\n        for e in self.ancestors(tuple(Classes)):\n            return e\n        raise NoSuchAnnotation",
    "reference": "Find the most immediate ancestor of the specified type, multiple classes may be specified.\n\n        Arguments:\n            *Classes: The possible classes (:class:`AbstractElement` or subclasses) to select from. Not instances!\n\n        Example::\n\n            paragraph = word.ancestor(folia.Paragraph)",
    "generated": "Find the most immediate ancestor of the specified type, multiple classes may be specified."
  },
  {
    "code": "def json(self, attribs=None, recurse=True, ignorelist=False):\n        \"\"\"Serialises the FoLiA element and all its contents to a Python dictionary suitable for serialisation to JSON.\n\n        Example::\n\n            import json\n            json.dumps(word.json())\n\n        Returns:\n            dict\n        \"\"\"\n        jsonnode = {}\n\n        jsonnode['type'] = self.XMLTAG\n        if self.id:\n            jsonnode['id'] = self.id\n        if self.set:\n            jsonnode['set'] = self.set\n        if self.cls:\n            jsonnode['class'] = self.cls\n        if self.annotator:\n            jsonnode['annotator'] = self.annotator\n        if self.annotatortype:\n            if self.annotatortype == AnnotatorType.AUTO:\n                jsonnode['annotatortype'] = \"auto\"\n            elif self.annotatortype == AnnotatorType.MANUAL:\n                jsonnode['annotatortype'] = \"manual\"\n        if self.confidence is not None:\n            jsonnode['confidence'] = self.confidence\n        if self.n:\n            jsonnode['n'] = self.n\n        if self.auth:\n            jsonnode['auth'] = self.auth\n        if self.datetime:\n            jsonnode['datetime'] = self.datetime.strftime(\"%Y-%m-%dT%H:%M:%S\")\n\n        if recurse: #pylint: disable=too-many-nested-blocks\n            jsonnode['children'] = []\n            if self.TEXTCONTAINER:\n                jsonnode['text'] = self.text()\n            if self.PHONCONTAINER:\n                jsonnode['phon'] = self.phon()\n            for child in self:\n                if self.TEXTCONTAINER and isstring(child):\n                    jsonnode['children'].append(child)\n                elif not self.PHONCONTAINER:\n                    #check ignore list\n                    ignore = False\n                    if ignorelist:\n                        for e in ignorelist:\n                            if isinstance(child,e):\n                                ignore = True\n                                break\n                    if not ignore:\n                        jsonnode['children'].append(child.json(attribs,recurse,ignorelist))\n\n        if attribs:\n            for attrib in attribs:\n                jsonnode[attrib] = attribs\n\n        return jsonnode",
    "reference": "Serialises the FoLiA element and all its contents to a Python dictionary suitable for serialisation to JSON.\n\n        Example::\n\n            import json\n            json.dumps(word.json())\n\n        Returns:\n            dict",
    "generated": "Serialises the FoLiA element and all its contents to a Python dictionary suitable for serialisation to JSON.\n\n        Example::\n\n            import json\n            json.dumps(word.json())\n\n        Returns:\n            dict"
  },
  {
    "code": "def xmlstring(self, pretty_print=False):\n        \"\"\"Serialises this FoLiA element and all its contents to XML.\n\n        Returns:\n            str: a string with XML representation for this element and all its children\"\"\"\n        s = ElementTree.tostring(self.xml(), xml_declaration=False, pretty_print=pretty_print, encoding='utf-8')\n        if sys.version < '3':\n            if isinstance(s, str):\n                s = unicode(s,'utf-8') #pylint: disable=undefined-variable\n        else:\n            if isinstance(s,bytes):\n                s = str(s,'utf-8')\n\n        s = s.replace('ns0:','') #ugly patch to get rid of namespace prefix\n        s = s.replace(':ns0','')\n        return s",
    "reference": "Serialises this FoLiA element and all its contents to XML.\n\n        Returns:\n            str: a string with XML representation for this element and all its children",
    "generated": "Serialises this FoLiA element and all its contents to XML.\n\n        Returns:\n            str: a string with XML representation for this FoLiA element and all its children to XML.\n\n        Returns:\n            str: a string with XML representation for this element and all its children"
  },
  {
    "code": "def select(self, Class, set=None, recursive=True,  ignore=True, node=None): #pylint: disable=bad-classmethod-argument,redefined-builtin\n        \"\"\"Select child elements of the specified class.\n\n        A further restriction can be made based on set.\n\n        Arguments:\n            Class (class): The class to select; any python class (not instance) subclassed off :class:`AbstractElement`\n            Set (str): The set to match against, only elements pertaining to this set will be returned. If set to None (default), all elements regardless of set will be returned.\n            recursive (bool): Select recursively? Descending into child elements? Defaults to ``True``.\n            ignore: A list of Classes to ignore, if set to ``True`` instead of a list, all non-authoritative elements will be skipped (this is the default behaviour and corresponds to the following elements: :class:`Alternative`, :class:`AlternativeLayer`, :class:`Suggestion`, and :class:`folia.Original`. These elements and those contained within are never *authorative*. You may also include the boolean True as a member of a list, if you want to skip additional tags along the predefined non-authoritative ones.\n            * ``node``: Reserved for internal usage, used in recursion.\n\n        Yields:\n            Elements (instances derived from :class:`AbstractElement`)\n\n        Example::\n\n            for sense in text.select(folia.Sense, 'cornetto', True, [folia.Original, folia.Suggestion, folia.Alternative] ):\n                ..\n\n        \"\"\"\n\n        #if ignorelist is True:\n        #    ignorelist = default_ignore\n\n        if not node:\n            node = self\n        for e in self.data: #pylint: disable=too-many-nested-blocks\n            if (not self.TEXTCONTAINER and not self.PHONCONTAINER) or isinstance(e, AbstractElement):\n                if ignore is True:\n                    try:\n                        if not e.auth:\n                            continue\n                    except AttributeError:\n                        #not all elements have auth attribute..\n                        pass\n                elif ignore: #list\n                    doignore = False\n                    for c in ignore:\n                        if c is True:\n                            try:\n                                if not e.auth:\n                                    doignore =True\n                                    break\n                            except AttributeError:\n                                #not all elements have auth attribute..\n                                pass\n                        elif c == e.__class__ or issubclass(e.__class__,c):\n                            doignore = True\n                            break\n                    if doignore:\n                        continue\n\n                if isinstance(e, Class):\n                    if not set is None:\n                        try:\n                            if e.set != set:\n                                continue\n                        except AttributeError:\n                            continue\n                    yield e\n                if recursive:\n                    for e2 in e.select(Class, set, recursive, ignore, e):\n                        if not set is None:\n                            try:\n                                if e2.set != set:\n                                    continue\n                            except AttributeError:\n                                continue\n                        yield e2",
    "reference": "Select child elements of the specified class.\n\n        A further restriction can be made based on set.\n\n        Arguments:\n            Class (class): The class to select; any python class (not instance) subclassed off :class:`AbstractElement`\n            Set (str): The set to match against, only elements pertaining to this set will be returned. If set to None (default), all elements regardless of set will be returned.\n            recursive (bool): Select recursively? Descending into child elements? Defaults to ``True``.\n            ignore: A list of Classes to ignore, if set to ``True`` instead of a list, all non-authoritative elements will be skipped (this is the default behaviour and corresponds to the following elements: :class:`Alternative`, :class:`AlternativeLayer`, :class:`Suggestion`, and :class:`folia.Original`. These elements and those contained within are never *authorative*. You may also include the boolean True as a member of a list, if you want to skip additional tags along the predefined non-authoritative ones.\n            * ``node``: Reserved for internal usage, used in recursion.\n\n        Yields:\n            Elements (instances derived from :class:`AbstractElement`)\n\n        Example::\n\n            for sense in text.select(folia.Sense, 'cornetto', True, [folia.Original, folia.Suggestion, folia.Alternative] ):\n                ..",
    "generated": "Select child elements of the specified class.\n\n        A further restriction can be made based on set.\n\n        Arguments:\n            Class (class): The class to select; any python class (not instance) subclassed off :class:`AbstractElement`\n            Set (str): The set to match against, only elements pertaining to this set will be returned."
  },
  {
    "code": "def getmetadata(self, key=None):\n        \"\"\"Get the metadata that applies to this element, automatically inherited from parent elements\"\"\"\n        if self.metadata:\n            d =  self.doc.submetadata[self.metadata]\n        elif self.parent:\n            d =  self.parent.getmetadata()\n        elif self.doc:\n            d =  self.doc.metadata\n        else:\n            return None\n        if key:\n            return d[key]\n        else:\n            return d",
    "reference": "Get the metadata that applies to this element, automatically inherited from parent elements",
    "generated": "Get the metadata that applies to this element, automatically inherited from parent elements"
  },
  {
    "code": "def getindex(self, child, recursive=True, ignore=True):\n        \"\"\"Get the index at which an element occurs, recursive by default!\n\n        Returns:\n            int\n        \"\"\"\n\n        #breadth first search\n        for i, c in enumerate(self.data):\n            if c is child:\n                return i\n        if recursive:  #pylint: disable=too-many-nested-blocks\n            for i, c in enumerate(self.data):\n                if ignore is True:\n                    try:\n                        if not c.auth:\n                            continue\n                    except AttributeError:\n                        #not all elements have auth attribute..\n                        pass\n                elif ignore: #list\n                    doignore = False\n                    for e in ignore:\n                        if e is True:\n                            try:\n                                if not c.auth:\n                                    doignore =True\n                                    break\n                            except AttributeError:\n                                #not all elements have auth attribute..\n                                pass\n                        elif e == c.__class__ or issubclass(c.__class__,e):\n                            doignore = True\n                            break\n                    if doignore:\n                        continue\n                if isinstance(c, AbstractElement):\n                    j = c.getindex(child, recursive)\n                    if j != -1:\n                        return i #yes, i ... not j!\n        return -1",
    "reference": "Get the index at which an element occurs, recursive by default!\n\n        Returns:\n            int",
    "generated": "Get the index at which an element occurs, recursive by default!\n\n        Returns:\n            int\n       "
  },
  {
    "code": "def precedes(self, other):\n        \"\"\"Returns a boolean indicating whether this element precedes the other element\"\"\"\n        try:\n            ancestor = next(commonancestors(AbstractElement, self, other))\n        except StopIteration:\n            raise Exception(\"Elements share no common ancestor\")\n        #now we just do a depth first search and see who comes first\n        def callback(e):\n            if e is self:\n                return True\n            elif e is other:\n                return False\n            return None\n        result = ancestor.depthfirstsearch(callback)\n        if result is None:\n            raise Exception(\"Unable to find relation between elements! (shouldn't happen)\")\n        return result",
    "reference": "Returns a boolean indicating whether this element precedes the other element",
    "generated": "Returns a boolean indicating whether this element precedes the other element"
  },
  {
    "code": "def depthfirstsearch(self, function):\n        \"\"\"Generic depth first search algorithm using a callback function, continues as long as the callback function returns None\"\"\"\n        result = function(self)\n        if result is not None:\n            return result\n        for e in self:\n            result = e.depthfirstsearch(function)\n            if result is not None:\n                return result\n        return None",
    "reference": "Generic depth first search algorithm using a callback function, continues as long as the callback function returns None",
    "generated": "Generic depth first search algorithm using a callback function, continues as long as the callback function returns None"
  },
  {
    "code": "def next(self, Class=True, scope=True, reverse=False):\n        \"\"\"Returns the next element, if it is of the specified type and if it does not cross the boundary of the defined scope. Returns None if no next element is found. Non-authoritative elements are never returned.\n\n        Arguments:\n            * ``Class``: The class to select; any python class subclassed off `'AbstractElement``, may also be a tuple of multiple classes. Set to ``True`` to constrain to the same class as that of the current instance, set to ``None`` to not constrain at all\n            * ``scope``: A list of classes which are never crossed looking for a next element. Set to ``True`` to constrain to a default list of structure elements (Sentence,Paragraph,Division,Event, ListItem,Caption), set to ``None`` to not constrain at all.\n\n        \"\"\"\n        if Class is True: Class = self.__class__\n        if scope is True: scope = STRUCTURESCOPE\n\n        structural = Class is not None and issubclass(Class,AbstractStructureElement)\n\n        if reverse:\n            order = reversed\n            descendindex = -1\n        else:\n            order = lambda x: x #pylint: disable=redefined-variable-type\n            descendindex = 0\n\n        child = self\n        parent = self.parent\n        while parent: #pylint: disable=too-many-nested-blocks\n            if len(parent) > 1:\n                returnnext = False\n                for e in order(parent):\n                    if e is child:\n                        #we found the current item, next item will be the one to return\n                        returnnext = True\n                    elif returnnext and e.auth and not isinstance(e,AbstractAnnotationLayer) and (not structural or (structural and (not isinstance(e,(AbstractTokenAnnotation,TextContent)) ) )):\n                        if structural and isinstance(e,Correction):\n                            if not list(e.select(AbstractStructureElement)): #skip-over non-structural correction\n                                continue\n\n                        if Class is None or (isinstance(Class,tuple) and (any(isinstance(e,C) for C in Class))) or isinstance(e,Class):\n                            return e\n                        else:\n                            #this is not yet the element of the type we are looking for, we are going to descend again in the very leftmost (rightmost if reversed) branch only\n                            while e.data:\n                                e = e.data[descendindex]\n                                if not isinstance(e, AbstractElement):\n                                    return None #we've gone too far\n                                if e.auth and not isinstance(e,AbstractAnnotationLayer):\n                                    if Class is None or (isinstance(Class,tuple) and (any(isinstance(e,C) for C in Class))) or isinstance(e,Class):\n                                        return e\n                                    else:\n                                        #descend deeper\n                                        continue\n                        return None\n\n            #generational iteration\n            child = parent\n            if scope is not None and child.__class__ in scope:\n                #you shall not pass!\n                break\n            parent = parent.parent\n\n        return None",
    "reference": "Returns the next element, if it is of the specified type and if it does not cross the boundary of the defined scope. Returns None if no next element is found. Non-authoritative elements are never returned.\n\n        Arguments:\n            * ``Class``: The class to select; any python class subclassed off `'AbstractElement``, may also be a tuple of multiple classes. Set to ``True`` to constrain to the same class as that of the current instance, set to ``None`` to not constrain at all\n            * ``scope``: A list of classes which are never crossed looking for a next element. Set to ``True`` to constrain to a default list of structure elements (Sentence,Paragraph,Division,Event, ListItem,Caption), set to ``None`` to not constrain at all.",
    "generated": "Returns the next element, if it is of the specified type and if it does not cross the boundary of the defined scope. Returns None if no next element is found. Non-authoritative elements are never returned."
  },
  {
    "code": "def previous(self, Class=True, scope=True):\n        \"\"\"Returns the previous element, if it is of the specified type and if it does not cross the boundary of the defined scope. Returns None if no next element is found. Non-authoritative elements are never returned.\n\n        Arguments:\n            * ``Class``: The class to select; any python class subclassed off `'AbstractElement``. Set to ``True`` to constrain to the same class as that of the current instance, set to ``None`` to not constrain at all\n            * ``scope``: A list of classes which are never crossed looking for a next element. Set to ``True`` to constrain to a default list of structure elements (Sentence,Paragraph,Division,Event, ListItem,Caption), set to ``None`` to not constrain at all.\n\n        \"\"\"\n        return self.next(Class,scope, True)",
    "reference": "Returns the previous element, if it is of the specified type and if it does not cross the boundary of the defined scope. Returns None if no next element is found. Non-authoritative elements are never returned.\n\n        Arguments:\n            * ``Class``: The class to select; any python class subclassed off `'AbstractElement``. Set to ``True`` to constrain to the same class as that of the current instance, set to ``None`` to not constrain at all\n            * ``scope``: A list of classes which are never crossed looking for a next element. Set to ``True`` to constrain to a default list of structure elements (Sentence,Paragraph,Division,Event, ListItem,Caption), set to ``None`` to not constrain at all.",
    "generated": "Returns the previous element, if it is of the specified type and if it does not cross the boundary of the defined scope. Returns None if no next element is found. Non-authoritative elements are never returned."
  },
  {
    "code": "def remove(self, child):\n        \"\"\"Removes the child element\"\"\"\n        if not isinstance(child, AbstractElement):\n            raise ValueError(\"Expected AbstractElement, got \" + str(type(child)))\n        if child.parent == self:\n            child.parent = None\n        self.data.remove(child)\n        #delete from index\n        if child.id and self.doc and child.id in self.doc.index:\n            del self.doc.index[child.id]",
    "reference": "Removes the child element",
    "generated": "Removes the child element."
  },
  {
    "code": "def hasannotation(self,Class,set=None):\n        \"\"\"Returns an integer indicating whether such as annotation exists, and if so, how many.\n\n        See :meth:`AllowTokenAnnotation.annotations`` for a description of the parameters.\"\"\"\n        return sum( 1 for _ in self.select(Class,set,True,default_ignore_annotations))",
    "reference": "Returns an integer indicating whether such as annotation exists, and if so, how many.\n\n        See :meth:`AllowTokenAnnotation.annotations`` for a description of the parameters.",
    "generated": "Returns an integer indicating whether such as annotation exists, and if so, how many.\n\n        See :meth:`AllowTokenAnnotation.annotations`` for a description of the parameters."
  },
  {
    "code": "def annotation(self, type, set=None):\n        \"\"\"Obtain a single annotation element.\n\n        A further restriction can be made based on set.\n\n        Arguments:\n            Class (class): The class to select; any python class (not instance) subclassed off :class:`AbstractElement`\n            Set (str): The set to match against, only elements pertaining to this set will be returned. If set to None (default), all elements regardless of set will be returned.\n\n        Returns:\n            An element (instance derived from :class:`AbstractElement`)\n\n        Example::\n\n            sense = word.annotation(folia.Sense, 'http://some/path/cornetto').cls\n\n        See also:\n            :meth:`AllowTokenAnnotation.annotations`\n            :meth:`AbstractElement.select`\n\n        Raises:\n            :class:`NoSuchAnnotation` if no such annotation exists\n        \"\"\"\n        \"\"\"Will return a **single** annotation (even if there are multiple). Raises a ``NoSuchAnnotation`` exception if none was found\"\"\"\n        for e in self.select(type,set,True,default_ignore_annotations):\n            return e\n        raise NoSuchAnnotation()",
    "reference": "Obtain a single annotation element.\n\n        A further restriction can be made based on set.\n\n        Arguments:\n            Class (class): The class to select; any python class (not instance) subclassed off :class:`AbstractElement`\n            Set (str): The set to match against, only elements pertaining to this set will be returned. If set to None (default), all elements regardless of set will be returned.\n\n        Returns:\n            An element (instance derived from :class:`AbstractElement`)\n\n        Example::\n\n            sense = word.annotation(folia.Sense, 'http://some/path/cornetto').cls\n\n        See also:\n            :meth:`AllowTokenAnnotation.annotations`\n            :meth:`AbstractElement.select`\n\n        Raises:\n            :class:`NoSuchAnnotation` if no such annotation exists",
    "generated": "Obtain a single annotation element.\n\n        A further restriction can be made based on set.\n\n        Arguments:\n            Class (class): The class to select; any python class (not instance), only elements pertaining to this set will be returned. If set to None (default), all elements regardless of set will be returned."
  },
  {
    "code": "def hasannotationlayer(self, annotationtype=None,set=None):\n        \"\"\"Does the specified annotation layer exist?\"\"\"\n        l = self.layers(annotationtype, set)\n        return (len(l) > 0)",
    "reference": "Does the specified annotation layer exist?",
    "generated": "Does the specified annotation layer exist?"
  },
  {
    "code": "def getreference(self, validate=True):\n        \"\"\"Returns and validates the Text Content's reference. Raises UnresolvableTextContent when invalid\"\"\"\n\n        if self.offset is None: return None #nothing to test\n        if self.ref:\n            ref = self.doc[self.ref]\n        else:\n            ref = self.finddefaultreference()\n\n        if not ref:\n            raise UnresolvableTextContent(\"Default reference for textcontent not found!\")\n        elif not ref.hastext(self.cls):\n            raise UnresolvableTextContent(\"Reference (ID \" + str(ref.id) + \") has no such text (class=\" + self.cls+\")\")\n        elif validate and self.text() != ref.textcontent(self.cls).text()[self.offset:self.offset+len(self.data[0])]:\n            raise UnresolvableTextContent(\"Reference (ID \" + str(ref.id) + \", class=\" + self.cls+\") found but no text match at specified offset (\"+str(self.offset)+\")! Expected '\" + self.text() + \"', got '\" + ref.textcontent(self.cls).text()[self.offset:self.offset+len(self.data[0])] +\"'\")\n        else:\n            #finally, we made it!\n            return ref",
    "reference": "Returns and validates the Text Content's reference. Raises UnresolvableTextContent when invalid",
    "generated": "Returns and validates the Text Content's reference. Raises UnresolvableTextContent when invalid. Raises UnresolvableTextContent when invalid"
  },
  {
    "code": "def getreference(self, validate=True):\n        \"\"\"Return and validate the Phonetic Content's reference. Raises UnresolvableTextContent when invalid\"\"\"\n\n        if self.offset is None: return None #nothing to test\n        if self.ref:\n            ref = self.doc[self.ref]\n        else:\n            ref = self.finddefaultreference()\n\n        if not ref:\n            raise UnresolvableTextContent(\"Default reference for phonetic content not found!\")\n        elif not ref.hasphon(self.cls):\n            raise UnresolvableTextContent(\"Reference has no such phonetic content (class=\" + self.cls+\")\")\n        elif validate and self.phon() != ref.textcontent(self.cls).phon()[self.offset:self.offset+len(self.data[0])]:\n            raise UnresolvableTextContent(\"Reference (class=\" + self.cls+\") found but no phonetic match at specified offset (\"+str(self.offset)+\")! Expected '\" + self.text() + \"', got '\" + ref.textcontent(self.cls).text()[self.offset:self.offset+len(self.data[0])] +\"'\")\n        else:\n            #finally, we made it!\n            return ref",
    "reference": "Return and validate the Phonetic Content's reference. Raises UnresolvableTextContent when invalid",
    "generated": "Return and validate the Phonetic Content's reference. Raises UnresolvableTextContent when invalid"
  },
  {
    "code": "def findspans(self, type,set=None):\n        \"\"\"Yields span annotation elements of the specified type that include this word.\n\n        Arguments:\n            type: The annotation type, can be passed as using any of the :class:`AnnotationType` member, or by passing the relevant :class:`AbstractSpanAnnotation` or :class:`AbstractAnnotationLayer` class.\n            set (str or None): Constrain by set\n\n        Example::\n\n            for chunk in word.findspans(folia.Chunk):\n                print(\" Chunk class=\", chunk.cls, \" words=\")\n                for word2 in chunk.wrefs(): #print all words in the chunk (of which the word is a part)\n                    print(word2, end=\"\")\n                print()\n\n        Yields:\n            Matching span annotation instances (derived from :class:`AbstractSpanAnnotation`)\n        \"\"\"\n\n        if issubclass(type, AbstractAnnotationLayer):\n            layerclass = type\n        else:\n            layerclass = ANNOTATIONTYPE2LAYERCLASS[type.ANNOTATIONTYPE]\n        e = self\n        while True:\n            if not e.parent: break\n            e = e.parent\n            for layer in e.select(layerclass,set,False):\n                if type is layerclass:\n                    for e2 in layer.select(AbstractSpanAnnotation,set,True, (True, Word, Morpheme)):\n                        if not isinstance(e2, AbstractSpanRole) and self in e2.wrefs():\n                            yield e2\n                else:\n                    for e2 in layer.select(type,set,True, (True, Word, Morpheme)):\n                        if not isinstance(e2, AbstractSpanRole) and self in e2.wrefs():\n                            yield e2",
    "reference": "Yields span annotation elements of the specified type that include this word.\n\n        Arguments:\n            type: The annotation type, can be passed as using any of the :class:`AnnotationType` member, or by passing the relevant :class:`AbstractSpanAnnotation` or :class:`AbstractAnnotationLayer` class.\n            set (str or None): Constrain by set\n\n        Example::\n\n            for chunk in word.findspans(folia.Chunk):\n                print(\" Chunk class=\", chunk.cls, \" words=\")\n                for word2 in chunk.wrefs(): #print all words in the chunk (of which the word is a part)\n                    print(word2, end=\"\")\n                print()\n\n        Yields:\n            Matching span annotation instances (derived from :class:`AbstractSpanAnnotation`)",
    "generated": "Yields span annotation elements of the specified type that include this word.\n\n        Arguments:\n            type: The annotation type that include this word.\n\n        Arguments:\n            type: The annotation type that include this word.\n\n        Arguments:\n            type: The annotation type that include this word.\n\n        Arguments:\n            type: The annotation type that include this word.\n\n        Arguments:\n            type: The annotation type, can be passed as using any of the :class:`AnnotationType` member, or by passing the relevant :class:`AnnotationType` member, or by"
  },
  {
    "code": "def setspan(self, *args):\n        \"\"\"Sets the span of the span element anew, erases all data inside.\n\n        Arguments:\n            *args: Instances of :class:`Word`, :class:`Morpheme` or :class:`Phoneme`\n        \"\"\"\n        self.data = []\n        for child in args:\n            self.append(child)",
    "reference": "Sets the span of the span element anew, erases all data inside.\n\n        Arguments:\n            *args: Instances of :class:`Word`, :class:`Morpheme` or :class:`Phoneme`",
    "generated": "Sets the span of the span element anew, erases all data inside.\n\n        Arguments:\n            *args: Instances of :class:`Word`, :class:`Morpheme` or :class:`Phoneme`"
  },
  {
    "code": "def _helper_wrefs(self, targets, recurse=True):\n        \"\"\"Internal helper function\"\"\"\n        for c in self:\n            if isinstance(c,Word) or isinstance(c,Morpheme) or isinstance(c, Phoneme):\n                targets.append(c)\n            elif isinstance(c,WordReference):\n                try:\n                    targets.append(self.doc[c.id]) #try to resolve\n                except KeyError:\n                    targets.append(c) #add unresolved\n            elif isinstance(c, AbstractSpanAnnotation) and recurse:\n                #recursion\n                c._helper_wrefs(targets) #pylint: disable=protected-access\n            elif isinstance(c, Correction) and c.auth: #recurse into corrections\n                for e in c:\n                    if isinstance(e, AbstractCorrectionChild) and e.auth:\n                        for e2 in e:\n                            if isinstance(e2, AbstractSpanAnnotation):\n                                #recursion\n                                e2._helper_wrefs(targets)",
    "reference": "Internal helper function",
    "generated": "Internal helper function."
  },
  {
    "code": "def wrefs(self, index = None, recurse=True):\n        \"\"\"Returns a list of word references, these can be Words but also Morphemes or Phonemes.\n\n        Arguments:\n            index (int or None): If set to an integer, will retrieve and return the n'th element (starting at 0) instead of returning the list of all\n        \"\"\"\n        targets =[]\n        self._helper_wrefs(targets, recurse)\n        if index is None:\n            return targets\n        else:\n            return targets[index]",
    "reference": "Returns a list of word references, these can be Words but also Morphemes or Phonemes.\n\n        Arguments:\n            index (int or None): If set to an integer, will retrieve and return the n'th element (starting at 0) instead of returning the list of all",
    "generated": "Returns a list of word references, these can be Words but also Morphemes or Phonemes.\n\n        Arguments:\n            index (int or None): If set to an integer, will retrieve and return the n'th element (starting at 0) instead of returning the list of all"
  },
  {
    "code": "def copychildren(self, newdoc=None, idsuffix=\"\"):\n        \"\"\"Generator creating a deep copy of the children of this element. If idsuffix is a string, if set to True, a random idsuffix will be generated including a random 32-bit hash\"\"\"\n        if idsuffix is True: idsuffix = \".copy.\" + \"%08x\" % random.getrandbits(32) #random 32-bit hash for each copy, same one will be reused for all children\n        for c in self:\n            if isinstance(c, Word):\n                yield WordReference(newdoc, id=c.id)\n            else:\n                yield c.copy(newdoc,idsuffix)",
    "reference": "Generator creating a deep copy of the children of this element. If idsuffix is a string, if set to True, a random idsuffix will be generated including a random 32-bit hash",
    "generated": "Generator creating a deep copy of the children of this element. If idsuffix is a string, if set to True, a random idsuffix will be generated including a random 32-bit hash\"\"\"\n        if idsuffix is True, a random idsuffix will be generated including a random 32-bit hash\"\"\"\n        if idsuffix is True, a random idsuffix will be generated including a random 32-bit hash\"\"\""
  },
  {
    "code": "def alternatives(self, Class=None, set=None):\n        \"\"\"Generator over alternatives, either all or only of a specific annotation type, and possibly restrained also by set.\n\n        Arguments:\n            * ``Class`` - The Class you want to retrieve (e.g. PosAnnotation). Or set to None to select all alternatives regardless of what type they are.\n            * ``set``   - The set you want to retrieve (defaults to None, which selects irregardless of set)\n\n        Returns:\n            Generator over Alternative elements\n        \"\"\"\n\n        for e in self.select(AlternativeLayers,None, True, ['Original','Suggestion']): #pylint: disable=too-many-nested-blocks\n            if Class is None:\n                yield e\n            elif len(e) >= 1: #child elements?\n                for e2 in e:\n                    try:\n                        if isinstance(e2, Class):\n                            try:\n                                if set is None or e2.set == set:\n                                    yield e #not e2\n                                    break #yield an alternative only once (in case there are multiple matches)\n                            except AttributeError:\n                                continue\n                    except AttributeError:\n                        continue",
    "reference": "Generator over alternatives, either all or only of a specific annotation type, and possibly restrained also by set.\n\n        Arguments:\n            * ``Class`` - The Class you want to retrieve (e.g. PosAnnotation). Or set to None to select all alternatives regardless of what type they are.\n            * ``set``   - The set you want to retrieve (defaults to None, which selects irregardless of set)\n\n        Returns:\n            Generator over Alternative elements",
    "generated": "Generator over alternatives, either all or only of a specific annotation type, and possibly restrained also by set.\n\n        Returns:\n            Generator over Alternative elements\n       "
  },
  {
    "code": "def findspan(self, *words):\n        \"\"\"Returns the span element which spans over the specified words or morphemes.\n\n        See also:\n            :meth:`Word.findspans`\n        \"\"\"\n\n        for span in self.select(AbstractSpanAnnotation,None,True):\n            if tuple(span.wrefs()) == words:\n                return span\n        raise NoSuchAnnotation",
    "reference": "Returns the span element which spans over the specified words or morphemes.\n\n        See also:\n            :meth:`Word.findspans`",
    "generated": "Returns the span element which spans over the specified words or morphemes.\n\n        See also:\n            :meth:`Word.findspans`"
  },
  {
    "code": "def hasnew(self,allowempty=False):\n        \"\"\"Does the correction define new corrected annotations?\"\"\"\n        for e in  self.select(New,None,False, False):\n            if not allowempty and len(e) == 0: continue\n            return True\n        return False",
    "reference": "Does the correction define new corrected annotations?",
    "generated": "Does the correction define new corrected annotations?"
  },
  {
    "code": "def hasoriginal(self,allowempty=False):\n        \"\"\"Does the correction record the old annotations prior to correction?\"\"\"\n        for e in self.select(Original,None,False, False):\n            if not allowempty and len(e) == 0: continue\n            return True\n        return False",
    "reference": "Does the correction record the old annotations prior to correction?",
    "generated": "Does the correction record the old annotations prior to correction?"
  },
  {
    "code": "def hassuggestions(self,allowempty=False):\n        \"\"\"Does the correction propose suggestions for correction?\"\"\"\n        for e in self.select(Suggestion,None,False, False):\n            if not allowempty and len(e) == 0: continue\n            return True\n        return False",
    "reference": "Does the correction propose suggestions for correction?",
    "generated": "Does the correction propose suggestions for correction?"
  },
  {
    "code": "def new(self,index = None):\n        \"\"\"Get the new corrected annotation.\n\n        This returns only one annotation if multiple exist, use `index` to select another in the sequence.\n\n        Returns:\n            an annotation element (:class:`AbstractElement`)\n\n        Raises:\n            :class:`NoSuchAnnotation`\n        \"\"\"\n\n        if index is None:\n            try:\n                return next(self.select(New,None,False))\n            except StopIteration:\n                raise NoSuchAnnotation\n        else:\n            for e in self.select(New,None,False):\n                return e[index]\n            raise NoSuchAnnotation",
    "reference": "Get the new corrected annotation.\n\n        This returns only one annotation if multiple exist, use `index` to select another in the sequence.\n\n        Returns:\n            an annotation element (:class:`AbstractElement`)\n\n        Raises:\n            :class:`NoSuchAnnotation`",
    "generated": "Get the new corrected annotation.\n\n        This returns only one annotation if multiple exist, use `index` to select another in the sequence.\n\n        Raises:\n            :class:`NoSuchAnnotation`"
  },
  {
    "code": "def original(self,index=None):\n        \"\"\"Get the old annotation prior to correction.\n\n        This returns only one annotation if multiple exist, use `index` to select another in the sequence.\n\n        Returns:\n            an annotation element (:class:`AbstractElement`)\n\n        Raises:\n            :class:`NoSuchAnnotation`\n        \"\"\"\n        if index is None:\n            try:\n                return next(self.select(Original,None,False, False))\n            except StopIteration:\n                raise NoSuchAnnotation\n        else:\n            for e in self.select(Original,None,False, False):\n                return e[index]\n            raise NoSuchAnnotation",
    "reference": "Get the old annotation prior to correction.\n\n        This returns only one annotation if multiple exist, use `index` to select another in the sequence.\n\n        Returns:\n            an annotation element (:class:`AbstractElement`)\n\n        Raises:\n            :class:`NoSuchAnnotation`",
    "generated": "Get the old annotation prior to correction.\n\n        This returns only one annotation if multiple exist, use `index` to select another in the sequence.\n\n        Raises:\n            :class:`NoSuchAnnotation`"
  },
  {
    "code": "def suggestions(self,index=None):\n        \"\"\"Get suggestions for correction.\n\n        Yields:\n            :class:`Suggestion` element that encapsulate the suggested annotations (if index is ``None``, default)\n\n        Returns:\n            a :class:`Suggestion` element that encapsulate the suggested annotations (if index is set)\n\n        Raises:\n            :class:`IndexError`\n        \"\"\"\n        if index is None:\n            return self.select(Suggestion,None,False, False)\n        else:\n            for i, e in enumerate(self.select(Suggestion,None,False, False)):\n                if index == i:\n                    return e\n            raise IndexError",
    "reference": "Get suggestions for correction.\n\n        Yields:\n            :class:`Suggestion` element that encapsulate the suggested annotations (if index is ``None``, default)\n\n        Returns:\n            a :class:`Suggestion` element that encapsulate the suggested annotations (if index is set)\n\n        Raises:\n            :class:`IndexError`",
    "generated": "Get suggestions for correction.\n\n        Yields:\n            :class:`Suggestion` element that encapsulate the suggested annotations (if index is ``None``, default)\n\n        Raises:\n            :class:`IndexError`"
  },
  {
    "code": "def findspans(self, type,set=None):\n        \"\"\"Find span annotation of the specified type that include this word\"\"\"\n        if issubclass(type, AbstractAnnotationLayer):\n            layerclass = type\n        else:\n            layerclass = ANNOTATIONTYPE2LAYERCLASS[type.ANNOTATIONTYPE]\n        e = self\n        while True:\n            if not e.parent: break\n            e = e.parent\n            for layer in e.select(layerclass,set,False):\n                for e2 in layer:\n                    if isinstance(e2, AbstractSpanAnnotation):\n                        if self in e2.wrefs():\n                            yield e2",
    "reference": "Find span annotation of the specified type that include this word",
    "generated": "Find span annotation of the specified type that include this word"
  },
  {
    "code": "def resolve(self,size, distribution):\n        \"\"\"Resolve a variable sized pattern to all patterns of a certain fixed size\"\"\"\n        if not self.variablesize():\n            raise Exception(\"Can only resize patterns with * wildcards\")\n\n        nrofwildcards = 0\n        for x in self.sequence:\n            if x == '*':\n                nrofwildcards += 1\n\n        assert (len(distribution) == nrofwildcards)\n\n        wildcardnr = 0\n        newsequence = []\n        for x in self.sequence:\n            if x == '*':\n                newsequence += [True] * distribution[wildcardnr]\n                wildcardnr += 1\n            else:\n                newsequence.append(x)\n        d = { 'matchannotation':self.matchannotation, 'matchannotationset':self.matchannotationset, 'casesensitive':self.casesensitive }\n        yield Pattern(*newsequence, **d )",
    "reference": "Resolve a variable sized pattern to all patterns of a certain fixed size",
    "generated": "Resolve a variable sized pattern to all patterns of a certain fixed size"
  },
  {
    "code": "def load(self, filename):\n        \"\"\"Load a FoLiA XML file.\n\n        Argument:\n            filename (str): The file to load\n        \"\"\"\n        #if LXE and self.mode != Mode.XPATH:\n        #    #workaround for xml:id problem (disabled)\n        #    #f = open(filename)\n        #    #s = f.read().replace(' xml:id=', ' id=')\n        #    #f.close()\n        #    self.tree = ElementTree.parse(filename)\n        #else:\n        self.tree = xmltreefromfile(filename)\n        self.parsexml(self.tree.getroot())\n        if self.mode != Mode.XPATH:\n            #XML Tree is now obsolete (only needed when partially loaded for xpath queries)\n            self.tree = None",
    "reference": "Load a FoLiA XML file.\n\n        Argument:\n            filename (str): The file to load",
    "generated": "Load a FoLiA XML file.\n\n        Argument:\n            filename (str): The file to load\n       "
  },
  {
    "code": "def items(self):\n        \"\"\"Returns a depth-first flat list of all items in the document\"\"\"\n        l = []\n        for e in self.data:\n            l += e.items()\n        return l",
    "reference": "Returns a depth-first flat list of all items in the document",
    "generated": "Returns an array of all items in the document"
  },
  {
    "code": "def save(self, filename=None):\n        \"\"\"Save the document to file.\n\n        Arguments:\n            * filename (str): The filename to save to. If not set (``None``, default), saves to the same file as loaded from.\n        \"\"\"\n        if not filename:\n            filename = self.filename\n        if not filename:\n            raise Exception(\"No filename specified\")\n        if filename[-4:].lower() == '.bz2':\n            f = bz2.BZ2File(filename,'wb')\n            f.write(self.xmlstring().encode('utf-8'))\n            f.close()\n        elif filename[-3:].lower() == '.gz':\n            f = gzip.GzipFile(filename,'wb') #pylint: disable=redefined-variable-type\n            f.write(self.xmlstring().encode('utf-8'))\n            f.close()\n        else:\n            f = io.open(filename,'w',encoding='utf-8')\n            f.write(self.xmlstring())\n            f.close()",
    "reference": "Save the document to file.\n\n        Arguments:\n            * filename (str): The filename to save to. If not set (``None``, default), saves to the same file as loaded from.",
    "generated": "Save the document to file.\n\n        Arguments:\n            * filename (str): The filename to save to. If not set (``None``, default), saves to the same file as loaded from."
  },
  {
    "code": "def xmldeclarations(self):\n        \"\"\"Internal method to generate XML nodes for all declarations\"\"\"\n        l = []\n        E = ElementMaker(namespace=\"http://ilk.uvt.nl/folia\",nsmap={None: \"http://ilk.uvt.nl/folia\", 'xml' : \"http://www.w3.org/XML/1998/namespace\"})\n\n        for annotationtype, set in self.annotations:\n            label = None\n            #Find the 'label' for the declarations dynamically (aka: AnnotationType --> String)\n            for key, value in vars(AnnotationType).items():\n                if value == annotationtype:\n                    label = key\n                    break\n            #gather attribs\n\n            if (annotationtype == AnnotationType.TEXT or annotationtype == AnnotationType.PHON) and set == 'undefined' and len(self.annotationdefaults[annotationtype][set]) == 0:\n                #this is the implicit TextContent declaration, no need to output it explicitly\n                continue\n\n            attribs = {}\n            if set and set != 'undefined':\n                attribs['{' + NSFOLIA + '}set'] = set\n\n\n            for key, value in self.annotationdefaults[annotationtype][set].items():\n                if key == 'annotatortype':\n                    if value == AnnotatorType.MANUAL:\n                        attribs['{' + NSFOLIA + '}' + key] = 'manual'\n                    elif value == AnnotatorType.AUTO:\n                        attribs['{' + NSFOLIA + '}' + key] = 'auto'\n                elif key == 'datetime':\n                    attribs['{' + NSFOLIA + '}' + key] = value.strftime(\"%Y-%m-%dT%H:%M:%S\") #proper iso-formatting\n                elif value:\n                    attribs['{' + NSFOLIA + '}' + key] = value\n            if label:\n                l.append( makeelement(E,'{' + NSFOLIA + '}' + label.lower() + '-annotation', **attribs) )\n            else:\n                raise Exception(\"Invalid annotation type\")\n        return l",
    "reference": "Internal method to generate XML nodes for all declarations",
    "generated": "Internal method to generate XML nodes for all declarations."
  },
  {
    "code": "def jsondeclarations(self):\n        \"\"\"Return all declarations in a form ready to be serialised to JSON.\n\n        Returns:\n            list of dict\n        \"\"\"\n        l = []\n        for annotationtype, set in self.annotations:\n            label = None\n            #Find the 'label' for the declarations dynamically (aka: AnnotationType --> String)\n            for key, value in vars(AnnotationType).items():\n                if value == annotationtype:\n                    label = key\n                    break\n            #gather attribs\n\n            if (annotationtype == AnnotationType.TEXT or annotationtype == AnnotationType.PHON) and set == 'undefined' and len(self.annotationdefaults[annotationtype][set]) == 0:\n                #this is the implicit TextContent declaration, no need to output it explicitly\n                continue\n\n            jsonnode = {'annotationtype': label.lower()}\n            if set and set != 'undefined':\n                jsonnode['set'] = set\n\n\n            for key, value in self.annotationdefaults[annotationtype][set].items():\n                if key == 'annotatortype':\n                    if value == AnnotatorType.MANUAL:\n                        jsonnode[key] = 'manual'\n                    elif value == AnnotatorType.AUTO:\n                        jsonnode[key] = 'auto'\n                elif key == 'datetime':\n                    jsonnode[key] = value.strftime(\"%Y-%m-%dT%H:%M:%S\") #proper iso-formatting\n                elif value:\n                    jsonnode[key] = value\n            if label:\n                l.append( jsonnode  )\n            else:\n                raise Exception(\"Invalid annotation type\")\n        return l",
    "reference": "Return all declarations in a form ready to be serialised to JSON.\n\n        Returns:\n            list of dict",
    "generated": "Return all declarations in a form ready to be serialised to JSON.\n\n        Returns:\n            list of dict"
  },
  {
    "code": "def xml(self):\n        \"\"\"Serialise the document to XML.\n\n        Returns:\n            lxml.etree.Element\n\n        See also:\n            :meth:`Document.xmlstring`\n        \"\"\"\n\n        self.pendingvalidation()\n\n        E = ElementMaker(namespace=\"http://ilk.uvt.nl/folia\",nsmap={'xml' : \"http://www.w3.org/XML/1998/namespace\", 'xlink':\"http://www.w3.org/1999/xlink\"})\n        attribs = {}\n        attribs['{http://www.w3.org/XML/1998/namespace}id'] = self.id\n\n        #if self.version:\n        #    attribs['version'] = self.version\n        #else:\n        attribs['version'] = FOLIAVERSION\n\n        attribs['generator'] = 'pynlpl.formats.folia-v' + LIBVERSION\n\n        metadataattribs = {}\n        metadataattribs['{' + NSFOLIA + '}type'] = self.metadatatype\n\n        if isinstance(self.metadata, ExternalMetaData):\n            metadataattribs['{' + NSFOLIA + '}src'] = self.metadata.url\n\n        e = E.FoLiA(\n            E.metadata(\n                E.annotations(\n                    *self.xmldeclarations()\n                ),\n                *self.xmlmetadata(),\n                **metadataattribs\n            )\n            , **attribs)\n        for text in self.data:\n            e.append(text.xml())\n        return e",
    "reference": "Serialise the document to XML.\n\n        Returns:\n            lxml.etree.Element\n\n        See also:\n            :meth:`Document.xmlstring`",
    "generated": "Serialise the document to XML.\n\n        Returns:\n            lxml.etree.Element\n        See also:\n            :meth:`Document.xmlstring`"
  },
  {
    "code": "def json(self):\n        \"\"\"Serialise the document to a ``dict`` ready for serialisation to JSON.\n\n        Example::\n\n            import json\n            jsondoc = json.dumps(doc.json())\n        \"\"\"\n        self.pendingvalidation()\n\n        jsondoc = {'id': self.id, 'children': [], 'declarations': self.jsondeclarations() }\n        if self.version:\n            jsondoc['version'] = self.version\n        else:\n            jsondoc['version'] = FOLIAVERSION\n        jsondoc['generator'] = 'pynlpl.formats.folia-v' + LIBVERSION\n\n        for text in self.data:\n            jsondoc['children'].append(text.json())\n        return jsondoc",
    "reference": "Serialise the document to a ``dict`` ready for serialisation to JSON.\n\n        Example::\n\n            import json\n            jsondoc = json.dumps(doc.json())",
    "generated": "Serialise the document to a ``dict`` ready for serialisation to JSON.\n\n        Example::\n\n            import json\n           "
  },
  {
    "code": "def xmlmetadata(self):\n        \"\"\"Internal method to serialize metadata to XML\"\"\"\n        E = ElementMaker(namespace=\"http://ilk.uvt.nl/folia\",nsmap={None: \"http://ilk.uvt.nl/folia\", 'xml' : \"http://www.w3.org/XML/1998/namespace\"})\n        elements = []\n        if self.metadatatype == \"native\":\n            if isinstance(self.metadata, NativeMetaData):\n                for key, value in self.metadata.items():\n                    elements.append(E.meta(value,id=key) )\n        else:\n            if isinstance(self.metadata, ForeignData):\n                #in-document\n                m = self.metadata\n                while m is not None:\n                    elements.append(m.xml())\n                    m = m.next\n        for metadata_id, submetadata in self.submetadata.items():\n            subelements = []\n            attribs = {\n                \"{http://www.w3.org/XML/1998/namespace}id\": metadata_id,\n                \"type\": self.submetadatatype[metadata_id] }\n            if isinstance(submetadata, NativeMetaData):\n                for key, value in submetadata.items():\n                    subelements.append(E.meta(value,id=key) )\n            elif isinstance(submetadata, ExternalMetaData):\n                attribs['src'] = submetadata.url\n            elif isinstance(submetadata, ForeignData):\n                #in-document\n                m = submetadata\n                while m is not None:\n                    subelements.append(m.xml())\n                    m = m.next\n            elements.append( E.submetadata(*subelements, **attribs))\n        return elements",
    "reference": "Internal method to serialize metadata to XML",
    "generated": "Internal method to serialize metadata to XML"
  },
  {
    "code": "def declare(self, annotationtype, set, **kwargs):\n        \"\"\"Declare a new annotation type to be used in the document.\n\n        Keyword arguments can be used to set defaults for any annotation of this type and set.\n\n        Arguments:\n            annotationtype: The type of annotation, this is conveyed by passing the corresponding annototion class (such as :class:`PosAnnotation` for example), or a member of :class:`AnnotationType`, such as ``AnnotationType.POS``.\n            set (str): the set, should formally be a URL pointing to the set definition\n\n        Keyword Arguments:\n            annotator (str): Sets a default annotator\n            annotatortype: Should be either ``AnnotatorType.MANUAL`` or ``AnnotatorType.AUTO``, indicating whether the annotation was performed manually or by an automated process.\n            datetime (datetime.datetime): Sets the default datetime\n            alias (str): Defines alias that may be used in set attribute of elements instead of the full set name\n\n        Example::\n\n            doc.declare(folia.PosAnnotation, 'http://some/path/brown-tag-set', annotator=\"mytagger\", annotatortype=folia.AnnotatorType.AUTO)\n        \"\"\"\n        if (sys.version > '3' and not isinstance(set,str)) or (sys.version < '3' and not isinstance(set,(str,unicode))):\n            raise ValueError(\"Set parameter for declare() must be a string\")\n\n        if inspect.isclass(annotationtype):\n            annotationtype = annotationtype.ANNOTATIONTYPE\n        if annotationtype in self.alias_set and set in self.alias_set[annotationtype]:\n            raise ValueError(\"Set \" + set + \" conflicts with alias, may not be equal!\")\n        if not (annotationtype, set) in self.annotations:\n            self.annotations.append( (annotationtype,set) )\n            if set and self.loadsetdefinitions and not set in self.setdefinitions:\n                if set[:7] == \"http://\" or set[:8] == \"https://\" or set[:6] == \"ftp://\":\n                    self.setdefinitions[set] = SetDefinition(set,verbose=self.verbose) #will raise exception on error\n        if not annotationtype in self.annotationdefaults:\n            self.annotationdefaults[annotationtype] = {}\n        self.annotationdefaults[annotationtype][set] = kwargs\n        if 'alias' in kwargs:\n            if annotationtype in self.set_alias and set in self.set_alias[annotationtype] and self.set_alias[annotationtype][set] != kwargs['alias']:\n                raise ValueError(\"Redeclaring set \" + set + \" with another alias ('\"+kwargs['alias']+\"') is not allowed!\")\n            if annotationtype in self.alias_set and kwargs['alias'] in self.alias_set[annotationtype] and self.alias_set[annotationtype][kwargs['alias']] != set:\n                raise ValueError(\"Redeclaring alias \" + kwargs['alias'] + \" with another set ('\"+set+\"') is not allowed!\")\n            if annotationtype in self.set_alias and kwargs['alias'] in self.set_alias[annotationtype]:\n                raise ValueError(\"Alias \" + kwargs['alias'] + \" conflicts with set name, may not be equal!\")\n            if annotationtype not in self.alias_set:\n                self.alias_set[annotationtype] = {}\n            if annotationtype not in self.set_alias:\n                self.set_alias[annotationtype] = {}\n            self.alias_set[annotationtype][kwargs['alias']] = set\n            self.set_alias[annotationtype][set] = kwargs['alias']",
    "reference": "Declare a new annotation type to be used in the document.\n\n        Keyword arguments can be used to set defaults for any annotation of this type and set.\n\n        Arguments:\n            annotationtype: The type of annotation, this is conveyed by passing the corresponding annototion class (such as :class:`PosAnnotation` for example), or a member of :class:`AnnotationType`, such as ``AnnotationType.POS``.\n            set (str): the set, should formally be a URL pointing to the set definition\n\n        Keyword Arguments:\n            annotator (str): Sets a default annotator\n            annotatortype: Should be either ``AnnotatorType.MANUAL`` or ``AnnotatorType.AUTO``, indicating whether the annotation was performed manually or by an automated process.\n            datetime (datetime.datetime): Sets the default datetime\n            alias (str): Defines alias that may be used in set attribute of elements instead of the full set name\n\n        Example::\n\n            doc.declare(folia.PosAnnotation, 'http://some/path/brown-tag-set', annotator=\"mytagger\", annotatortype=folia.AnnotatorType.AUTO)",
    "generated": "Declare a new annotation type to be used in the document.\n\n        Keyword arguments can be used to set defaults for any annotation of this type and set.\n\n        Arguments:\n            annotationtype: The type of annotation, this is conveyed by passing the corresponding annototion class (such as :class:`PosAnnotation` for example), or a member of :class:`AnnotationType`, such as ``AnnotationType.POS``.\n            set (str): Sets a default annotator\n            annotatortype: Should be either ``AnnotatorType.MANUAL`` or ``AnnotatorType"
  },
  {
    "code": "def defaultset(self, annotationtype):\n        \"\"\"Obtain the default set for the specified annotation type.\n\n        Arguments:\n            annotationtype: The type of annotation, this is conveyed by passing the corresponding annototion class (such as :class:`PosAnnotation` for example), or a member of :class:`AnnotationType`, such as ``AnnotationType.POS``.\n\n        Returns:\n            the set (str)\n\n        Raises:\n            :class:`NoDefaultError` if the annotation type does not exist or if there is ambiguity (multiple sets for the same type)\n        \"\"\"\n\n        if inspect.isclass(annotationtype) or isinstance(annotationtype,AbstractElement): annotationtype = annotationtype.ANNOTATIONTYPE\n        try:\n            return list(self.annotationdefaults[annotationtype].keys())[0]\n        except KeyError:\n            raise NoDefaultError\n        except IndexError:\n            raise NoDefaultError",
    "reference": "Obtain the default set for the specified annotation type.\n\n        Arguments:\n            annotationtype: The type of annotation, this is conveyed by passing the corresponding annototion class (such as :class:`PosAnnotation` for example), or a member of :class:`AnnotationType`, such as ``AnnotationType.POS``.\n\n        Returns:\n            the set (str)\n\n        Raises:\n            :class:`NoDefaultError` if the annotation type does not exist or if there is ambiguity (multiple sets for the same type)",
    "generated": "Obtain the default set for the specified annotation type.\n\n        Arguments:\n            annotationtype: The type of annotation, this is conveyed by passing the corresponding annototion class (such as :class:`PosAnnotation`, such as ``AnnotationType.POS``.\n\n        Returns:\n            :class:`NoDefaultError` if the annotation type does not exist or if there is ambiguity (multiple sets for the same type)"
  },
  {
    "code": "def defaultannotator(self, annotationtype, set=None):\n        \"\"\"Obtain the default annotator for the specified annotation type and set.\n\n        Arguments:\n            annotationtype: The type of annotation, this is conveyed by passing the corresponding annototion class (such as :class:`PosAnnotation` for example), or a member of :class:`AnnotationType`, such as ``AnnotationType.POS``.\n            set (str): the set, should formally be a URL pointing to the set definition\n\n        Returns:\n            the set (str)\n\n        Raises:\n            :class:`NoDefaultError` if the annotation type does not exist or if there is ambiguity (multiple sets for the same type)\n        \"\"\"\n\n        if inspect.isclass(annotationtype) or isinstance(annotationtype,AbstractElement): annotationtype = annotationtype.ANNOTATIONTYPE\n        if not set: set = self.defaultset(annotationtype)\n        try:\n            return self.annotationdefaults[annotationtype][set]['annotator']\n        except KeyError:\n            raise NoDefaultError",
    "reference": "Obtain the default annotator for the specified annotation type and set.\n\n        Arguments:\n            annotationtype: The type of annotation, this is conveyed by passing the corresponding annototion class (such as :class:`PosAnnotation` for example), or a member of :class:`AnnotationType`, such as ``AnnotationType.POS``.\n            set (str): the set, should formally be a URL pointing to the set definition\n\n        Returns:\n            the set (str)\n\n        Raises:\n            :class:`NoDefaultError` if the annotation type does not exist or if there is ambiguity (multiple sets for the same type)",
    "generated": "Obtain the default annotator for the specified annotation type and set.\n\n        Arguments:\n            annotationtype: The type of annotation, this is conveyed by passing the corresponding annototion class (such as :class:`PosAnnotation` for example), or a member of :class:`AnnotationType`, such as ``AnnotationType.POS``.\n            set (str): the set, should formally be a URL pointing to the set definition\n\n        Returns:\n            the set (str)"
  },
  {
    "code": "def parsemetadata(self, node):\n        \"\"\"Internal method to parse metadata\"\"\"\n\n        if 'type' in node.attrib:\n            self.metadatatype = node.attrib['type']\n        else:\n            #no type specified, default to native\n            self.metadatatype = \"native\"\n\n        if 'src' in node.attrib:\n            self.metadata = ExternalMetaData(node.attrib['src'])\n        elif self.metadatatype == \"native\":\n            self.metadata = NativeMetaData()\n        else:\n            self.metadata = None #may be set below to ForeignData\n\n        for subnode in node:\n            if subnode.tag == '{' + NSFOLIA + '}annotations':\n                self.parsexmldeclarations(subnode)\n            elif subnode.tag == '{' + NSFOLIA + '}meta':\n                if self.metadatatype == \"native\":\n                    if subnode.text:\n                        self.metadata[subnode.attrib['id']] = subnode.text\n                else:\n                    raise MetaDataError(\"Encountered a meta element but metadata type is not native!\")\n            elif subnode.tag == '{' + NSFOLIA + '}provenance':\n                #forward compatibility with FoLiA 2.0; ignore provenance\n                print(\"WARNING: Ignoring provenance data. Use foliapy instead of pynlpl.formats.folia for FoLiA v2.0 compatibility!\",file=sys.stderr)\n                pass\n            elif subnode.tag == '{' + NSFOLIA + '}foreign-data':\n                if self.metadatatype == \"native\":\n                    raise MetaDataError(\"Encountered a foreign-data element but metadata type is native!\")\n                elif self.metadata is not None:\n                    #multiple foreign-data elements, chain:\n                    e = self.metadata\n                    while e.next is not None:\n                        e = e.next\n                    e.next = ForeignData(self, node=subnode)\n                else:\n                    self.metadata = ForeignData(self, node=subnode)\n            elif subnode.tag == '{' + NSFOLIA + '}submetadata':\n                self.parsesubmetadata(subnode)\n            elif subnode.tag == '{http://www.mpi.nl/IMDI/Schema/IMDI}METATRANSCRIPT': #backward-compatibility for old IMDI without foreign-key\n                E = ElementMaker(namespace=NSFOLIA,nsmap={None: NSFOLIA, 'xml' : \"http://www.w3.org/XML/1998/namespace\"})\n                self.metadatatype = \"imdi\"\n                self.metadata = ForeignData(self, node=subnode)",
    "reference": "Internal method to parse metadata",
    "generated": "Internal method to parse metadata."
  },
  {
    "code": "def pendingvalidation(self, warnonly=None):\n        \"\"\"Perform any pending validations\n\n        Parameters:\n            warnonly (bool): Warn only (True) or raise exceptions (False). If set to None then this value will be determined based on the document's FoLiA version (Warn only before FoLiA v1.5)\n\n        Returns:\n            bool\n        \"\"\"\n        if self.debug: print(\"[PyNLPl FoLiA DEBUG] Processing pending validations (if any)\",file=stderr)\n\n        if warnonly is None and self and self.version:\n            warnonly = (checkversion(self.version, '1.5.0') < 0) #warn only for documents older than FoLiA v1.5\n        if self.textvalidation:\n            while self.offsetvalidationbuffer:\n                structureelement, textclass = self.offsetvalidationbuffer.pop()\n\n                if self.debug: print(\"[PyNLPl FoLiA DEBUG] Performing offset validation on \" + repr(structureelement) + \" textclass \" + textclass,file=stderr)\n\n                #validate offsets\n                tc = structureelement.textcontent(textclass)\n                if tc.offset is not None:\n                    try:\n                        tc.getreference(validate=True)\n                    except UnresolvableTextContent:\n                        msg = \"Text for \" + structureelement.__class__.__name__ + \", ID \" + str(structureelement.id) + \", textclass \" + textclass  + \", has incorrect offset \" + str(tc.offset) + \" or invalid reference\"\n                        print(\"TEXT VALIDATION ERROR: \" + msg,file=sys.stderr)\n                        if not warnonly:\n                            raise",
    "reference": "Perform any pending validations\n\n        Parameters:\n            warnonly (bool): Warn only (True) or raise exceptions (False). If set to None then this value will be determined based on the document's FoLiA version (Warn only before FoLiA v1.5)\n\n        Returns:\n            bool",
    "generated": "Perform any pending validations\n        Parameters:\n            warnonly (bool): Warn only before FoLiA v1.5"
  },
  {
    "code": "def paragraphs(self, index = None):\n        \"\"\"Return a generator of all paragraphs found in the document.\n\n        If an index is specified, return the n'th paragraph only (starting at 0)\"\"\"\n        if index is None:\n            return self.select(Paragraph)\n        else:\n            if index < 0:\n                index = sum(t.count(Paragraph) for t in self.data) + index\n            for t in self.data:\n                for i,e in enumerate(t.select(Paragraph)) :\n                    if i == index:\n                        return e\n            raise IndexError",
    "reference": "Return a generator of all paragraphs found in the document.\n\n        If an index is specified, return the n'th paragraph only (starting at 0)",
    "generated": "Return all paragraphs found in the document.\n\n        If an index is specified, return the n'th paragraph only (starting at 0)\"\"\""
  },
  {
    "code": "def sentences(self, index = None):\n        \"\"\"Return a generator of all sentence found in the document. Except for sentences in quotes.\n\n        If an index is specified, return the n'th sentence only (starting at 0)\"\"\"\n        if index is None:\n            return self.select(Sentence,None,True,[Quote])\n        else:\n            if index < 0:\n                index = sum(t.count(Sentence,None,True,[Quote]) for t in self.data) + index\n            for t in self.data:\n                for i,e in enumerate(t.select(Sentence,None,True,[Quote])) :\n                    if i == index:\n                        return e\n            raise IndexError",
    "reference": "Return a generator of all sentence found in the document. Except for sentences in quotes.\n\n        If an index is specified, return the n'th sentence only (starting at 0)",
    "generated": "Return a generator of all sentence found in the document. Except for sentences in quotes.\n\n        If an index is specified, return the n'th sentence only (starting at 0)\"\"\""
  },
  {
    "code": "def _states(self, state, processedstates=[]): #pylint: disable=dangerous-default-value\n        \"\"\"Iterate over all states in no particular order\"\"\"\n        processedstates.append(state)\n\n        for nextstate in state.epsilon:\n            if not nextstate in processedstates:\n                self._states(nextstate, processedstates)\n\n        for _, nextstate in state.transitions:\n            if not nextstate in processedstates:\n                self._states(nextstate, processedstates)\n\n        return processedstates",
    "reference": "Iterate over all states in no particular order",
    "generated": "Iterate over all states in no particular order"
  },
  {
    "code": "def log(msg, **kwargs):\n    \"\"\"Generic log method. Will prepend timestamp.\n\n    Keyword arguments:\n      system   - Name of the system/module\n      indent   - Integer denoting the desired level of indentation\n      streams  - List of streams to output to\n      stream   - Stream to output to (singleton version of streams)\n    \"\"\"\n    if 'debug' in kwargs:\n        if 'currentdebug' in kwargs:\n            if kwargs['currentdebug'] < kwargs['debug']:\n                return False\n        else:\n            return False #no currentdebug passed, assuming no debug mode and thus skipping message\n\n    s = \"[\" + datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"] \"\n    if 'system' in kwargs:\n        s += \"[\" + system + \"] \"\n\n\n    if 'indent' in kwargs:\n        s += (\"\\t\" * int(kwargs['indent']))\n\n    s += u(msg)\n\n    if s[-1] != '\\n':\n        s += '\\n'\n\n    if 'streams' in kwargs:\n        streams = kwargs['streams']\n    elif 'stream' in kwargs:\n        streams = [kwargs['stream']]\n    else:\n        streams = [stderr]\n\n    for stream in streams:\n        stream.write(s)\n    return s",
    "reference": "Generic log method. Will prepend timestamp.\n\n    Keyword arguments:\n      system   - Name of the system/module\n      indent   - Integer denoting the desired level of indentation\n      streams  - List of streams to output to\n      stream   - Stream to output to (singleton version of streams)",
    "generated": "Generic log method. Will prepend timestamp.\n\n    Keyword arguments:\n      system   - Name of the system/module\n      indent   - Integer denoting the desired level of indentation\n      streams"
  },
  {
    "code": "def get_syn_ids_by_lemma(self, lemma):\n        \"\"\"Returns a list of synset IDs based on a lemma\"\"\"\n        if not isinstance(lemma,unicode):\n            lemma = unicode(lemma,'utf-8')\n\n\n        http, resp, content = self.connect()\n\n        params   = \"\"\n        fragment = \"\"\n\n        path = \"cdb_syn\"\n        if self.debug:\n            printf( \"cornettodb/views/query_remote_syn_lemma: db_opt: %s\" % path )\n\n        query_opt = \"dict_search\"\n        if self.debug:\n            printf( \"cornettodb/views/query_remote_syn_lemma: query_opt: %s\" % query_opt )\n    \n        qdict = {}\n        qdict[ \"action\" ] = \"queryList\"\n        qdict[ \"word\" ]   = lemma.encode('utf-8')\n\n\n        query = urllib.urlencode( qdict )\n\n        db_url_tuple = ( self.scheme, self.host + ':' + str(self.port), path, params, query, fragment )\n        db_url = urlparse.urlunparse( db_url_tuple )\n        if self.debug:\n            printf( \"db_url: %s\" % db_url )\n\n        resp, content = http.request( db_url, \"GET\" )\n        if self.debug:\n            printf( \"resp:\\n%s\" % resp )\n            printf( \"content:\\n%s\" % content )\n        #    printf( \"content is of type: %s\" % type( content ) )\n\n        dict_list = []\n        dict_list = eval( content )        # string to list\n\n        synsets = []\n        items = len( dict_list )\n        if self.debug:\n            printf( \"items: %d\" % items )\n\n        # syn dict: like lu dict, but without pos: part-of-speech\n        for dict in dict_list:\n            if self.debug:\n                printf( dict )\n\n            seq_nr = dict[ \"seq_nr\" ]   # sense number\n            value  = dict[ \"value\" ]    # lexical unit identifier\n            form   = dict[ \"form\" ]     # lemma\n            label  = dict[ \"label\" ]    # label to be shown\n\n            if self.debug:\n                printf( \"seq_nr: %s\" % seq_nr )\n                printf( \"value:  %s\" % value )\n                printf( \"form:   %s\" % form )\n                printf( \"label:  %s\" % label )\n\n            if value != \"\":\n                synsets.append( value )\n\n        return synsets",
    "reference": "Returns a list of synset IDs based on a lemma",
    "generated": "Returns a list of synset IDs based on a lemma\"\"\"\n        if not isinstance(lemma, unicode):\n            lemma = unicode(lemma,'utf-8')\n\n\n        http, resp, content = self.connect()\n\n        fragment = \"\""
  },
  {
    "code": "def get_synset_xml(self,syn_id):\n        \"\"\"\n        call cdb_syn with synset identifier -> returns the synset xml;\n        \"\"\"\n\n        http, resp, content = self.connect()\n\n        params   = \"\"\n        fragment = \"\"\n\n        path = \"cdb_syn\"\n        if self.debug:\n            printf( \"cornettodb/views/query_remote_syn_id: db_opt: %s\" % path )\n\n        # output_opt: plain, html, xml\n        # 'xml' is actually xhtml (with markup), but it is not valid xml!\n        # 'plain' is actually valid xml (without markup)\n        output_opt = \"plain\"\n        if self.debug:\n            printf( \"cornettodb/views/query_remote_syn_id: output_opt: %s\" % output_opt )\n\n        action = \"runQuery\"\n        if self.debug:\n            printf( \"cornettodb/views/query_remote_syn_id: action: %s\" % action )\n            printf( \"cornettodb/views/query_remote_syn_id: query: %s\" % syn_id )\n\n        qdict = {}\n        qdict[ \"action\" ]  = action\n        qdict[ \"query\" ]   = syn_id\n        qdict[ \"outtype\" ] = output_opt\n\n        query = urllib.urlencode( qdict )\n\n        db_url_tuple = ( self.scheme, self.host + ':' + str(self.port), path, params, query, fragment )\n        db_url = urlparse.urlunparse( db_url_tuple )\n        if self.debug:\n            printf( \"db_url: %s\" % db_url )\n\n        resp, content = http.request( db_url, \"GET\" )\n        if self.debug:\n            printf( \"resp:\\n%s\" % resp )\n        #    printf( \"content:\\n%s\" % content )\n        #    printf( \"content is of type: %s\" % type( content ) )        #<type 'str'>\n\n        xml_data = eval( content )\n        return etree.fromstring( xml_data )",
    "reference": "call cdb_syn with synset identifier -> returns the synset xml;",
    "generated": "Call cdb_syn with synset identifier -> returns the synset xml;"
  },
  {
    "code": "def senses(self, bestonly=False):\n        \"\"\"Returns a list of all predicted senses\"\"\"\n        l = []\n        for word_id, senses,distance in self:\n            for sense, confidence in senses:\n                if not sense in l: l.append(sense)\n                if bestonly:\n                    break\n        return l",
    "reference": "Returns a list of all predicted senses",
    "generated": "Returns a list of all predicted senses"
  },
  {
    "code": "def align(self,inputwords, outputwords):\n        \"\"\"For each inputword, provides the index of the outputword\"\"\"\n        alignment = []\n        cursor = 0\n        for inputword in inputwords:\n            if len(outputwords) > cursor and outputwords[cursor] == inputword:\n                alignment.append(cursor)\n                cursor += 1\n            elif len(outputwords) > cursor+1 and outputwords[cursor+1] == inputword:\n                alignment.append(cursor+1)\n                cursor += 2\n            else:\n                alignment.append(None)\n                cursor += 1\n        return alignment",
    "reference": "For each inputword, provides the index of the outputword",
    "generated": "For each inputword, provides the index of the inputword\"\"\""
  },
  {
    "code": "def tokenize(text, regexps=TOKENIZERRULES):\n    \"\"\"Tokenizes a string and returns a list of tokens\n\n    :param text: The text to tokenise\n    :type text: string\n    :param regexps: Regular expressions to use as tokeniser rules in tokenisation (default=_pynlpl.textprocessors.TOKENIZERRULES_)\n    :type regexps:  Tuple/list of regular expressions to use in tokenisation\n    :rtype: Returns a list of tokens\n\n    Examples:\n\n    >>> for token in tokenize(\"This is a test.\"):\n    ...    print(token)\n    This\n    is\n    a\n    test\n    .\n\n\n    \"\"\"\n\n    for i,regexp in list(enumerate(regexps)):\n        if isstring(regexp):\n            regexps[i] = re.compile(regexp)\n\n    tokens = []\n    begin = 0\n    for i, c in enumerate(text):\n        if begin > i:\n            continue\n        elif i == begin:\n            m = False\n            for regexp in regexps:\n                m = regexp.findall(text[i:i+300])\n                if m:\n                    tokens.append(m[0])\n                    begin = i + len(m[0])\n                    break\n            if m: continue\n\n        if c in string.punctuation or c in WHITESPACE:\n            prev = text[i-1] if i > 0 else \"\"\n            next = text[i+1] if i < len(text)-1 else \"\"\n\n            if (c == '.' or c == ',') and prev.isdigit() and next.isdigit():\n                #punctuation in between numbers, keep as one token\n                pass\n            elif (c == \"'\" or c == \"`\") and prev.isalpha() and next.isalpha():\n                #quote in between chars, keep...\n                pass\n            elif c not in WHITESPACE and next == c: #group clusters of identical punctuation together\n                continue\n            elif c == '\\r' and prev == '\\n':\n                #ignore\n                begin = i+1\n                continue\n            else:\n                token = text[begin:i]\n                if token: tokens.append(token)\n\n                if c not in WHITESPACE:\n                    tokens.append(c) #anything but spaces and newlines (i.e. punctuation) counts as a token too\n                begin = i + 1 #set the begin cursor\n\n    if begin <= len(text) - 1:\n        token = text[begin:]\n        tokens.append(token)\n\n    return tokens",
    "reference": "Tokenizes a string and returns a list of tokens\n\n    :param text: The text to tokenise\n    :type text: string\n    :param regexps: Regular expressions to use as tokeniser rules in tokenisation (default=_pynlpl.textprocessors.TOKENIZERRULES_)\n    :type regexps:  Tuple/list of regular expressions to use in tokenisation\n    :rtype: Returns a list of tokens\n\n    Examples:\n\n    >>> for token in tokenize(\"This is a test.\"):\n    ...    print(token)\n    This\n    is\n    a\n    test\n    .",
    "generated": "Tokenizes a string and returns a list of tokens\n\n    Examples:\n\n    >>> for token in tokenize(\"This is a test.\"):\n    ...    print(token)\n    This\n    is\n    a\n    test\n    ."
  },
  {
    "code": "def strip_accents(s, encoding= 'utf-8'):\n    \"\"\"Strip characters with diacritics and return a flat ascii representation\"\"\"\n    if sys.version < '3':\n        if isinstance(s,unicode):\n           return unicodedata.normalize('NFKD', s).encode('ASCII', 'ignore')\n        else:\n           return unicodedata.normalize('NFKD', unicode(s,encoding)).encode('ASCII', 'ignore')\n    else:\n        if isinstance(s,bytes): s = str(s,encoding)\n        return str(unicodedata.normalize('NFKD', s).encode('ASCII', 'ignore'),'ascii')",
    "reference": "Strip characters with diacritics and return a flat ascii representation",
    "generated": "Strip characters with diacritics and return a flat ascii representation."
  },
  {
    "code": "def swap(tokens, maxdist=2):\n    \"\"\"Perform a swap operation on a sequence of tokens, exhaustively swapping all tokens up to the maximum specified distance. This is a subset of all permutations.\"\"\"\n    assert maxdist >= 2\n    tokens = list(tokens)\n    if maxdist > len(tokens):\n        maxdist = len(tokens)\n    l = len(tokens)\n    for i in range(0,l - 1):\n        for permutation in permutations(tokens[i:i+maxdist]):\n            if permutation != tuple(tokens[i:i+maxdist]):\n                newtokens = tokens[:i]\n                newtokens += permutation\n                newtokens += tokens[i+maxdist:]\n                yield newtokens\n        if maxdist == len(tokens):\n            break",
    "reference": "Perform a swap operation on a sequence of tokens, exhaustively swapping all tokens up to the maximum specified distance. This is a subset of all permutations.",
    "generated": "Perform a swap operation on a sequence of tokens, exhaustively swapping all tokens up to the maximum specified distance. This is a subset of all permutations."
  },
  {
    "code": "def find_keyword_in_context(tokens, keyword, contextsize=1):\n    \"\"\"Find a keyword in a particular sequence of tokens, and return the local context. Contextsize is the number of words to the left and right. The keyword may have multiple word, in which case it should to passed as a tuple or list\"\"\"\n    if isinstance(keyword,tuple) and isinstance(keyword,list):\n        l = len(keyword)\n    else:\n        keyword = (keyword,)\n        l = 1\n    n = l + contextsize*2\n    focuspos = contextsize + 1\n    for ngram in Windower(tokens,n,None,None):\n        if ngram[focuspos:focuspos+l] == keyword:\n            yield ngram[:focuspos], ngram[focuspos:focuspos+l],ngram[focuspos+l+1:]",
    "reference": "Find a keyword in a particular sequence of tokens, and return the local context. Contextsize is the number of words to the left and right. The keyword may have multiple word, in which case it should to passed as a tuple or list",
    "generated": "Find a keyword in a particular sequence of tokens, and return the local context. Contextsize is the number of words to the left and right. The keyword may have multiple word, in which case it should to passed as a tuple or list"
  },
  {
    "code": "def randomprune(self,n):\n        \"\"\"prune down to n items at random, disregarding their score\"\"\"\n        self.data = random.sample(self.data, n)",
    "reference": "prune down to n items at random, disregarding their score",
    "generated": "prune down to n items at random, disregarding their score"
  },
  {
    "code": "def append(self, item):\n        \"\"\"Add an item to the Tree\"\"\"\n        if not isinstance(item, Tree):\n            return ValueError(\"Can only append items of type Tree\")\n        if not self.children: self.children = []\n        item.parent = self\n        self.children.append(item)",
    "reference": "Add an item to the Tree",
    "generated": "Add an item to the Tree"
  },
  {
    "code": "def size(self):\n        \"\"\"Size is number of nodes under the trie, including the current node\"\"\"\n        if self.children:\n            return sum( ( c.size() for c in self.children.values() ) ) + 1\n        else:\n            return 1",
    "reference": "Size is number of nodes under the trie, including the current node",
    "generated": "Size is number of nodes under the trie, including the current node"
  },
  {
    "code": "def validate(self, formats_dir=\"../formats/\"):\n        \"\"\"checks if the document is valid\"\"\"\n        #TODO: download XSD from web\n        if self.inline:\n            xmlschema = ElementTree.XMLSchema(ElementTree.parse(StringIO(\"\\n\".join(open(formats_dir+\"dcoi-dsc.xsd\").readlines()))))\n            xmlschema.assertValid(self.tree)\n            #return xmlschema.validate(self)\n        else:\n            xmlschema = ElementTree.XMLSchema(ElementTree.parse(StringIO(\"\\n\".join(open(formats_dir+\"dutchsemcor-standalone.xsd\").readlines()))))\n            xmlschema.assertValid(self.tree)",
    "reference": "checks if the document is valid",
    "generated": "checks if the document is valid."
  },
  {
    "code": "def xpath(self, expression):\n        \"\"\"Executes an xpath expression using the correct namespaces\"\"\"\n        global namespaces\n        return self.tree.xpath(expression, namespaces=namespaces)",
    "reference": "Executes an xpath expression using the correct namespaces",
    "generated": "Executes an xpath expression using the correct namespaces"
  },
  {
    "code": "def align(self, referencewords, datatuple):\n        \"\"\"align the reference sentence with the tagged data\"\"\"\n        targetwords = []\n        for i, (word,lemma,postag) in enumerate(zip(datatuple[0],datatuple[1],datatuple[2])):\n            if word:\n                subwords = word.split(\"_\")\n                for w in subwords: #split multiword expressions\n                    targetwords.append( (w, lemma, postag, i, len(subwords) > 1 ) ) #word, lemma, pos, index, multiword? \n\n        referencewords = [ w.lower() for w in referencewords ]          \n        alignment = []\n        for i, referenceword in enumerate(referencewords):\n            found = False\n            best = 0  \n            distance = 999999          \n            for j, (targetword, lemma, pos, index, multiword) in enumerate(targetwords):\n                if referenceword == targetword and abs(i-j) < distance:\n                    found = True\n                    best = j\n                    distance = abs(i-j)\n\n            if found:\n                alignment.append(targetwords[best])\n            else:                \n                alignment.append((None,None,None,None,False)) #no alignment found        \n        \n        return alignment",
    "reference": "align the reference sentence with the tagged data",
    "generated": "align the reference sentence with the tagged data"
  },
  {
    "code": "def build(self, **kwargs):\n        \"\"\"Build the lexer.\"\"\"\n        self.lexer = ply.lex.lex(object=self, **kwargs)",
    "reference": "Build the lexer.",
    "generated": "Build the lexer."
  },
  {
    "code": "def is_authorized(self, request):\n        \"\"\"Check if the user is authenticated for the given request.\n\n        The include_paths and exclude_paths are first checked. If\n        authentication is required then the Authorization HTTP header is\n        checked against the credentials.\n\n        \"\"\"\n        if self._is_request_in_include_path(request):\n            if self._is_request_in_exclude_path(request):\n                return True\n            else:\n                auth = request.authorization\n                if auth and auth[0] == 'Basic':\n                    credentials = b64decode(auth[1]).decode('UTF-8')\n                    username, password = credentials.split(':', 1)\n                    return self._users.get(username) == password\n                else:\n                    return False\n        else:\n            return True",
    "reference": "Check if the user is authenticated for the given request.\n\n        The include_paths and exclude_paths are first checked. If\n        authentication is required then the Authorization HTTP header is\n        checked against the credentials.",
    "generated": "Check if the user is authenticated for the given request.\n\n        The include_paths and exclude_paths are first checked. If\n        authentication is required then the Authorization HTTP header is\n        checked against the credentials."
  },
  {
    "code": "def _login(self, environ, start_response):\n        \"\"\"Send a login response back to the client.\"\"\"\n        response = HTTPUnauthorized()\n        response.www_authenticate = ('Basic', {'realm': self._realm})\n        return response(environ, start_response)",
    "reference": "Send a login response back to the client.",
    "generated": "Send a login response back to the client."
  },
  {
    "code": "def _is_request_in_include_path(self, request):\n        \"\"\"Check if the request path is in the `_include_paths` list.\n\n        If no specific include paths are given then we assume that\n        authentication is required for all paths.\n\n        \"\"\"\n        if self._include_paths:\n            for path in self._include_paths:\n                if request.path.startswith(path):\n                    return True\n            return False\n        else:\n            return True",
    "reference": "Check if the request path is in the `_include_paths` list.\n\n        If no specific include paths are given then we assume that\n        authentication is required for all paths.",
    "generated": "Check if the request path is in the `_include_paths` list.\n\n        If no specific include paths are given then we assume that\n        authentication is required for all paths."
  },
  {
    "code": "def _is_request_in_exclude_path(self, request):\n        \"\"\"Check if the request path is in the `_exclude_paths` list\"\"\"\n        if self._exclude_paths:\n            for path in self._exclude_paths:\n                if request.path.startswith(path):\n                    return True\n            return False\n        else:\n            return False",
    "reference": "Check if the request path is in the `_exclude_paths` list",
    "generated": "Check if the request path is in the `_exclude_paths` list"
  },
  {
    "code": "def bootstrap_prompt(prompt_kwargs, group):\n    \"\"\"\n    Bootstrap prompt_toolkit kwargs or use user defined values.\n\n    :param prompt_kwargs: The user specified prompt kwargs.\n    \"\"\"\n    prompt_kwargs = prompt_kwargs or {}\n\n    defaults = {\n        \"history\": InMemoryHistory(),\n        \"completer\": ClickCompleter(group),\n        \"message\": u\"> \",\n    }\n\n    for key in defaults:\n        default_value = defaults[key]\n        if key not in prompt_kwargs:\n            prompt_kwargs[key] = default_value\n\n    return prompt_kwargs",
    "reference": "Bootstrap prompt_toolkit kwargs or use user defined values.\n\n    :param prompt_kwargs: The user specified prompt kwargs.",
    "generated": "Bootstrap prompt_toolkit kwargs or use user defined values.\n\n    :param prompt_kwargs: The user specified prompt kwargs."
  },
  {
    "code": "def repl(  # noqa: C901\n    old_ctx,\n    prompt_kwargs=None,\n    allow_system_commands=True,\n    allow_internal_commands=True,\n):\n    \"\"\"\n    Start an interactive shell. All subcommands are available in it.\n\n    :param old_ctx: The current Click context.\n    :param prompt_kwargs: Parameters passed to\n        :py:func:`prompt_toolkit.shortcuts.prompt`.\n\n    If stdin is not a TTY, no prompt will be printed, but only commands read\n    from stdin.\n\n    \"\"\"\n    # parent should be available, but we're not going to bother if not\n    group_ctx = old_ctx.parent or old_ctx\n    group = group_ctx.command\n    isatty = sys.stdin.isatty()\n\n    # Delete the REPL command from those available, as we don't want to allow\n    # nesting REPLs (note: pass `None` to `pop` as we don't want to error if\n    # REPL command already not present for some reason).\n    repl_command_name = old_ctx.command.name\n    if isinstance(group_ctx.command, click.CommandCollection):\n        available_commands = {\n            cmd_name: cmd_obj\n            for source in group_ctx.command.sources\n            for cmd_name, cmd_obj in source.commands.items()\n        }\n    else:\n        available_commands = group_ctx.command.commands\n    available_commands.pop(repl_command_name, None)\n\n    prompt_kwargs = bootstrap_prompt(prompt_kwargs, group)\n\n    if isatty:\n\n        def get_command():\n            return prompt(**prompt_kwargs)\n\n    else:\n        get_command = sys.stdin.readline\n\n    while True:\n        try:\n            command = get_command()\n        except KeyboardInterrupt:\n            continue\n        except EOFError:\n            break\n\n        if not command:\n            if isatty:\n                continue\n            else:\n                break\n\n        if allow_system_commands and dispatch_repl_commands(command):\n            continue\n\n        if allow_internal_commands:\n            try:\n                result = handle_internal_commands(command)\n                if isinstance(result, six.string_types):\n                    click.echo(result)\n                    continue\n            except ExitReplException:\n                break\n\n        try:\n            args = shlex.split(command)\n        except ValueError as e:\n            click.echo(\"{}: {}\".format(type(e).__name__, e))\n            continue\n\n        try:\n            with group.make_context(None, args, parent=group_ctx) as ctx:\n                group.invoke(ctx)\n                ctx.exit()\n        except click.ClickException as e:\n            e.show()\n        except ClickExit:\n            pass\n        except SystemExit:\n            pass\n        except ExitReplException:\n            break",
    "reference": "Start an interactive shell. All subcommands are available in it.\n\n    :param old_ctx: The current Click context.\n    :param prompt_kwargs: Parameters passed to\n        :py:func:`prompt_toolkit.shortcuts.prompt`.\n\n    If stdin is not a TTY, no prompt will be printed, but only commands read\n    from stdin.",
    "generated": "Start an interactive shell. All subcommands are available in it.\n\n    :param old_ctx: The current Click context.\n    :param prompt_kwargs: Parameters passed to\n        :py:func:`prompt_toolkit.shortcuts.prompt`.\n\n    If stdin is not a TTY, no prompt will be printed, but only commands read\n    from stdin."
  },
  {
    "code": "def handle_internal_commands(command):\n    \"\"\"Run repl-internal commands.\n\n    Repl-internal commands are all commands starting with \":\".\n\n    \"\"\"\n    if command.startswith(\":\"):\n        target = _get_registered_target(command[1:], default=None)\n        if target:\n            return target()",
    "reference": "Run repl-internal commands.\n\n    Repl-internal commands are all commands starting with \":\".",
    "generated": "Run repl-internal commands.\n\n    Repl-internal commands are all commands starting with \":\"."
  },
  {
    "code": "def node_definitions(id_fetcher, type_resolver=None, id_resolver=None):\n    '''\n    Given a function to map from an ID to an underlying object, and a function\n    to map from an underlying object to the concrete GraphQLObjectType it\n    corresponds to, constructs a `Node` interface that objects can implement,\n    and a field config for a `node` root field.\n\n    If the type_resolver is omitted, object resolution on the interface will be\n    handled with the `isTypeOf` method on object types, as with any GraphQL\n    interface without a provided `resolveType` method.\n    '''\n    node_interface = GraphQLInterfaceType(\n        'Node',\n        description='An object with an ID',\n        fields=lambda: OrderedDict((\n            ('id', GraphQLField(\n                GraphQLNonNull(GraphQLID),\n                description='The id of the object.',\n                resolver=id_resolver,\n            )),\n        )),\n        resolve_type=type_resolver\n    )\n    node_field = GraphQLField(\n        node_interface,\n        description='Fetches an object given its ID',\n        args=OrderedDict((\n            ('id', GraphQLArgument(\n                GraphQLNonNull(GraphQLID),\n                description='The ID of an object'\n            )),\n        )),\n        resolver=lambda obj, args, *_: id_fetcher(args.get('id'), *_)\n    )\n    return node_interface, node_field",
    "reference": "Given a function to map from an ID to an underlying object, and a function\n    to map from an underlying object to the concrete GraphQLObjectType it\n    corresponds to, constructs a `Node` interface that objects can implement,\n    and a field config for a `node` root field.\n\n    If the type_resolver is omitted, object resolution on the interface will be\n    handled with the `isTypeOf` method on object types, as with any GraphQL\n    interface without a provided `resolveType` method.",
    "generated": "Given a function to map from an ID to an underlying object, and a function\n    to map from an underlying object to the concrete GraphQLObjectType it\n    corresponds to, constructs a `Node` interface that objects can implement,\n    and a field config for a `node` root field.\n\n    If the type_resolver is omitted, object resolution on the interface will be\n    handled with the `isTypeOf` method on object types, as with any GraphQL\n    interface without a provided `resolveType` method."
  },
  {
    "code": "def from_global_id(global_id):\n    '''\n    Takes the \"global ID\" created by toGlobalID, and retuns the type name and ID\n    used to create it.\n    '''\n    unbased_global_id = unbase64(global_id)\n    _type, _id = unbased_global_id.split(':', 1)\n    return _type, _id",
    "reference": "Takes the \"global ID\" created by toGlobalID, and retuns the type name and ID\n    used to create it.",
    "generated": "Takes the global ID created by toGlobalID, and retuns the type name and ID\n    used to create it."
  },
  {
    "code": "def global_id_field(type_name, id_fetcher=None):\n    '''\n    Creates the configuration for an id field on a node, using `to_global_id` to\n    construct the ID from the provided typename. The type-specific ID is fetcher\n    by calling id_fetcher on the object, or if not provided, by accessing the `id`\n    property on the object.\n    '''\n    return GraphQLField(\n        GraphQLNonNull(GraphQLID),\n        description='The ID of an object',\n        resolver=lambda obj, args, context, info: to_global_id(\n            type_name or info.parent_type.name,\n            id_fetcher(obj, context, info) if id_fetcher else obj.id\n        )\n    )",
    "reference": "Creates the configuration for an id field on a node, using `to_global_id` to\n    construct the ID from the provided typename. The type-specific ID is fetcher\n    by calling id_fetcher on the object, or if not provided, by accessing the `id`\n    property on the object.",
    "generated": "Creates the configuration for an id field on a node, using `to_global_id` to\n    construct the ID from the provided typename. The type-specific ID is fetcher\n    by calling id_fetcher on the object, or if not provided, by accessing the `id`\n    property on the object."
  },
  {
    "code": "def connection_from_list(data, args=None, **kwargs):\n    '''\n    A simple function that accepts an array and connection arguments, and returns\n    a connection object for use in GraphQL. It uses array offsets as pagination,\n    so pagination will only work if the array is static.\n    '''\n    _len = len(data)\n    return connection_from_list_slice(\n        data,\n        args,\n        slice_start=0,\n        list_length=_len,\n        list_slice_length=_len,\n        **kwargs\n    )",
    "reference": "A simple function that accepts an array and connection arguments, and returns\n    a connection object for use in GraphQL. It uses array offsets as pagination,\n    so pagination will only work if the array is static.",
    "generated": "A simple function that accepts an array and connection arguments, and returns\n    a connection object for use in GraphQL. It uses array offsets as pagination,"
  },
  {
    "code": "def connection_from_promised_list(data_promise, args=None, **kwargs):\n    '''\n    A version of `connectionFromArray` that takes a promised array, and returns a\n    promised connection.\n    '''\n    return data_promise.then(lambda data: connection_from_list(data, args, **kwargs))",
    "reference": "A version of `connectionFromArray` that takes a promised array, and returns a\n    promised connection.",
    "generated": "The version of `connectionFromArray` that takes a promised array, and returns a\n    promised connection."
  },
  {
    "code": "def cursor_for_object_in_connection(data, _object):\n    '''\n    Return the cursor associated with an object in an array.\n    '''\n    if _object not in data:\n        return None\n\n    offset = data.index(_object)\n    return offset_to_cursor(offset)",
    "reference": "Return the cursor associated with an object in an array.",
    "generated": "Return the cursor associated with an object in an array."
  },
  {
    "code": "def get_offset_with_default(cursor=None, default_offset=0):\n    '''\n    Given an optional cursor and a default offset, returns the offset\n    to use; if the cursor contains a valid offset, that will be used,\n    otherwise it will be the default.\n    '''\n    if not is_str(cursor):\n        return default_offset\n\n    offset = cursor_to_offset(cursor)\n    try:\n        return int(offset)\n    except:\n        return default_offset",
    "reference": "Given an optional cursor and a default offset, returns the offset\n    to use; if the cursor contains a valid offset, that will be used,\n    otherwise it will be the default.",
    "generated": "Given an optional cursor and a default offset, returns the offset\n    to use; if the cursor contains a valid offset, returns the offset\n    otherwise it will be the default."
  },
  {
    "code": "def generate(data, iterations=1000, force_strength=5.0, dampening=0.01,\n             max_velocity=2.0, max_distance=50, is_3d=True):\n    \"\"\"Runs a force-directed algorithm on a graph, returning a data structure.\n\n    Args:\n        data: An adjacency list of tuples (ie. [(1,2),...])\n        iterations: (Optional) Number of FDL iterations to run in coordinate\n            generation\n        force_strength: (Optional) Strength of Coulomb and Hooke forces\n            (edit this to scale the distance between nodes)\n        dampening: (Optional) Multiplier to reduce force applied to nodes\n        max_velocity: (Optional) Maximum distance a node can move in one step\n        max_distance: (Optional) The maximum inter-node distance considered\n        is_3d: (Optional) Generates three-dimensional coordinates\n\n    Outputs a json-serializable Python object. To visualize, pass the output to\n    `jgraph.draw(...)`.\n    \"\"\"\n\n    edges = [{'source': s, 'target': t} for s, t in data]\n    nodes = force_directed_layout.run(edges, iterations, force_strength,\n                                      dampening, max_velocity, max_distance,\n                                      is_3d)\n    return {'edges': edges, 'nodes': nodes}",
    "reference": "Runs a force-directed algorithm on a graph, returning a data structure.\n\n    Args:\n        data: An adjacency list of tuples (ie. [(1,2),...])\n        iterations: (Optional) Number of FDL iterations to run in coordinate\n            generation\n        force_strength: (Optional) Strength of Coulomb and Hooke forces\n            (edit this to scale the distance between nodes)\n        dampening: (Optional) Multiplier to reduce force applied to nodes\n        max_velocity: (Optional) Maximum distance a node can move in one step\n        max_distance: (Optional) The maximum inter-node distance considered\n        is_3d: (Optional) Generates three-dimensional coordinates\n\n    Outputs a json-serializable Python object. To visualize, pass the output to\n    `jgraph.draw(...)`.",
    "generated": "Runs a force-directed algorithm on a graph, returning a data structure.\n\n    Args:\n        data: An adjacency list of tuples (ie. [(1,2),...])\n        iterations: (Optional) Number of FDL iterations to run in coordinate\n            generation\n        force_strength: (Optional) Strength of Coulomb and Hooke forces\n            (edit this to scale the distance between nodes)"
  },
  {
    "code": "def compress(obj):\n    \"\"\"Outputs json without whitespace.\"\"\"\n    return json.dumps(obj, sort_keys=True, separators=(',', ':'),\n                      cls=CustomEncoder)",
    "reference": "Outputs json without whitespace.",
    "generated": "Outputs json without whitespace."
  },
  {
    "code": "def dumps(obj):\n    \"\"\"Outputs json with formatting edits + object handling.\"\"\"\n    return json.dumps(obj, indent=4, sort_keys=True, cls=CustomEncoder)",
    "reference": "Outputs json with formatting edits + object handling.",
    "generated": "Outputs json with formatting edits + object handling."
  },
  {
    "code": "def encode(self, obj):\n        \"\"\"Fired for every object.\"\"\"\n        s = super(CustomEncoder, self).encode(obj)\n        # If uncompressed, postprocess for formatting\n        if len(s.splitlines()) > 1:\n            s = self.postprocess(s)\n        return s",
    "reference": "Fired for every object.",
    "generated": "Fired for every object."
  },
  {
    "code": "def postprocess(self, json_string):\n        \"\"\"Displays each entry on its own line.\"\"\"\n        is_compressing, is_hash, compressed, spaces = False, False, [], 0\n        for row in json_string.split('\\n'):\n            if is_compressing:\n                if (row[:spaces + 5] == ' ' * (spaces + 4) +\n                        ('\"' if is_hash else '{')):\n                    compressed.append(row.rstrip())\n                elif (len(row) > spaces and row[:spaces] == ' ' * spaces and\n                        re.match('[\\]\\}],?', row[spaces:].rstrip())):\n                    compressed.append(row.rstrip())\n                    is_compressing = False\n                else:\n                    compressed[-1] += ' ' + row.strip()\n            else:\n                compressed.append(row.rstrip())\n                if any(a in row for a in ['edges', 'nodes']):\n                    # Fix to handle issues that arise with empty lists\n                    if '[]' in row:\n                        continue\n                    spaces = sum(1 for _ in takewhile(str.isspace, row))\n                    is_compressing, is_hash = True, '{' in row\n        return '\\n'.join(compressed)",
    "reference": "Displays each entry on its own line.",
    "generated": "Displays each entry on its own line."
  },
  {
    "code": "def run(edges, iterations=1000, force_strength=5.0, dampening=0.01,\n        max_velocity=2.0, max_distance=50, is_3d=True):\n    \"\"\"Runs a force-directed-layout algorithm on the input graph.\n\n    iterations - Number of FDL iterations to run in coordinate generation\n    force_strength - Strength of Coulomb and Hooke forces\n                     (edit this to scale the distance between nodes)\n    dampening - Multiplier to reduce force applied to nodes\n    max_velocity - Maximum distance a node can move in one step\n    max_distance - The maximum distance considered for interactions\n    \"\"\"\n\n    # Get a list of node ids from the edge data\n    nodes = set(e['source'] for e in edges) | set(e['target'] for e in edges)\n\n    # Convert to a data-storing object and initialize some values\n    d = 3 if is_3d else 2\n    nodes = {n: {'velocity': [0.0] * d, 'force': [0.0] * d} for n in nodes}\n\n    # Repeat n times (is there a more Pythonic way to do this?)\n    for _ in repeat(None, iterations):\n\n        # Add in Coulomb-esque node-node repulsive forces\n        for node1, node2 in combinations(nodes.values(), 2):\n            _coulomb(node1, node2, force_strength, max_distance)\n\n        # And Hooke-esque edge spring forces\n        for edge in edges:\n            _hooke(nodes[edge['source']], nodes[edge['target']],\n                   force_strength * edge.get('size', 1), max_distance)\n\n        # Move by resultant force\n        for node in nodes.values():\n            # Constrain the force to the bounds specified by input parameter\n            force = [_constrain(dampening * f, -max_velocity, max_velocity)\n                     for f in node['force']]\n            # Update velocities and reset force\n            node['velocity'] = [v + dv\n                                for v, dv in zip(node['velocity'], force)]\n            node['force'] = [0] * d\n\n    # Clean and return\n    for node in nodes.values():\n        del node['force']\n        node['location'] = node['velocity']\n        del node['velocity']\n        # Even if it's 2D, let's specify three dimensions\n        if not is_3d:\n            node['location'] += [0.0]\n    return nodes",
    "reference": "Runs a force-directed-layout algorithm on the input graph.\n\n    iterations - Number of FDL iterations to run in coordinate generation\n    force_strength - Strength of Coulomb and Hooke forces\n                     (edit this to scale the distance between nodes)\n    dampening - Multiplier to reduce force applied to nodes\n    max_velocity - Maximum distance a node can move in one step\n    max_distance - The maximum distance considered for interactions",
    "generated": "Runs a force-directed-layout algorithm on the input graph.\n\n    iterations - Number of FDL iterations to run in coordinate generation\n    force_strength - Strength of Coulomb and Hooke forces\n                     (edit this to scale the distance between nodes)"
  },
  {
    "code": "def _coulomb(n1, n2, k, r):\n    \"\"\"Calculates Coulomb forces and updates node data.\"\"\"\n    # Get relevant positional data\n    delta = [x2 - x1 for x1, x2 in zip(n1['velocity'], n2['velocity'])]\n    distance = sqrt(sum(d ** 2 for d in delta))\n\n    # If the deltas are too small, use random values to keep things moving\n    if distance < 0.1:\n        delta = [uniform(0.1, 0.2) for _ in repeat(None, 3)]\n        distance = sqrt(sum(d ** 2 for d in delta))\n\n    # If the distance isn't huge (ie. Coulomb is negligible), calculate\n    if distance < r:\n        force = (k / distance) ** 2\n        n1['force'] = [f - force * d for f, d in zip(n1['force'], delta)]\n        n2['force'] = [f + force * d for f, d in zip(n2['force'], delta)]",
    "reference": "Calculates Coulomb forces and updates node data.",
    "generated": "Calculates Coulomb forces and updates node data."
  },
  {
    "code": "def run_step(context):\n    \"\"\"Wipe the entire context.\n\n    Args:\n        Context is a dictionary or dictionary-like.\n        Does not require any specific keys in context.\n    \"\"\"\n    logger.debug(\"started\")\n\n    context.clear()\n    logger.info(f\"Context wiped. New context size: {len(context)}\")\n\n    logger.debug(\"done\")",
    "reference": "Wipe the entire context.\n\n    Args:\n        Context is a dictionary or dictionary-like.\n        Does not require any specific keys in context.",
    "generated": "The entire context.\n\n    Args:\n        Context is a dictionary or dictionary-like.\n        Does not require any specific keys in context."
  },
  {
    "code": "def run_step(context):\n    \"\"\"pypyr step that checks if a file or directory path exists.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                 The following context key must exist\n                - pathsToCheck. str/path-like or list of str/paths.\n                                Path to file on disk to check.\n\n    All inputs support formatting expressions. Supports globs.\n\n    This step creates pathCheckOut in context, containing the results of the\n    path check operation.\n\n    pathCheckOut:\n        'inpath':\n            exists: true # bool. True if path exists.\n            count: 0 # int. Number of files found for in path.\n            found: ['path1', 'path2'] # list of strings. Paths of files found.\n\n    [count] is 0 if no files found. If you specified a single input\n    path to check and it exists, it's going to be 1. If you specified multiple\n    in paths or a glob expression that found more than 1 result, well, take a\n    guess.\n\n    [found] is a list of all the paths found for the [inpath]. If you passed\n    in a glob or globs, will contain the globs found for [inpath].\n\n    This means you can do an existence evaluation like this in a formatting\n    expression: '{pathCheckOut[inpathhere][exists]}'\n\n    Returns:\n        None. updates context arg.\n\n    Raises:\n        pypyr.errors.KeyNotInContextError: pathExists missing in context.\n        pypyr.errors.KeyInContextHasNoValueError: pathCheck exists but is None.\n\n    \"\"\"\n    logger.debug(\"started\")\n    context.assert_key_has_value(key='pathCheck', caller=__name__)\n\n    paths_to_check = context['pathCheck']\n\n    if not paths_to_check:\n        raise KeyInContextHasNoValueError(\"context['pathCheck'] must have a \"\n                                          f\"value for {__name__}.\")\n\n    # pathsToCheck can be a string or a list in case there are multiple paths\n    if isinstance(paths_to_check, list):\n        check_me = paths_to_check\n    else:\n        # assuming it's a str/path at this point\n        check_me = [paths_to_check]\n\n    out = {}\n    total_found = 0\n\n    for path in check_me:\n        logger.debug(f\"checking path: {path}\")\n        formatted_path = context.get_formatted_string(path)\n        found_paths = pypyr.utils.filesystem.get_glob(formatted_path)\n        no_of_paths = len(found_paths)\n        out[path] = {\n            'exists': no_of_paths > 0,\n            'count': no_of_paths,\n            'found': found_paths\n        }\n        total_found = total_found + no_of_paths\n\n    context['pathCheckOut'] = out\n\n    logger.info(f'checked {len(out)} path(s) and found {total_found}')\n    logger.debug(\"done\")",
    "reference": "pypyr step that checks if a file or directory path exists.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                 The following context key must exist\n                - pathsToCheck. str/path-like or list of str/paths.\n                                Path to file on disk to check.\n\n    All inputs support formatting expressions. Supports globs.\n\n    This step creates pathCheckOut in context, containing the results of the\n    path check operation.\n\n    pathCheckOut:\n        'inpath':\n            exists: true # bool. True if path exists.\n            count: 0 # int. Number of files found for in path.\n            found: ['path1', 'path2'] # list of strings. Paths of files found.\n\n    [count] is 0 if no files found. If you specified a single input\n    path to check and it exists, it's going to be 1. If you specified multiple\n    in paths or a glob expression that found more than 1 result, well, take a\n    guess.\n\n    [found] is a list of all the paths found for the [inpath]. If you passed\n    in a glob or globs, will contain the globs found for [inpath].\n\n    This means you can do an existence evaluation like this in a formatting\n    expression: '{pathCheckOut[inpathhere][exists]}'\n\n    Returns:\n        None. updates context arg.\n\n    Raises:\n        pypyr.errors.KeyNotInContextError: pathExists missing in context.\n        pypyr.errors.KeyInContextHasNoValueError: pathCheck exists but is None.",
    "generated": "Checks if a file or directory path exists.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                 The following context key must exist\n                - pathsToCheck. str/path-like or list of str/paths.\n                                Path to file on disk to check.\n\n    All inputs support formatting expressions. Supports globs.\n\n    This step creates pathCheckOut in context, containing the results of the\n    path check operation.\n\n    pathCheckOut:\n        'inpath':\n            exists: true # bool. True if path exists."
  },
  {
    "code": "def run_step(context):\n    \"\"\"Write payload out to json file.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                 The following context keys expected:\n                - fileWriteJson\n                    - path. mandatory. path-like. Write output file to\n                      here. Will create directories in path for you.\n                    - payload. optional. Write this key to output file. If not\n                      specified, output entire context.\n\n    Returns:\n        None.\n\n    Raises:\n        pypyr.errors.KeyNotInContextError: fileWriteJson or\n            fileWriteJson['path'] missing in context.\n        pypyr.errors.KeyInContextHasNoValueError: fileWriteJson or\n            fileWriteJson['path'] exists but is None.\n\n    \"\"\"\n    logger.debug(\"started\")\n    context.assert_child_key_has_value('fileWriteJson', 'path', __name__)\n\n    out_path = context.get_formatted_string(context['fileWriteJson']['path'])\n    # doing it like this to safeguard against accidentally dumping all context\n    # with potentially sensitive values in it to disk if payload exists but is\n    # None.\n    is_payload_specified = 'payload' in context['fileWriteJson']\n\n    logger.debug(f\"opening destination file for writing: {out_path}\")\n    os.makedirs(os.path.abspath(os.path.dirname(out_path)), exist_ok=True)\n    with open(out_path, 'w') as outfile:\n        if is_payload_specified:\n            payload = context['fileWriteJson']['payload']\n            formatted_iterable = context.get_formatted_iterable(payload)\n        else:\n            formatted_iterable = context.get_formatted_iterable(context)\n\n        json.dump(formatted_iterable, outfile, indent=2, ensure_ascii=False)\n\n    logger.info(f\"formatted context content and wrote to {out_path}\")\n    logger.debug(\"done\")",
    "reference": "Write payload out to json file.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                 The following context keys expected:\n                - fileWriteJson\n                    - path. mandatory. path-like. Write output file to\n                      here. Will create directories in path for you.\n                    - payload. optional. Write this key to output file. If not\n                      specified, output entire context.\n\n    Returns:\n        None.\n\n    Raises:\n        pypyr.errors.KeyNotInContextError: fileWriteJson or\n            fileWriteJson['path'] missing in context.\n        pypyr.errors.KeyInContextHasNoValueError: fileWriteJson or\n            fileWriteJson['path'] exists but is None.",
    "generated": "Write payload out to json file.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                 The following context keys expected:\n                - fileWriteJson\n                    - path. mandatory. path-like. Write this key to output file. If not\n                      specified, output entire context.\n\n    Returns:\n        None."
  },
  {
    "code": "def run_step(context):\n    \"\"\"Run another pipeline from this step.\n\n    The parent pipeline is the current, executing pipeline. The invoked, or\n    child pipeline is the pipeline you are calling from this step.\n\n    Args:\n        context: dictionary-like pypyr.context.Context. context is mandatory.\n                 Uses the following context keys in context:\n            - pype\n                - name. mandatory. str. Name of pipeline to execute. This\n                  {name}.yaml must exist in the working directory/pipelines\n                  dir.\n                - pipeArg. string. optional. String to pass to the\n                  context_parser - the equivalent to context arg on the\n                  pypyr cli. Only used if skipParse==False.\n                - raiseError. bool. optional. Defaults to True. If False, log,\n                  but swallow any errors that happen during the invoked\n                  pipeline execution. Swallowing means that the current/parent\n                  pipeline will carry on with the next step even if an error\n                  occurs in the invoked pipeline.\n                - skipParse. bool. optional. Defaults to True. skip the\n                  context_parser on the invoked pipeline.\n                - useParentContext. optional. bool. Defaults to True. Pass the\n                  current (i.e parent) pipeline context to the invoked (child)\n                  pipeline.\n                - loader: str. optional. Absolute name of pipeline loader\n                  module. If not specified will use\n                  pypyr.pypeloaders.fileloader.\n\n    Returns:\n        None\n\n    Raises:\n        pypyr.errors.KeyNotInContextError: if ['pype'] or ['pype']['name']\n                                           is missing.\n        pypyr.errors.KeyInContextHasNoValueError: ['pype']['name'] exists but\n                                                  is empty.\n    \"\"\"\n    logger.debug(\"started\")\n\n    (pipeline_name,\n     use_parent_context,\n     pipe_arg,\n     skip_parse,\n     raise_error,\n     loader,\n     ) = get_arguments(context)\n\n    try:\n        if use_parent_context:\n            logger.info(f\"pyping {pipeline_name}, using parent context.\")\n            pipelinerunner.load_and_run_pipeline(\n                pipeline_name=pipeline_name,\n                pipeline_context_input=pipe_arg,\n                context=context,\n                parse_input=not skip_parse,\n                loader=loader\n            )\n        else:\n            logger.info(f\"pyping {pipeline_name}, without parent context.\")\n            pipelinerunner.load_and_run_pipeline(\n                pipeline_name=pipeline_name,\n                pipeline_context_input=pipe_arg,\n                working_dir=context.working_dir,\n                parse_input=not skip_parse,\n                loader=loader\n            )\n\n        logger.info(f\"pyped {pipeline_name}.\")\n    except Exception as ex_info:\n        # yes, yes, don't catch Exception. Have to, though, in order to swallow\n        # errs if !raise_error\n        logger.error(f\"Something went wrong pyping {pipeline_name}. \"\n                     f\"{type(ex_info).__name__}: {ex_info}\")\n\n        if raise_error:\n            logger.debug(\"Raising original exception to caller.\")\n            raise\n        else:\n            logger.debug(\n                f\"raiseError is False. Swallowing error in {pipeline_name}.\")\n\n    logger.debug(\"done\")",
    "reference": "Run another pipeline from this step.\n\n    The parent pipeline is the current, executing pipeline. The invoked, or\n    child pipeline is the pipeline you are calling from this step.\n\n    Args:\n        context: dictionary-like pypyr.context.Context. context is mandatory.\n                 Uses the following context keys in context:\n            - pype\n                - name. mandatory. str. Name of pipeline to execute. This\n                  {name}.yaml must exist in the working directory/pipelines\n                  dir.\n                - pipeArg. string. optional. String to pass to the\n                  context_parser - the equivalent to context arg on the\n                  pypyr cli. Only used if skipParse==False.\n                - raiseError. bool. optional. Defaults to True. If False, log,\n                  but swallow any errors that happen during the invoked\n                  pipeline execution. Swallowing means that the current/parent\n                  pipeline will carry on with the next step even if an error\n                  occurs in the invoked pipeline.\n                - skipParse. bool. optional. Defaults to True. skip the\n                  context_parser on the invoked pipeline.\n                - useParentContext. optional. bool. Defaults to True. Pass the\n                  current (i.e parent) pipeline context to the invoked (child)\n                  pipeline.\n                - loader: str. optional. Absolute name of pipeline loader\n                  module. If not specified will use\n                  pypyr.pypeloaders.fileloader.\n\n    Returns:\n        None\n\n    Raises:\n        pypyr.errors.KeyNotInContextError: if ['pype'] or ['pype']['name']\n                                           is missing.\n        pypyr.errors.KeyInContextHasNoValueError: ['pype']['name'] exists but\n                                                  is empty.",
    "generated": "Run another pipeline from this step.\n\n    The parent pipeline is the current, executing pipeline. The invoked, or\n    child pipeline is the pipeline you are calling from this step.\n\n    Args:\n        context: dictionary-like pypyr.context.Context. context is mandatory.\n                 Uses the following context keys in context:\n            - pype\n                - name. mandatory. str. Name of pipeline to execute. This\n                  {name}.yaml must exist in the working directory/pipelines\n                  dir.\n                - pipeArg. string. string to pass to the\n                 "
  },
  {
    "code": "def get_arguments(context):\n    \"\"\"Parse arguments for pype from context and assign default values.\n\n    Args:\n        context: pypyr.context.Context. context is mandatory.\n\n    Returns:\n        tuple (pipeline_name, #str\n               use_parent_context, #bool\n               pipe_arg, #str\n               skip_parse, #bool\n               raise_error #bool\n               )\n\n    Raises:\n       pypyr.errors.KeyNotInContextError: if ['pype']['name'] is missing.\n       pypyr.errors.KeyInContextHasNoValueError: if ['pype']['name'] exists but\n                                                 is None.\n    \"\"\"\n    context.assert_key_has_value(key='pype', caller=__name__)\n    pype = context.get_formatted('pype')\n\n    try:\n        pipeline_name = pype['name']\n\n        if pipeline_name is None:\n            raise KeyInContextHasNoValueError(\n                \"pypyr.steps.pype ['pype']['name'] exists but is empty.\")\n    except KeyError as err:\n        raise KeyNotInContextError(\n            \"pypyr.steps.pype missing 'name' in the 'pype' context item. \"\n            \"You need to specify the pipeline name to run another \"\n            \"pipeline.\") from err\n\n    use_parent_context = pype.get('useParentContext', True)\n    pipe_arg = pype.get('pipeArg', None)\n    skip_parse = pype.get('skipParse', True)\n    raise_error = pype.get('raiseError', True)\n    loader = pype.get('loader', None)\n\n    return (\n        pipeline_name,\n        use_parent_context,\n        pipe_arg,\n        skip_parse,\n        raise_error,\n        loader,\n    )",
    "reference": "Parse arguments for pype from context and assign default values.\n\n    Args:\n        context: pypyr.context.Context. context is mandatory.\n\n    Returns:\n        tuple (pipeline_name, #str\n               use_parent_context, #bool\n               pipe_arg, #str\n               skip_parse, #bool\n               raise_error #bool\n               )\n\n    Raises:\n       pypyr.errors.KeyNotInContextError: if ['pype']['name'] is missing.\n       pypyr.errors.KeyInContextHasNoValueError: if ['pype']['name'] exists but\n                                                 is None.",
    "generated": "Parse arguments for pype from context and assign default values.\n\n    Args:\n        context: pypyr.context.Context. context is mandatory.\n\n    Returns:\n        tuple (pipeline_name, #str\n               pipe_arg, #str\n               skip_parse,"
  },
  {
    "code": "def get_pipeline_path(pipeline_name, working_directory):\n    \"\"\"Look for the pipeline in the various places it could be.\n\n    First checks the cwd. Then checks pypyr/pipelines dir.\n\n    Args:\n        pipeline_name: string. Name of pipeline to find\n        working_directory: string. Path in which to look for pipeline_name.yaml\n\n    Returns:\n        Absolute path to the pipeline_name.yaml file\n\n    Raises:\n        PipelineNotFoundError: if pipeline_name.yaml not found in working_dir\n                               or in {pypyr install dir}/pipelines.\n\n    \"\"\"\n    logger.debug(\"starting\")\n\n    # look for name.yaml in the pipelines/ sub-directory\n    logger.debug(f\"current directory is {working_directory}\")\n\n    # looking for {cwd}/pipelines/[pipeline_name].yaml\n    pipeline_path = os.path.abspath(os.path.join(\n        working_directory,\n        'pipelines',\n        pipeline_name + '.yaml'))\n\n    if os.path.isfile(pipeline_path):\n        logger.debug(f\"Found {pipeline_path}\")\n    else:\n        logger.debug(f\"{pipeline_name} not found in current \"\n                     \"directory/pipelines folder. Looking in pypyr install \"\n                     \"directory instead.\")\n        pypyr_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n        logger.debug(f\"pypyr installation directory is: {pypyr_dir}\")\n        pipeline_path = os.path.abspath(os.path.join(\n            pypyr_dir,\n            'pipelines',\n            pipeline_name + '.yaml'))\n\n        if os.path.isfile(pipeline_path):\n            logger.debug(f\"Found {pipeline_path}\")\n        else:\n            raise PipelineNotFoundError(f\"{pipeline_name}.yaml not found in \"\n                                        f\"either \"\n                                        f\"{working_directory}/pipelines \"\n                                        f\"or {pypyr_dir}/pipelines\")\n\n    logger.debug(\"done\")\n    return pipeline_path",
    "reference": "Look for the pipeline in the various places it could be.\n\n    First checks the cwd. Then checks pypyr/pipelines dir.\n\n    Args:\n        pipeline_name: string. Name of pipeline to find\n        working_directory: string. Path in which to look for pipeline_name.yaml\n\n    Returns:\n        Absolute path to the pipeline_name.yaml file\n\n    Raises:\n        PipelineNotFoundError: if pipeline_name.yaml not found in working_dir\n                               or in {pypyr install dir}/pipelines.",
    "generated": "Look for the pipeline in the various places it could be.\n\n    First checks the cwd. Then checks pypyr/pipelines dir.\n\n    Returns:\n        Absolute path to the pipeline_name.yaml file\n\n    Raises:\n        PipelineNotFoundError: if pipeline_name.yaml not found in working_dir\n                               or in {pypyr install dir}/pipelines.\n\n    Raises:\n        PipelineNotFoundError: if pipeline_name.yaml not found in working_dir\n                               or in {pypyr install dir}/pipelines."
  },
  {
    "code": "def get_pipeline_definition(pipeline_name, working_dir):\n    \"\"\"Open and parse the pipeline definition yaml.\n\n    Parses pipeline yaml and returns dictionary representing the pipeline.\n\n    pipeline_name.yaml should be in the working_dir/pipelines/ directory.\n\n    Args:\n        pipeline_name: string. Name of pipeline. This will be the file-name of\n                       the pipeline - i.e {pipeline_name}.yaml\n        working_dir: path. Start looking in\n                           ./working_dir/pipelines/pipeline_name.yaml\n\n    Returns:\n        dict describing the pipeline, parsed from the pipeline yaml.\n\n    Raises:\n        FileNotFoundError: pipeline_name.yaml not found in the various pipeline\n                           dirs.\n\n    \"\"\"\n    logger.debug(\"starting\")\n\n    pipeline_path = get_pipeline_path(\n        pipeline_name=pipeline_name,\n        working_directory=working_dir)\n\n    logger.debug(f\"Trying to open pipeline at path {pipeline_path}\")\n    try:\n        with open(pipeline_path) as yaml_file:\n            pipeline_definition = pypyr.yaml.get_pipeline_yaml(\n                yaml_file)\n            logger.debug(\n                f\"found {len(pipeline_definition)} stages in pipeline.\")\n    except FileNotFoundError:\n        logger.error(\n            \"The pipeline doesn't exist. Looking for a file here: \"\n            f\"{pipeline_name}.yaml in the /pipelines sub directory.\")\n        raise\n\n    logger.debug(\"pipeline definition loaded\")\n\n    logger.debug(\"done\")\n    return pipeline_definition",
    "reference": "Open and parse the pipeline definition yaml.\n\n    Parses pipeline yaml and returns dictionary representing the pipeline.\n\n    pipeline_name.yaml should be in the working_dir/pipelines/ directory.\n\n    Args:\n        pipeline_name: string. Name of pipeline. This will be the file-name of\n                       the pipeline - i.e {pipeline_name}.yaml\n        working_dir: path. Start looking in\n                           ./working_dir/pipelines/pipeline_name.yaml\n\n    Returns:\n        dict describing the pipeline, parsed from the pipeline yaml.\n\n    Raises:\n        FileNotFoundError: pipeline_name.yaml not found in the various pipeline\n                           dirs.",
    "generated": "Open and parse the pipeline definition yaml.\n\n    Parses pipeline yaml and returns dictionary representing the pipeline.\n\n    pipeline_name.yaml should be in the working_dir/pipelines/ directory.\n\n    Returns:\n        dict describing the pipeline, parsed from the pipeline yaml.\n\n    Raises:\n        FileNotFoundError: pipeline_name.yaml not found in the various pipeline\n                           dirs."
  },
  {
    "code": "def to_yaml(cls, representer, node):\n        \"\"\"How to serialize this class back to yaml.\"\"\"\n        return representer.represent_scalar(cls.yaml_tag, node.value)",
    "reference": "How to serialize this class back to yaml.",
    "generated": "How to serialize this class back to yaml."
  },
  {
    "code": "def get_value(self, context):\n        \"\"\"Run python eval on the input string.\"\"\"\n        if self.value:\n            return expressions.eval_string(self.value, context)\n        else:\n            # Empty input raises cryptic EOF syntax err, this more human\n            # friendly\n            raise ValueError('!py string expression is empty. It must be a '\n                             'valid python expression instead.')",
    "reference": "Run python eval on the input string.",
    "generated": "Run python eval on the input string."
  },
  {
    "code": "def foreach_loop(self, context):\n        \"\"\"Run step once for each item in foreach_items.\n\n        On each iteration, the invoked step can use context['i'] to get the\n        current iterator value.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate.\n        \"\"\"\n        logger.debug(\"starting\")\n\n        # Loop decorators only evaluated once, not for every step repeat\n        # execution.\n        foreach = context.get_formatted_iterable(self.foreach_items)\n\n        foreach_length = len(foreach)\n\n        logger.info(f\"foreach decorator will loop {foreach_length} times.\")\n\n        for i in foreach:\n            logger.info(f\"foreach: running step {i}\")\n            # the iterator must be available to the step when it executes\n            context['i'] = i\n            # conditional operators apply to each iteration, so might be an\n            # iteration run, skips or swallows.\n            self.run_conditional_decorators(context)\n            logger.debug(f\"foreach: done step {i}\")\n\n        logger.debug(f\"foreach decorator looped {foreach_length} times.\")\n        logger.debug(\"done\")",
    "reference": "Run step once for each item in foreach_items.\n\n        On each iteration, the invoked step can use context['i'] to get the\n        current iterator value.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate.",
    "generated": "Run step once for each item in foreach_items.\n\n        On each iteration, the invoked step can use context['i'] to get the\n        current iterator value.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate."
  },
  {
    "code": "def invoke_step(self, context):\n        \"\"\"Invoke 'run_step' in the dynamically loaded step module.\n\n        Don't invoke this from outside the Step class. Use\n        pypyr.dsl.Step.run_step instead.\n        invoke_step just does the bare module step invocation, it does not\n        evaluate any of the decorator logic surrounding the step. So unless\n        you really know what you're doing, use run_step if you intend on\n        executing the step the same way pypyr does.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate.\n        \"\"\"\n        logger.debug(\"starting\")\n\n        logger.debug(f\"running step {self.module}\")\n\n        self.run_step_function(context)\n\n        logger.debug(f\"step {self.module} done\")",
    "reference": "Invoke 'run_step' in the dynamically loaded step module.\n\n        Don't invoke this from outside the Step class. Use\n        pypyr.dsl.Step.run_step instead.\n        invoke_step just does the bare module step invocation, it does not\n        evaluate any of the decorator logic surrounding the step. So unless\n        you really know what you're doing, use run_step if you intend on\n        executing the step the same way pypyr does.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate.",
    "generated": "Invoke 'run_step' in the dynamically loaded step module.\n\n        Don't invoke this from outside the Step class. Use\n        pypyr.dsl.Step.run_step instead.\n        invoke_step just does the bare module step invocation, it does not\n        evaluate any of the decorator logic surrounding the step. So unless\n        you really know what you're doing, use run_step if you intend on\n        executing the step the same way pypyr does."
  },
  {
    "code": "def run_conditional_decorators(self, context):\n        \"\"\"Evaluate the step decorators to decide whether to run step or not.\n\n        Use pypyr.dsl.Step.run_step if you intend on executing the step the\n        same way pypyr does.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate.\n        \"\"\"\n        logger.debug(\"starting\")\n\n        # The decorator attributes might contain formatting expressions that\n        # change whether they evaluate True or False, thus apply formatting at\n        # last possible instant.\n        run_me = context.get_formatted_as_type(self.run_me, out_type=bool)\n        skip_me = context.get_formatted_as_type(self.skip_me, out_type=bool)\n        swallow_me = context.get_formatted_as_type(self.swallow_me,\n                                                   out_type=bool)\n\n        if run_me:\n            if not skip_me:\n                try:\n                    if self.retry_decorator:\n                        self.retry_decorator.retry_loop(context,\n                                                        self.invoke_step)\n                    else:\n                        self.invoke_step(context=context)\n                except Exception as ex_info:\n                    if swallow_me:\n                        logger.error(\n                            f\"{self.name} Ignoring error because swallow \"\n                            \"is True for this step.\\n\"\n                            f\"{type(ex_info).__name__}: {ex_info}\")\n                    else:\n                        raise\n            else:\n                logger.info(\n                    f\"{self.name} not running because skip is True.\")\n        else:\n            logger.info(f\"{self.name} not running because run is False.\")\n\n        logger.debug(\"done\")",
    "reference": "Evaluate the step decorators to decide whether to run step or not.\n\n        Use pypyr.dsl.Step.run_step if you intend on executing the step the\n        same way pypyr does.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate.",
    "generated": "Evaluate the step decorators to decide whether to run step or not.\n\n        Use pypyr.dsl.Step.run_step if you intend on executing the step the\n        same way pypyr does.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate."
  },
  {
    "code": "def run_foreach_or_conditional(self, context):\n        \"\"\"Run the foreach sequence or the conditional evaluation.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate.\n        \"\"\"\n        logger.debug(\"starting\")\n        # friendly reminder [] list obj (i.e empty) evals False\n        if self.foreach_items:\n            self.foreach_loop(context)\n        else:\n            # since no looping required, don't pollute output with looping info\n            self.run_conditional_decorators(context)\n\n        logger.debug(\"done\")",
    "reference": "Run the foreach sequence or the conditional evaluation.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate.",
    "generated": "Run the foreach sequence or the conditional evaluation.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate."
  },
  {
    "code": "def run_step(self, context):\n        \"\"\"Run a single pipeline step.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate.\n        \"\"\"\n        logger.debug(\"starting\")\n        # the in params should be added to context before step execution.\n        self.set_step_input_context(context)\n\n        if self.while_decorator:\n            self.while_decorator.while_loop(context,\n                                            self.run_foreach_or_conditional)\n        else:\n            self.run_foreach_or_conditional(context)\n\n        logger.debug(\"done\")",
    "reference": "Run a single pipeline step.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate.",
    "generated": "Run a pipeline step.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate."
  },
  {
    "code": "def set_step_input_context(self, context):\n        \"\"\"Append step's 'in' parameters to context, if they exist.\n\n        Append the[in] dictionary to the context. This will overwrite\n        existing values if the same keys are already in there. I.e if\n        in_parameters has {'eggs': 'boiled'} and key 'eggs' already\n        exists in context, context['eggs'] hereafter will be 'boiled'.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate - after method execution will contain the new\n                     updated context.\n        \"\"\"\n        logger.debug(\"starting\")\n        if self.in_parameters is not None:\n            parameter_count = len(self.in_parameters)\n            if parameter_count > 0:\n                logger.debug(\n                    f\"Updating context with {parameter_count} 'in' \"\n                    \"parameters.\")\n                context.update(self.in_parameters)\n\n        logger.debug(\"done\")",
    "reference": "Append step's 'in' parameters to context, if they exist.\n\n        Append the[in] dictionary to the context. This will overwrite\n        existing values if the same keys are already in there. I.e if\n        in_parameters has {'eggs': 'boiled'} and key 'eggs' already\n        exists in context, context['eggs'] hereafter will be 'boiled'.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate - after method execution will contain the new\n                     updated context.",
    "generated": "Append step's 'in' parameters to context, if they exist.\n\n        Append the[in] dictionary to the context. This will overwrite\n        existing values if the same keys are already in there. I.e if\n        in_parameters has {'eggs': 'boiled'} and key 'eggs' already\n        exists in context, context['eggs'] hereafter will be 'boiled'."
  },
  {
    "code": "def exec_iteration(self, counter, context, step_method):\n        \"\"\"Run a single retry iteration.\n\n        This method abides by the signature invoked by poll.while_until_true,\n        which is to say (counter, *args, **kwargs). In a normal execution\n        chain, this method's args passed by self.retry_loop where context\n        and step_method set. while_until_true injects counter as a 1st arg.\n\n        Args:\n            counter. int. loop counter, which number of iteration is this.\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate - after method execution will contain the new\n                     updated context.\n            step_method: (method/function) This is the method/function that\n                         will execute on every loop iteration. Signature is:\n                         function(context)\n\n         Returns:\n            bool. True if step execution completed without error.\n                  False if error occured during step execution.\n\n        \"\"\"\n        logger.debug(\"starting\")\n        context['retryCounter'] = counter\n\n        logger.info(f\"retry: running step with counter {counter}\")\n        try:\n            step_method(context)\n            result = True\n        except Exception as ex_info:\n            if self.max:\n                if counter == self.max:\n                    logger.debug(f\"retry: max {counter} retries exhausted. \"\n                                 \"raising error.\")\n                    # arguably shouldn't be using errs for control of flow.\n                    # but would lose the err info if not, so lesser of 2 evils.\n                    raise\n\n            if self.stop_on or self.retry_on:\n                error_name = get_error_name(ex_info)\n                if self.stop_on:\n                    formatted_stop_list = context.get_formatted_iterable(\n                        self.stop_on)\n                    if error_name in formatted_stop_list:\n                        logger.error(f\"{error_name} in stopOn. Raising error \"\n                                     \"and exiting retry.\")\n                        raise\n                    else:\n                        logger.debug(f\"{error_name} not in stopOn. Continue.\")\n\n                if self.retry_on:\n                    formatted_retry_list = context.get_formatted_iterable(\n                        self.retry_on)\n                    if error_name not in formatted_retry_list:\n                        logger.error(f\"{error_name} not in retryOn. Raising \"\n                                     \"error and exiting retry.\")\n                        raise\n                    else:\n                        logger.debug(f\"{error_name} in retryOn. Retry again.\")\n\n            result = False\n            logger.error(f\"retry: ignoring error because retryCounter < max.\\n\"\n                         f\"{type(ex_info).__name__}: {ex_info}\")\n\n        logger.debug(f\"retry: done step with counter {counter}\")\n\n        logger.debug(\"done\")\n        return result",
    "reference": "Run a single retry iteration.\n\n        This method abides by the signature invoked by poll.while_until_true,\n        which is to say (counter, *args, **kwargs). In a normal execution\n        chain, this method's args passed by self.retry_loop where context\n        and step_method set. while_until_true injects counter as a 1st arg.\n\n        Args:\n            counter. int. loop counter, which number of iteration is this.\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate - after method execution will contain the new\n                     updated context.\n            step_method: (method/function) This is the method/function that\n                         will execute on every loop iteration. Signature is:\n                         function(context)\n\n         Returns:\n            bool. True if step execution completed without error.\n                  False if error occured during step execution.",
    "generated": "Run a single retry iteration.\n\n        This method abides by the signature invoked by poll.while_until_true,\n        which is to say (counter, *args, **kwargs). In a normal execution\n        chain, this method's args passed by self.retry_loop where context\n        and step_method set. while_until_true injects counter as a 1st arg.\n\n        Args:\n            counter. int. loop counter, which number of iteration is this.\n            context: (pypyr.context.Context) The pypyr context. This arg will\n"
  },
  {
    "code": "def retry_loop(self, context, step_method):\n        \"\"\"Run step inside a retry loop.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate - after method execution will contain the new\n                     updated context.\n            step_method: (method/function) This is the method/function that\n                         will execute on every loop iteration. Signature is:\n                         function(context)\n\n        \"\"\"\n        logger.debug(\"starting\")\n\n        context['retryCounter'] = 0\n\n        sleep = context.get_formatted_as_type(self.sleep, out_type=float)\n        if self.max:\n            max = context.get_formatted_as_type(self.max, out_type=int)\n\n            logger.info(f\"retry decorator will try {max} times at {sleep}s \"\n                        \"intervals.\")\n        else:\n            max = None\n            logger.info(f\"retry decorator will try indefinitely at {sleep}s \"\n                        \"intervals.\")\n\n        # this will never be false. because on counter == max,\n        # exec_iteration raises an exception, breaking out of the loop.\n        # pragma because cov doesn't know the implied else is impossible.\n        # unit test cov is 100%, though.\n        if poll.while_until_true(interval=sleep,\n                                 max_attempts=max)(\n                self.exec_iteration)(context=context,\n                                     step_method=step_method\n                                     ):  # pragma: no cover\n            logger.debug(\"retry loop complete, reporting success.\")\n\n        logger.debug(\"retry loop done\")\n\n        logger.debug(\"done\")",
    "reference": "Run step inside a retry loop.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate - after method execution will contain the new\n                     updated context.\n            step_method: (method/function) This is the method/function that\n                         will execute on every loop iteration. Signature is:\n                         function(context)",
    "generated": "Run step inside a retry loop.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate - after method execution will contain the new\n                     updated context.\n            step_method: (method/function) This is the method/function that\n                         will execute on every loop iteration. Signature is:\n                         function(context)"
  },
  {
    "code": "def exec_iteration(self, counter, context, step_method):\n        \"\"\"Run a single loop iteration.\n\n        This method abides by the signature invoked by poll.while_until_true,\n        which is to say (counter, *args, **kwargs). In a normal execution\n        chain, this method's args passed by self.while_loop where context\n        and step_method set. while_until_true injects counter as a 1st arg.\n\n        Args:\n            counter. int. loop counter, which number of iteration is this.\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate - after method execution will contain the new\n                     updated context.\n            step_method: (method/function) This is the method/function that\n                         will execute on every loop iteration. Signature is:\n                         function(context)\n\n         Returns:\n            bool. True if self.stop evaluates to True after step execution,\n                  False otherwise.\n\n        \"\"\"\n        logger.debug(\"starting\")\n        context['whileCounter'] = counter\n\n        logger.info(f\"while: running step with counter {counter}\")\n        step_method(context)\n        logger.debug(f\"while: done step {counter}\")\n\n        result = False\n        # if no stop, just iterating to max)\n        if self.stop:\n            # dynamically evaluate stop after step execution, since the step\n            # might have changed True/False status for stop.\n            result = context.get_formatted_as_type(self.stop, out_type=bool)\n\n        logger.debug(\"done\")\n        return result",
    "reference": "Run a single loop iteration.\n\n        This method abides by the signature invoked by poll.while_until_true,\n        which is to say (counter, *args, **kwargs). In a normal execution\n        chain, this method's args passed by self.while_loop where context\n        and step_method set. while_until_true injects counter as a 1st arg.\n\n        Args:\n            counter. int. loop counter, which number of iteration is this.\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate - after method execution will contain the new\n                     updated context.\n            step_method: (method/function) This is the method/function that\n                         will execute on every loop iteration. Signature is:\n                         function(context)\n\n         Returns:\n            bool. True if self.stop evaluates to True after step execution,\n                  False otherwise.",
    "generated": "Run a single loop iteration.\n\n        This method abides by the signature invoked by poll.while_until_true,\n        which is to say (counter, *args, **kwargs). In a normal execution\n        chain, this method's args passed by self.while_loop where context\n        and step_method set. while_until_true injects counter as a 1st arg.\n\n        Args:\n            counter. int. loop counter, which number of iteration is this.\n            context: (pypyr.context.Context) The pypyr context. This arg will\n"
  },
  {
    "code": "def while_loop(self, context, step_method):\n        \"\"\"Run step inside a while loop.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate - after method execution will contain the new\n                     updated context.\n            step_method: (method/function) This is the method/function that\n                         will execute on every loop iteration. Signature is:\n                         function(context)\n\n        \"\"\"\n        logger.debug(\"starting\")\n\n        context['whileCounter'] = 0\n\n        if self.stop is None and self.max is None:\n            # the ctor already does this check, but guess theoretically\n            # consumer could have messed with the props since ctor\n            logger.error(f\"while decorator missing both max and stop.\")\n            raise PipelineDefinitionError(\"the while decorator must have \"\n                                          \"either max or stop, or both. \"\n                                          \"But not neither.\")\n\n        error_on_max = context.get_formatted_as_type(\n            self.error_on_max, out_type=bool)\n        sleep = context.get_formatted_as_type(self.sleep, out_type=float)\n        if self.max is None:\n            max = None\n            logger.info(f\"while decorator will loop until {self.stop} \"\n                        f\"evaluates to True at {sleep}s intervals.\")\n        else:\n            max = context.get_formatted_as_type(self.max, out_type=int)\n\n            if max < 1:\n                logger.info(\n                    f\"max {self.max} is {max}. while only runs when max > 0.\")\n                logger.debug(\"done\")\n                return\n\n            if self.stop is None:\n                logger.info(f\"while decorator will loop {max} times at \"\n                            f\"{sleep}s intervals.\")\n            else:\n                logger.info(f\"while decorator will loop {max} times, or \"\n                            f\"until {self.stop} evaluates to True at \"\n                            f\"{sleep}s intervals.\")\n\n        if not poll.while_until_true(interval=sleep,\n                                     max_attempts=max)(\n                self.exec_iteration)(context=context,\n                                     step_method=step_method):\n            # False means loop exhausted and stop never eval-ed True.\n            if error_on_max:\n                logger.error(f\"exhausted {max} iterations of while loop, \"\n                             \"and errorOnMax is True.\")\n                if self.stop and max:\n                    raise LoopMaxExhaustedError(\"while loop reached \"\n                                                f\"{max} and {self.stop} \"\n                                                \"never evaluated to True.\")\n                else:\n                    raise LoopMaxExhaustedError(f\"while loop reached {max}.\")\n            else:\n                if self.stop and max:\n                    logger.info(\n                        f\"while decorator looped {max} times, \"\n                        f\"and {self.stop} never evaluated to True.\")\n\n            logger.debug(\"while loop done\")\n        else:\n            logger.info(f\"while loop done, stop condition {self.stop} \"\n                        \"evaluated True.\")\n\n        logger.debug(\"done\")",
    "reference": "Run step inside a while loop.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate - after method execution will contain the new\n                     updated context.\n            step_method: (method/function) This is the method/function that\n                         will execute on every loop iteration. Signature is:\n                         function(context)",
    "generated": "Run step inside a while loop.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate - after method execution will contain the new\n                     updated context.\n            step_method: (method/function) This is the method/function that\n                         will execute on every loop iteration. Signature is:\n                         function(context)"
  },
  {
    "code": "def run_step(context):\n    \"\"\"Load a yaml file into the pypyr context.\n\n    Yaml parsed from the file will be merged into the pypyr context. This will\n    overwrite existing values if the same keys are already in there.\n    I.e if file yaml has {'eggs' : 'boiled'} and context {'eggs': 'fried'}\n    already exists, returned context['eggs'] will be 'boiled'.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                 The following context key must exist\n                - fetchYaml\n                    - path. path-like. Path to file on disk.\n                    - key. string. If exists, write yaml to this context key.\n                      Else yaml writes to context root.\n\n    All inputs support formatting expressions.\n\n    Also supports a passing path as string to fetchYaml, but in this case you\n    won't be able to specify a key.\n\n    Returns:\n        None. updates context arg.\n\n    Raises:\n        FileNotFoundError: take a guess\n        pypyr.errors.KeyNotInContextError: fetchYamlPath missing in context.\n        pypyr.errors.KeyInContextHasNoValueError: fetchYamlPath exists but is\n                                                  None.\n\n    \"\"\"\n    logger.debug(\"started\")\n\n    deprecated(context)\n\n    context.assert_key_has_value(key='fetchYaml', caller=__name__)\n\n    fetch_yaml_input = context.get_formatted('fetchYaml')\n\n    if isinstance(fetch_yaml_input, str):\n        file_path = fetch_yaml_input\n        destination_key_expression = None\n    else:\n        context.assert_child_key_has_value(parent='fetchYaml',\n                                           child='path',\n                                           caller=__name__)\n        file_path = fetch_yaml_input['path']\n        destination_key_expression = fetch_yaml_input.get('key', None)\n\n    logger.debug(f\"attempting to open file: {file_path}\")\n    with open(file_path) as yaml_file:\n        yaml_loader = yaml.YAML(typ='safe', pure=True)\n        payload = yaml_loader.load(yaml_file)\n\n    if destination_key_expression:\n        destination_key = context.get_formatted_iterable(\n            destination_key_expression)\n        logger.debug(f\"yaml file loaded. Writing to context {destination_key}\")\n        context[destination_key] = payload\n    else:\n        if not isinstance(payload, MutableMapping):\n            raise TypeError(\n                \"yaml input should describe a dictionary at the top \"\n                \"level when fetchYamlKey isn't specified. You should have \"\n                \"something like \\n'key1: value1'\\n key2: value2'\\n\"\n                \"in the yaml top-level, not \\n'- value1\\n - value2'\")\n\n        logger.debug(\"yaml file loaded. Merging into pypyr context. . .\")\n        context.update(payload)\n\n    logger.info(f\"yaml file written into pypyr context. Count: {len(payload)}\")\n    logger.debug(\"done\")",
    "reference": "Load a yaml file into the pypyr context.\n\n    Yaml parsed from the file will be merged into the pypyr context. This will\n    overwrite existing values if the same keys are already in there.\n    I.e if file yaml has {'eggs' : 'boiled'} and context {'eggs': 'fried'}\n    already exists, returned context['eggs'] will be 'boiled'.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                 The following context key must exist\n                - fetchYaml\n                    - path. path-like. Path to file on disk.\n                    - key. string. If exists, write yaml to this context key.\n                      Else yaml writes to context root.\n\n    All inputs support formatting expressions.\n\n    Also supports a passing path as string to fetchYaml, but in this case you\n    won't be able to specify a key.\n\n    Returns:\n        None. updates context arg.\n\n    Raises:\n        FileNotFoundError: take a guess\n        pypyr.errors.KeyNotInContextError: fetchYamlPath missing in context.\n        pypyr.errors.KeyInContextHasNoValueError: fetchYamlPath exists but is\n                                                  None.",
    "generated": "Load a yaml file into the pypyr context.\n\n    Yaml parsed from the file will be merged into the pypyr context. This will\n    overwrite existing values if the same keys are already in there.\n    I.e if file yaml has {'eggs': 'boiled'} and context {'eggs': 'fried'}\n    already exists, returned context['eggs'] will be 'boiled'.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                 The following context key must exist\n                - fetchYaml\n                    - path."
  },
  {
    "code": "def run_step(context):\n    \"\"\"pypyr step saves current utc datetime to context.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                 The following context key is optional:\n                - nowUtcIn. str. Datetime formatting expression. For full list\n                  of possible expressions, check here:\n                  https://docs.python.org/3.7/library/datetime.html#strftime-and-strptime-behavior\n\n    All inputs support pypyr formatting expressions.\n\n    This step creates now in context, containing a string representation of the\n    timestamp. If input formatting not specified, defaults to ISO8601.\n\n    Default is:\n    YYYY-MM-DDTHH:MM:SS.ffffff+00:00, or, if microsecond is 0,\n    YYYY-MM-DDTHH:MM:SS\n\n    Returns:\n        None. updates context arg.\n\n    \"\"\"\n    logger.debug(\"started\")\n\n    format_expression = context.get('nowUtcIn', None)\n\n    if format_expression:\n        formatted_expression = context.get_formatted_string(format_expression)\n        context['nowUtc'] = datetime.now(\n            timezone.utc).strftime(formatted_expression)\n    else:\n        context['nowUtc'] = datetime.now(timezone.utc).isoformat()\n\n    logger.info(f\"timestamp {context['nowUtc']} saved to context nowUtc\")\n    logger.debug(\"done\")",
    "reference": "pypyr step saves current utc datetime to context.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                 The following context key is optional:\n                - nowUtcIn. str. Datetime formatting expression. For full list\n                  of possible expressions, check here:\n                  https://docs.python.org/3.7/library/datetime.html#strftime-and-strptime-behavior\n\n    All inputs support pypyr formatting expressions.\n\n    This step creates now in context, containing a string representation of the\n    timestamp. If input formatting not specified, defaults to ISO8601.\n\n    Default is:\n    YYYY-MM-DDTHH:MM:SS.ffffff+00:00, or, if microsecond is 0,\n    YYYY-MM-DDTHH:MM:SS\n\n    Returns:\n        None. updates context arg.",
    "generated": "The following context key is optional:\n                - nowUtcIn. str. Datetime formatting expression. For full list\n                  of possible expressions, check here:\n                  https://docs.python.org/3.7/library/datetime.html#strftime-and-strptime-behavior\n\n    All inputs support pypyr formatting expressions.\n\n    This step creates now in context, containing a string representation of the\n    timestamp. If input formatting not specified, defaults to ISO8601.\n\n    Default is:\n    YYYY-MM-DDTHH:SS.ffffff+00:"
  },
  {
    "code": "def run_step(context):\n    \"\"\"Assert that something is True or equal to something else.\n\n    Args:\n        context: dictionary-like pypyr.context.Context. context is mandatory.\n        Uses the following context keys in context:\n            - assert\n                - this. mandatory. Any type. If assert['equals'] not specified,\n                  evals as boolean.\n                - equals. optional. Any type.\n\n    If assert['this'] evaluates to False raises error.\n    If assert['equals'] is specified, raises error if\n    assert.this != assert.equals.\n\n    assert['this'] & assert['equals'] both support string substitutions.\n\n    Returns:\n        None\n\n    Raises:\n        ContextError: if assert evaluates to False.\n\n    \"\"\"\n    logger.debug(\"started\")\n    assert context, f\"context must have value for {__name__}\"\n\n    deprecated(context)\n    context.assert_key_has_value('assert', __name__)\n\n    assert_this = context['assert']['this']\n    is_equals_there = 'equals' in context['assert']\n    if is_equals_there:\n        assert_equals = context['assert']['equals']\n        # compare assertThis to assertEquals\n        logger.debug(\"comparing assert['this'] to assert['equals'].\")\n        assert_result = (context.get_formatted_iterable(assert_this)\n                         == context.get_formatted_iterable(assert_equals))\n    else:\n        # nothing to compare means treat assertThis as a bool.\n        logger.debug(\"evaluating assert['this'] as a boolean.\")\n        assert_result = context.get_formatted_as_type(assert_this,\n                                                      out_type=bool)\n\n    logger.info(f\"assert evaluated to {assert_result}\")\n\n    if not assert_result:\n        if is_equals_there:\n            # emit type to help user, but not the actual field contents.\n            type_this = (\n                type(context.get_formatted_iterable(assert_this)).__name__)\n            type_equals = (\n                type(context.get_formatted_iterable(assert_equals)).__name__)\n            error_text = (\n                f\"assert assert['this'] is of type {type_this} \"\n                f\"and does not equal assert['equals'] of type {type_equals}.\")\n        else:\n            # if it's a bool it's presumably not a sensitive value.\n            error_text = (\n                f\"assert {assert_this} evaluated to False.\")\n        raise ContextError(error_text)\n\n    logger.debug(\"done\")",
    "reference": "Assert that something is True or equal to something else.\n\n    Args:\n        context: dictionary-like pypyr.context.Context. context is mandatory.\n        Uses the following context keys in context:\n            - assert\n                - this. mandatory. Any type. If assert['equals'] not specified,\n                  evals as boolean.\n                - equals. optional. Any type.\n\n    If assert['this'] evaluates to False raises error.\n    If assert['equals'] is specified, raises error if\n    assert.this != assert.equals.\n\n    assert['this'] & assert['equals'] both support string substitutions.\n\n    Returns:\n        None\n\n    Raises:\n        ContextError: if assert evaluates to False.",
    "generated": "Assert that something is True or equal to something else.\n\n    Args:\n        context: dictionary-like pypyr.context.Context. context is mandatory.\n        Uses the following context keys in context:\n            - this. mandatory. Any type. If assert['equals'] not specified,\n                  evals as boolean.\n                - equals. optional. Any type."
  },
  {
    "code": "def tar_archive(context):\n    \"\"\"Archive specified path to a tar archive.\n\n    Args:\n        context: dictionary-like. context is mandatory.\n            context['tar']['archive'] must exist. It's a dictionary.\n            keys are the paths to archive.\n            values are the destination output paths.\n\n    Example:\n        tar:\n            archive:\n                - in: path/to/dir\n                  out: path/to/destination.tar.xs\n                - in: another/my.file\n                  out: ./my.tar.xs\n\n        This will archive directory path/to/dir to path/to/destination.tar.xs,\n        and also archive file another/my.file to ./my.tar.xs\n    \"\"\"\n    logger.debug(\"start\")\n\n    mode = get_file_mode_for_writing(context)\n\n    for item in context['tar']['archive']:\n        # value is the destination tar. Allow string interpolation.\n        destination = context.get_formatted_string(item['out'])\n        # key is the source to archive\n        source = context.get_formatted_string(item['in'])\n        with tarfile.open(destination, mode) as archive_me:\n            logger.debug(f\"Archiving '{source}' to '{destination}'\")\n\n            archive_me.add(source, arcname='.')\n            logger.info(f\"Archived '{source}' to '{destination}'\")\n\n    logger.debug(\"end\")",
    "reference": "Archive specified path to a tar archive.\n\n    Args:\n        context: dictionary-like. context is mandatory.\n            context['tar']['archive'] must exist. It's a dictionary.\n            keys are the paths to archive.\n            values are the destination output paths.\n\n    Example:\n        tar:\n            archive:\n                - in: path/to/dir\n                  out: path/to/destination.tar.xs\n                - in: another/my.file\n                  out: ./my.tar.xs\n\n        This will archive directory path/to/dir to path/to/destination.tar.xs,\n        and also archive file another/my.file to ./my.tar.xs",
    "generated": "Archive specified path to a tar archive.\n\n    Args:\n        context: dictionary-like. context is mandatory.\n            context['tar']['archive'] must exist. It's a dictionary.\n            keys are the paths to archive.\n            values are the destination output paths.\n\n    Example:\n        tar:\n            archive:\n                - in: path/to/dir\n                  out: path/to/destination.tar.xs\n                - in: another/my.file\n                  out: ./my.file\n        This will archive directory path/to/dir\n"
  },
  {
    "code": "def tar_extract(context):\n    \"\"\"Extract all members of tar archive to specified path.\n\n    Args:\n        context: dictionary-like. context is mandatory.\n            context['tar']['extract'] must exist. It's a dictionary.\n            keys are the path to the tar to extract.\n            values are the destination paths.\n\n    Example:\n        tar:\n            extract:\n                - in: path/to/my.tar.xs\n                  out: /path/extract/here\n                - in: another/tar.xs\n                  out: .\n\n        This will extract path/to/my.tar.xs to /path/extract/here, and also\n        extract another/tar.xs to $PWD.\n    \"\"\"\n    logger.debug(\"start\")\n\n    mode = get_file_mode_for_reading(context)\n\n    for item in context['tar']['extract']:\n        # in is the path to the tar to extract. Allows string interpolation.\n        source = context.get_formatted_string(item['in'])\n        # out is the outdir, dhur. Allows string interpolation.\n        destination = context.get_formatted_string(item['out'])\n        with tarfile.open(source, mode) as extract_me:\n            logger.debug(f\"Extracting '{source}' to '{destination}'\")\n\n            extract_me.extractall(destination)\n            logger.info(f\"Extracted '{source}' to '{destination}'\")\n\n    logger.debug(\"end\")",
    "reference": "Extract all members of tar archive to specified path.\n\n    Args:\n        context: dictionary-like. context is mandatory.\n            context['tar']['extract'] must exist. It's a dictionary.\n            keys are the path to the tar to extract.\n            values are the destination paths.\n\n    Example:\n        tar:\n            extract:\n                - in: path/to/my.tar.xs\n                  out: /path/extract/here\n                - in: another/tar.xs\n                  out: .\n\n        This will extract path/to/my.tar.xs to /path/extract/here, and also\n        extract another/tar.xs to $PWD.",
    "generated": "Extract all members of tar archive to specified path.\n\n    Args:\n        context: dictionary-like. context is mandatory.\n            context['tar']['extract'] must exist. It's a dictionary.\n            keys are the path to the tar to extract.\n            values are the destination paths.\n\n    Example:\n        tar:\n            extract:\n                - in: path/to/my.tar.xs\n                  out: /path/extract/here\n                - in: another/tar.xs\n                  out: ."
  },
  {
    "code": "def run_step(context):\n    \"\"\"Run shell command without shell interpolation.\n\n    Context is a dictionary or dictionary-like.\n\n    Context must contain the following keys:\n    cmd: <<cmd string>> (command + args to execute.)\n\n    OR, as a dict\n    cmd:\n        run: str. mandatory. <<cmd string>> command + args to execute.\n        save: bool. defaults False. save output to cmdOut.\n\n    Will execute command string in the shell as a sub-process.\n    The shell defaults to /bin/sh.\n    The context['cmd'] string must be formatted exactly as it would be when\n    typed at the shell prompt. This includes, for example, quoting or backslash\n    escaping filenames with spaces in them.\n    There is an exception to this: Escape curly braces: if you want a literal\n    curly brace, double it like {{ or }}.\n\n    If save is True, will save the output to context as follows:\n        cmdOut:\n            returncode: 0\n            stdout: 'stdout str here. None if empty.'\n            stderr: 'stderr str here. None if empty.'\n\n    cmdOut.returncode is the exit status of the called process. Typically 0\n    means OK. A negative value -N indicates that the child was terminated by\n    signal N (POSIX only).\n\n    context['cmd'] will interpolate anything in curly braces for values\n    found in context. So if your context looks like this:\n        key1: value1\n        key2: value2\n        cmd: mything --arg1 {key1}\n\n    The cmd passed to the shell will be \"mything --arg value1\"\n    \"\"\"\n    logger.debug(\"started\")\n\n    CmdStep(name=__name__, context=context).run_step(is_shell=True)\n\n    logger.debug(\"done\")",
    "reference": "Run shell command without shell interpolation.\n\n    Context is a dictionary or dictionary-like.\n\n    Context must contain the following keys:\n    cmd: <<cmd string>> (command + args to execute.)\n\n    OR, as a dict\n    cmd:\n        run: str. mandatory. <<cmd string>> command + args to execute.\n        save: bool. defaults False. save output to cmdOut.\n\n    Will execute command string in the shell as a sub-process.\n    The shell defaults to /bin/sh.\n    The context['cmd'] string must be formatted exactly as it would be when\n    typed at the shell prompt. This includes, for example, quoting or backslash\n    escaping filenames with spaces in them.\n    There is an exception to this: Escape curly braces: if you want a literal\n    curly brace, double it like {{ or }}.\n\n    If save is True, will save the output to context as follows:\n        cmdOut:\n            returncode: 0\n            stdout: 'stdout str here. None if empty.'\n            stderr: 'stderr str here. None if empty.'\n\n    cmdOut.returncode is the exit status of the called process. Typically 0\n    means OK. A negative value -N indicates that the child was terminated by\n    signal N (POSIX only).\n\n    context['cmd'] will interpolate anything in curly braces for values\n    found in context. So if your context looks like this:\n        key1: value1\n        key2: value2\n        cmd: mything --arg1 {key1}\n\n    The cmd passed to the shell will be \"mything --arg value1\"",
    "generated": "Run shell command without shell interpolation.\n\n    Context is a dictionary or dictionary-like.\n\n    Context must contain the following keys:\n    cmd: <<cmd string>> (command + args to execute.)\n\n    OR, as a dict\n    cmd:\n        run: str. mandatory. <<cmd string>> (command + args to execute.)\n        save: bool. defaults False. save output to cmdOut.\n\n    Will execute command string in the shell as a sub-process.\n   "
  },
  {
    "code": "def get_args(get_item):\n    \"\"\"Parse env, key, default out of input dict.\n\n    Args:\n        get_item: dict. contains keys env/key/default\n\n    Returns:\n        (env, key, has_default, default) tuple, where\n            env: str. env var name.\n            key: str. save env value to this context key.\n            has_default: bool. True if default specified.\n            default: the value of default, if specified.\n\n    Raises:\n        ContextError: envGet is not a list of dicts.\n        KeyNotInContextError: If env or key not found in get_config.\n\n    \"\"\"\n    if not isinstance(get_item, dict):\n        raise ContextError('envGet must contain a list of dicts.')\n\n    env = get_item.get('env', None)\n\n    if not env:\n        raise KeyNotInContextError(\n            'context envGet[env] must exist in context for envGet.')\n\n    key = get_item.get('key', None)\n\n    if not key:\n        raise KeyNotInContextError(\n            'context envGet[key] must exist in context for envGet.')\n\n    if 'default' in get_item:\n        has_default = True\n        default = get_item['default']\n    else:\n        has_default = False\n        default = None\n\n    return (env, key, has_default, default)",
    "reference": "Parse env, key, default out of input dict.\n\n    Args:\n        get_item: dict. contains keys env/key/default\n\n    Returns:\n        (env, key, has_default, default) tuple, where\n            env: str. env var name.\n            key: str. save env value to this context key.\n            has_default: bool. True if default specified.\n            default: the value of default, if specified.\n\n    Raises:\n        ContextError: envGet is not a list of dicts.\n        KeyNotInContextError: If env or key not found in get_config.",
    "generated": "Parse env, key, default out of input dict.\n\n    Args:\n        get_item: dict. contains keys env/key/default\n\n    Returns:\n        ContextError: envGet is not a list of dicts.\n        KeyNotInContextError: If env or key not found in get_config."
  },
  {
    "code": "def run_step(context):\n    \"\"\"Executes dynamic python code.\n\n    Context is a dictionary or dictionary-like.\n    Context must contain key 'pycode'\n    Will exec context['pycode'] as dynamically interpreted python statements.\n\n    context is mandatory. When you execute the pipeline, it should look\n    something like this:\n        pipeline-runner [name here] 'pycode=print(1+1)'.\n    \"\"\"\n    logger.debug(\"started\")\n    context.assert_key_has_value(key='pycode', caller=__name__)\n\n    logger.debug(f\"Executing python string: {context['pycode']}\")\n    locals_dictionary = locals()\n    exec(context['pycode'], globals(), locals_dictionary)\n\n    # It looks like this dance might be unnecessary in python 3.6\n    logger.debug(\"looking for context update in exec\")\n    exec_context = locals_dictionary['context']\n    context.update(exec_context)\n    logger.debug(\"exec output context merged with pipeline context\")\n\n    logger.debug(\"done\")",
    "reference": "Executes dynamic python code.\n\n    Context is a dictionary or dictionary-like.\n    Context must contain key 'pycode'\n    Will exec context['pycode'] as dynamically interpreted python statements.\n\n    context is mandatory. When you execute the pipeline, it should look\n    something like this:\n        pipeline-runner [name here] 'pycode=print(1+1)'.",
    "generated": "Executes dynamic python code.\n\n    Context is a dictionary or dictionary-like.\n    Context must contain key 'pycode'\n    Will exec context['pycode'] as dynamically interpreted python statements.\n\n    context is mandatory. When you execute the pipeline, it should look\n    something like this:\n        pipeline-runner [name here] 'pycode=print(1+1)'"
  },
  {
    "code": "def get_parser():\n    \"\"\"Return ArgumentParser for pypyr cli.\"\"\"\n    parser = argparse.ArgumentParser(\n        allow_abbrev=True,\n        description='pypyr pipeline runner')\n    parser.add_argument('pipeline_name',\n                        help='Name of pipeline to run. It should exist in the '\n                        './pipelines directory.')\n    parser.add_argument(dest='pipeline_context',\n                        nargs='?',\n                        help='String for context values. Parsed by the '\n                        'pipeline\\'s context_parser function.')\n    parser.add_argument('--dir', dest='working_dir', default=os.getcwd(),\n                        help='Working directory. Use if your pipelines '\n                        'directory is elsewhere. Defaults to cwd.')\n    parser.add_argument('--log', '--loglevel', dest='log_level', type=int,\n                        default=20,\n                        help='Integer log level. Defaults to 20 (INFO). '\n                        '10=DEBUG\\n20=INFO\\n30=WARNING\\n40=ERROR\\n50=CRITICAL'\n                        '.\\n Log Level < 10 gives full traceback on errors.')\n    parser.add_argument('--logpath', dest='log_path',\n                        help='Log-file path. Append log output to this path')\n    parser.add_argument('--version', action='version',\n                        help='Echo version number.',\n                        version=f'{pypyr.version.get_version()}')\n    return parser",
    "reference": "Return ArgumentParser for pypyr cli.",
    "generated": "Return ArgumentParser for pypyr cli."
  },
  {
    "code": "def main(args=None):\n    \"\"\"Entry point for pypyr cli.\n\n    The setup_py entry_point wraps this in sys.exit already so this effectively\n    becomes sys.exit(main()).\n    The __main__ entry point similarly wraps sys.exit().\n    \"\"\"\n    if args is None:\n        args = sys.argv[1:]\n\n    parsed_args = get_args(args)\n\n    try:\n        return pypyr.pipelinerunner.main(\n            pipeline_name=parsed_args.pipeline_name,\n            pipeline_context_input=parsed_args.pipeline_context,\n            working_dir=parsed_args.working_dir,\n            log_level=parsed_args.log_level,\n            log_path=parsed_args.log_path)\n    except KeyboardInterrupt:\n        # Shell standard is 128 + signum = 130 (SIGINT = 2)\n        sys.stdout.write(\"\\n\")\n        return 128 + signal.SIGINT\n    except Exception as e:\n        # stderr and exit code 255\n        sys.stderr.write(\"\\n\")\n        sys.stderr.write(f\"\\033[91m{type(e).__name__}: {str(e)}\\033[0;0m\")\n        sys.stderr.write(\"\\n\")\n        # at this point, you're guaranteed to have args and thus log_level\n        if parsed_args.log_level < 10:\n            # traceback prints to stderr by default\n            traceback.print_exc()\n\n        return 255",
    "reference": "Entry point for pypyr cli.\n\n    The setup_py entry_point wraps this in sys.exit already so this effectively\n    becomes sys.exit(main()).\n    The __main__ entry point similarly wraps sys.exit().",
    "generated": "Entry point for pypyr cli.\n\n    The setup_py entry_point wraps this in sys.exit already so this effectively\n    becomes sys.exit(main()).\n    The __main__ entry point similarly wraps sys.exit()"
  },
  {
    "code": "def run_step(context):\n    \"\"\"Remove specified keys from context.\n\n    Args:\n        Context is a dictionary or dictionary-like.\n        context['contextClear'] must exist. It's a dictionary.\n        Will iterate context['contextClear'] and remove those keys from\n        context.\n\n    For example, say input context is:\n        key1: value1\n        key2: value2\n        key3: value3\n        key4: value4\n        contextClear:\n            - key2\n            - key4\n            - contextClear\n\n    This will result in return context:\n        key1: value1\n        key3: value3\n    \"\"\"\n    logger.debug(\"started\")\n    context.assert_key_has_value(key='contextClear', caller=__name__)\n\n    for k in context['contextClear']:\n        logger.debug(f\"removing {k} from context\")\n        # slightly unorthodox pop returning None means you don't get a KeyError\n        # if key doesn't exist\n        context.pop(k, None)\n        logger.info(f\"removed {k} from context\")\n\n    logger.debug(\"done\")",
    "reference": "Remove specified keys from context.\n\n    Args:\n        Context is a dictionary or dictionary-like.\n        context['contextClear'] must exist. It's a dictionary.\n        Will iterate context['contextClear'] and remove those keys from\n        context.\n\n    For example, say input context is:\n        key1: value1\n        key2: value2\n        key3: value3\n        key4: value4\n        contextClear:\n            - key2\n            - key4\n            - contextClear\n\n    This will result in return context:\n        key1: value1\n        key3: value3",
    "generated": "Remove specified keys from context.\n\n    Args:\n        Context is a dictionary or dictionary-like.\n        context['contextClear'] must exist. It's a dictionary.\n        Will iterate context['contextClear'] and remove those keys from\n        context."
  },
  {
    "code": "def run_step(context):\n    \"\"\"Run command, program or executable.\n\n    Context is a dictionary or dictionary-like.\n\n    Context must contain the following keys:\n    cmd: <<cmd string>> (command + args to execute.)\n\n    OR, as a dict\n    cmd:\n        run: str. mandatory. <<cmd string>> command + args to execute.\n        save: bool. defaults False. save output to cmdOut.\n\n    Will execute the command string in the shell as a sub-process.\n    Escape curly braces: if you want a literal curly brace, double it like\n    {{ or }}.\n\n    If save is True, will save the output to context as follows:\n        cmdOut:\n            returncode: 0\n            stdout: 'stdout str here. None if empty.'\n            stderr: 'stderr str here. None if empty.'\n\n    cmdOut.returncode is the exit status of the called process. Typically 0\n    means OK. A negative value -N indicates that the child was terminated by\n    signal N (POSIX only).\n\n    context['cmd'] will interpolate anything in curly braces for values\n    found in context. So if your context looks like this:\n        key1: value1\n        key2: value2\n        cmd: mything --arg1 {key1}\n\n    The cmd passed to the shell will be \"mything --arg value1\"\n    \"\"\"\n    logger.debug(\"started\")\n\n    pypyr.steps.cmd.run_step(context)\n\n    logger.debug(\"done\")",
    "reference": "Run command, program or executable.\n\n    Context is a dictionary or dictionary-like.\n\n    Context must contain the following keys:\n    cmd: <<cmd string>> (command + args to execute.)\n\n    OR, as a dict\n    cmd:\n        run: str. mandatory. <<cmd string>> command + args to execute.\n        save: bool. defaults False. save output to cmdOut.\n\n    Will execute the command string in the shell as a sub-process.\n    Escape curly braces: if you want a literal curly brace, double it like\n    {{ or }}.\n\n    If save is True, will save the output to context as follows:\n        cmdOut:\n            returncode: 0\n            stdout: 'stdout str here. None if empty.'\n            stderr: 'stderr str here. None if empty.'\n\n    cmdOut.returncode is the exit status of the called process. Typically 0\n    means OK. A negative value -N indicates that the child was terminated by\n    signal N (POSIX only).\n\n    context['cmd'] will interpolate anything in curly braces for values\n    found in context. So if your context looks like this:\n        key1: value1\n        key2: value2\n        cmd: mything --arg1 {key1}\n\n    The cmd passed to the shell will be \"mything --arg value1\"",
    "generated": "Run command, program or executable.\n\n    Context is a dictionary or dictionary-like.\n\n    Context must contain the following keys:\n    cmd: <<cmd string>> (command + args to execute.)\n\n    OR, as a dict\n    cmd:\n        run: str. mandatory. <<cmd string>> (command + args to execute.)\n        save: bool. defaults False. save output to cmdOut.\n\n    Will execute the command string in the shell as a sub-process.\n    Escape curly braces: if you want a literal curly brace, double it like\n   "
  },
  {
    "code": "def run_step(context):\n    \"\"\"Set hierarchy into context with substitutions if it doesn't exist yet.\n\n    context is a dictionary or dictionary-like.\n    context['defaults'] must exist. It's a dictionary.\n\n    Will iterate context['defaults'] and add these as new values where\n    their keys don't already exist. While it's doing so, it will leave\n    all other values in the existing hierarchy untouched.\n\n    List merging is purely additive, with no checks for uniqueness or already\n    existing list items. E.g context [0,1,2] with contextMerge=[2,3,4]\n    will result in [0,1,2,2,3,4]\n\n    Keep this in mind especially where complex types like\n    dicts nest inside a list - a merge will always add a new dict list item,\n    not merge it into whatever dicts might exist on the list already.\n\n    For example, say input context is:\n        key1: value1\n        key2: value2\n        key3:\n            k31: value31\n            k32: value32\n        defaults:\n            key2: 'aaa_{key1}_zzz'\n            key3:\n                k33: value33\n            key4: 'bbb_{key2}_yyy'\n\n    This will result in return context:\n        key1: value1\n        key2: value2\n        key3:\n            k31: value31\n            k32: value32\n            k33: value33\n        key4: bbb_value2_yyy\n    \"\"\"\n    logger.debug(\"started\")\n    context.assert_key_has_value(key='defaults', caller=__name__)\n\n    context.set_defaults(context['defaults'])\n\n    logger.info(f\"set {len(context['defaults'])} context item defaults.\")\n\n    logger.debug(\"done\")",
    "reference": "Set hierarchy into context with substitutions if it doesn't exist yet.\n\n    context is a dictionary or dictionary-like.\n    context['defaults'] must exist. It's a dictionary.\n\n    Will iterate context['defaults'] and add these as new values where\n    their keys don't already exist. While it's doing so, it will leave\n    all other values in the existing hierarchy untouched.\n\n    List merging is purely additive, with no checks for uniqueness or already\n    existing list items. E.g context [0,1,2] with contextMerge=[2,3,4]\n    will result in [0,1,2,2,3,4]\n\n    Keep this in mind especially where complex types like\n    dicts nest inside a list - a merge will always add a new dict list item,\n    not merge it into whatever dicts might exist on the list already.\n\n    For example, say input context is:\n        key1: value1\n        key2: value2\n        key3:\n            k31: value31\n            k32: value32\n        defaults:\n            key2: 'aaa_{key1}_zzz'\n            key3:\n                k33: value33\n            key4: 'bbb_{key2}_yyy'\n\n    This will result in return context:\n        key1: value1\n        key2: value2\n        key3:\n            k31: value31\n            k32: value32\n            k33: value33\n        key4: bbb_value2_yyy",
    "generated": "Set hierarchy into context with substitutions if it doesn't exist yet.\n\n    context is a dictionary or dictionary-like.\n    context['defaults'] must exist. It's a dictionary.\n\n    Will iterate context['defaults'] and add these as new values where\n    their keys don't already exist. While it's doing so, it will leave\n    all other values in the existing hierarchy untouched."
  },
  {
    "code": "def get_pipeline_steps(pipeline, steps_group):\n    \"\"\"Get the steps attribute of module pipeline.\n\n    If there is no steps sequence on the pipeline, return None. Guess you\n    could theoretically want to run a pipeline with nothing in it.\n    \"\"\"\n    logger.debug(\"starting\")\n    assert pipeline\n    assert steps_group\n\n    logger.debug(f\"retrieving {steps_group} steps from pipeline\")\n    if steps_group in pipeline:\n        steps = pipeline[steps_group]\n\n        if steps is None:\n            logger.warn(\n                f\"{steps_group}: sequence has no elements. So it won't do \"\n                \"anything.\")\n            logger.debug(\"done\")\n            return None\n\n        steps_count = len(steps)\n\n        logger.debug(f\"{steps_count} steps found under {steps_group} in \"\n                     \"pipeline definition.\")\n\n        logger.debug(\"done\")\n        return steps\n    else:\n        logger.debug(\n            f\"pipeline doesn't have a {steps_group} collection. Add a \"\n            f\"{steps_group}: sequence to the yaml if you want {steps_group} \"\n            \"actually to do something.\")\n        logger.debug(\"done\")\n        return None",
    "reference": "Get the steps attribute of module pipeline.\n\n    If there is no steps sequence on the pipeline, return None. Guess you\n    could theoretically want to run a pipeline with nothing in it.",
    "generated": "Get the steps attribute of module pipeline.\n\n    If there is no steps sequence on the pipeline, return None. Guess you\n    could theoretically want to run a pipeline with nothing in it."
  },
  {
    "code": "def run_failure_step_group(pipeline, context):\n    \"\"\"Run the on_failure step group if it exists.\n\n    This function will swallow all errors, to prevent obfuscating the error\n    condition that got it here to begin with.\n    \"\"\"\n    logger.debug(\"starting\")\n    try:\n        assert pipeline\n        # if no on_failure exists, it'll do nothing.\n        run_step_group(pipeline_definition=pipeline,\n                       step_group_name='on_failure',\n                       context=context)\n    except Exception as exception:\n        logger.error(\"Failure handler also failed. Swallowing.\")\n        logger.error(exception)\n\n    logger.debug(\"done\")",
    "reference": "Run the on_failure step group if it exists.\n\n    This function will swallow all errors, to prevent obfuscating the error\n    condition that got it here to begin with.",
    "generated": "Run the on_failure step group if it exists.\n\n    This function will swallow all errors, to prevent obfuscating the error\n    condition that got it here to begin with."
  },
  {
    "code": "def run_step_group(pipeline_definition, step_group_name, context):\n    \"\"\"Get the specified step group from the pipeline and run its steps.\"\"\"\n    logger.debug(f\"starting {step_group_name}\")\n    assert step_group_name\n\n    steps = get_pipeline_steps(pipeline=pipeline_definition,\n                               steps_group=step_group_name)\n\n    run_pipeline_steps(steps=steps, context=context)\n\n    logger.debug(f\"done {step_group_name}\")",
    "reference": "Get the specified step group from the pipeline and run its steps.",
    "generated": "Get the specified step group from the pipeline and run its steps."
  },
  {
    "code": "def ensure_dir(path):\n    \"\"\"Create all parent directories of path if they don't exist.\n\n    Args:\n        path. Path-like object. Create parent dirs to this path.\n\n    Return:\n        None.\n\n    \"\"\"\n    os.makedirs(os.path.abspath(os.path.dirname(path)), exist_ok=True)",
    "reference": "Create all parent directories of path if they don't exist.\n\n    Args:\n        path. Path-like object. Create parent dirs to this path.\n\n    Return:\n        None.",
    "generated": "Create all parent directories of path if they don't exist."
  },
  {
    "code": "def get_glob(path):\n    \"\"\"Process the input path, applying globbing and formatting.\n\n    Do note that this will returns files AND directories that match the glob.\n\n    No tilde expansion is done, but *, ?, and character ranges expressed with\n    [] will be correctly matched.\n\n    Escape all special characters ('?', '*' and '['). For a literal match, wrap\n    the meta-characters in brackets. For example, '[?]' matches the character\n    '?'.\n\n    If passing in an iterable of paths, will expand matches for each path in\n    the iterable. The function will return all the matches for each path\n    glob expression combined into a single list.\n\n    Args:\n        path: Path-like string, or iterable (list or tuple ) of paths.\n\n    Returns:\n        Combined list of paths found for input glob.\n\n    \"\"\"\n    if isinstance(path, str):\n        return glob.glob(path, recursive=True)\n    if isinstance(path, os.PathLike):\n        # hilariously enough, glob doesn't like path-like. Gotta be str.\n        return glob.glob(str(path), recursive=True)\n    elif isinstance(path, (list, tuple)):\n        # each glob returns a list, so chain all the lists into one big list\n        return list(chain.from_iterable(\n            glob.glob(str(p), recursive=True) for p in path))\n    else:\n        raise TypeError(\"path should be string, path-like or a list. Instead, \"\n                        f\"it's a {type(path)}\")",
    "reference": "Process the input path, applying globbing and formatting.\n\n    Do note that this will returns files AND directories that match the glob.\n\n    No tilde expansion is done, but *, ?, and character ranges expressed with\n    [] will be correctly matched.\n\n    Escape all special characters ('?', '*' and '['). For a literal match, wrap\n    the meta-characters in brackets. For example, '[?]' matches the character\n    '?'.\n\n    If passing in an iterable of paths, will expand matches for each path in\n    the iterable. The function will return all the matches for each path\n    glob expression combined into a single list.\n\n    Args:\n        path: Path-like string, or iterable (list or tuple ) of paths.\n\n    Returns:\n        Combined list of paths found for input glob.",
    "generated": "Process the input path, applying globbing and formatting.\n\n    Do note that this will returns files AND directories that match the glob.\n\n    No tilde expansion is done, but *, ?, and character ranges expressed with\n    [] will be correctly matched.\n\n    Escape all special characters ('?', '*' and '['). For a literal match, wrap\n    the meta-characters in brackets. For example, '[?]' matches the character\n    '?'.\n\n    If passing in an iterable of paths, will expand matches for each path in\n    the iterable. For example, '[?"
  },
  {
    "code": "def is_same_file(path1, path2):\n    \"\"\"Return True if path1 is the same file as path2.\n\n    The reason for this dance is that samefile throws if either file doesn't\n    exist.\n\n    Args:\n        path1: str or path-like.\n        path2: str or path-like.\n\n    Returns:\n        bool. True if the same file, False if not.\n\n    \"\"\"\n    return (\n        path1 and path2\n        and os.path.isfile(path1) and os.path.isfile(path2)\n        and os.path.samefile(path1, path2))",
    "reference": "Return True if path1 is the same file as path2.\n\n    The reason for this dance is that samefile throws if either file doesn't\n    exist.\n\n    Args:\n        path1: str or path-like.\n        path2: str or path-like.\n\n    Returns:\n        bool. True if the same file, False if not.",
    "generated": "Return True if path1 is the same file as path2.\n\n    The reason for this dance is that samefile throws if either file doesn't\n    exist."
  },
  {
    "code": "def move_file(src, dest):\n    \"\"\"Move source file to destination.\n\n    Overwrites dest.\n\n    Args:\n        src: str or path-like. source file\n        dest: str or path-like. destination file\n\n    Returns:\n        None.\n\n    Raises:\n        FileNotFoundError: out path parent doesn't exist.\n        OSError: if any IO operations go wrong.\n\n    \"\"\"\n    try:\n        os.replace(src, dest)\n    except Exception as ex_replace:\n        logger.error(f\"error moving file {src} to \"\n                     f\"{dest}. {ex_replace}\")\n        raise",
    "reference": "Move source file to destination.\n\n    Overwrites dest.\n\n    Args:\n        src: str or path-like. source file\n        dest: str or path-like. destination file\n\n    Returns:\n        None.\n\n    Raises:\n        FileNotFoundError: out path parent doesn't exist.\n        OSError: if any IO operations go wrong.",
    "generated": "Move source file to destination.\n\n    Overwrites dest."
  },
  {
    "code": "def move_temp_file(src, dest):\n    \"\"\"Move src to dest. Delete src if something goes wrong.\n\n    Overwrites dest.\n\n    Args:\n        src: str or path-like. source file\n        dest: str or path-like. destination file\n\n    Returns:\n        None.\n\n    Raises:\n        FileNotFoundError: out path parent doesn't exist.\n        OSError: if any IO operations go wrong. Does its best to clean up after\n                 itself and remove temp files.\n\n    \"\"\"\n    try:\n        move_file(src, dest)\n    except Exception:\n        try:\n            os.remove(src)\n        except Exception as ex_clean:\n            # at this point, something's deeply wrong, so log error.\n            # raising the original error, though, not this error in the\n            # error handler, as the 1st was the initial cause of all of\n            # this.\n            logger.error(f\"error removing temp file {src}. \"\n                         f\"{ex_clean}\")\n\n        raise",
    "reference": "Move src to dest. Delete src if something goes wrong.\n\n    Overwrites dest.\n\n    Args:\n        src: str or path-like. source file\n        dest: str or path-like. destination file\n\n    Returns:\n        None.\n\n    Raises:\n        FileNotFoundError: out path parent doesn't exist.\n        OSError: if any IO operations go wrong. Does its best to clean up after\n                 itself and remove temp files.",
    "generated": "Move src to dest. Delete src if something goes wrong.\n\n    Overwrites dest.\n\n    Returns:\n        None.\n\n    Raises:\n        FileNotFoundError: out path parent doesn't exist.\n        OSError: if any IO operations go wrong. Does its best to clean up after\n                 itself and remove temp files."
  },
  {
    "code": "def files_in_to_out(self, in_path, out_path=None):\n        \"\"\"Write in files to out, calling the line_handler on each line.\n\n        Calls file_in_to_out under the hood to format the in_path payload. The\n        formatting processing is done by the self.formatter instance.\n\n        Args:\n            in_path: str, path-like, or an iterable (list/tuple) of\n                     strings/paths. Each str/path can be a glob, relative or\n                     absolute path.\n            out_path: str or path-like. Can refer to a file or a directory.\n                      will create directory structure if it doesn't exist. If\n                      in-path refers to >1 file (e.g it's a glob or list), out\n                      path can only be a directory - it doesn't make sense to\n                      write >1 file to the same single file (this is no an\n                      appender.) To ensure out_path is read as a directory and\n                      not a file, be sure to have the path separator (/) at the\n                      end.\n                      Top tip: Path-like objects strip the trailing slash. If\n                      you want to pass in a dir that does not exist yet as\n                      out-path with a trailing /, you should be passing it as a\n                      str to preserve the /.\n                      If out_path is not specified or None, will in-place edit\n                      and overwrite the in-files.\n\n        Returns:\n            None.\n\n        \"\"\"\n        in_paths = get_glob(in_path)\n\n        in_count = len(in_paths)\n        if in_count == 0:\n            logger.debug(f'in path found {in_count} paths.')\n        else:\n            logger.debug(f'in path found {in_count} paths:')\n            for path in in_paths:\n                logger.debug(f'{path}')\n            logger.debug(\n                'herewith ends the paths. will now process each file.')\n\n        if in_paths:\n            # derive the destination directory, ensure it's ready for writing\n            basedir_out = None\n            is_outfile_name_known = False\n            if out_path:\n                # outpath could be a file, or a dir\n                pathlib_out = Path(out_path)\n                # yep, Path() strips trailing /, hence check original string\n                if isinstance(out_path, str) and out_path.endswith(os.sep):\n                    # ensure dir - mimic posix mkdir -p\n                    pathlib_out.mkdir(parents=True, exist_ok=True)\n                    basedir_out = pathlib_out\n                elif pathlib_out.is_dir():\n                    basedir_out = pathlib_out\n                else:\n                    if len(in_paths) > 1:\n                        raise Error(\n                            f'{in_path} resolves to {len(in_paths)} files, '\n                            'but you specified only a single file as out '\n                            f'{out_path}. If the outpath is meant to be a '\n                            'directory, put a / at the end.')\n\n                    # at this point it must be a file (not dir) path\n                    # make sure that the parent dir exists\n                    basedir_out = pathlib_out.parent\n                    basedir_out.parent.mkdir(parents=True, exist_ok=True)\n                    is_outfile_name_known = True\n\n            # loop through all the in files and write them to the out dir\n            file_counter = 0\n            is_edit = False\n            for path in in_paths:\n                actual_in = Path(path)\n                # recursive glob returns dirs too, only interested in files\n                if actual_in.is_file():\n                    if basedir_out:\n                        if is_outfile_name_known:\n                            actual_out = pathlib_out\n                        else:\n                            # default to original src file name if only out dir\n                            # specified without an out file name\n                            actual_out = basedir_out.joinpath(actual_in.name)\n\n                        logger.debug(f\"writing {path} to {actual_out}\")\n                        self.in_to_out(in_path=actual_in, out_path=actual_out)\n                    else:\n                        logger.debug(f\"editing {path}\")\n                        self.in_to_out(in_path=actual_in)\n                        is_edit = True\n                    file_counter += 1\n\n            if is_edit:\n                logger.info(\n                    f\"edited & wrote {file_counter} file(s) at {in_path}\")\n            else:\n                logger.info(\n                    f\"read {in_path}, formatted and wrote {file_counter} \"\n                    f\"file(s) to {out_path}\")\n        else:\n            logger.info(f\"{in_path} found no files\")",
    "reference": "Write in files to out, calling the line_handler on each line.\n\n        Calls file_in_to_out under the hood to format the in_path payload. The\n        formatting processing is done by the self.formatter instance.\n\n        Args:\n            in_path: str, path-like, or an iterable (list/tuple) of\n                     strings/paths. Each str/path can be a glob, relative or\n                     absolute path.\n            out_path: str or path-like. Can refer to a file or a directory.\n                      will create directory structure if it doesn't exist. If\n                      in-path refers to >1 file (e.g it's a glob or list), out\n                      path can only be a directory - it doesn't make sense to\n                      write >1 file to the same single file (this is no an\n                      appender.) To ensure out_path is read as a directory and\n                      not a file, be sure to have the path separator (/) at the\n                      end.\n                      Top tip: Path-like objects strip the trailing slash. If\n                      you want to pass in a dir that does not exist yet as\n                      out-path with a trailing /, you should be passing it as a\n                      str to preserve the /.\n                      If out_path is not specified or None, will in-place edit\n                      and overwrite the in-files.\n\n        Returns:\n            None.",
    "generated": "Write in files to out, calling the line_handler on each line.\n\n        Calls file_in_to_out under the hood to format the in_path payload. The\n        formatting processing is done by the self.formatter instance.\n\n        Args:\n            in_path: str, path-like, or an iterable (list/tuple) of\n                     strings/paths. Each str/path can be a glob, relative or\n                     absolute path."
  },
  {
    "code": "def in_to_out(self, in_path, out_path=None):\n        \"\"\"Load file into object, formats, writes object to out.\n\n        If in_path and out_path point to the same thing it will in-place edit\n        and overwrite the in path. Even easier, if you do want to edit a file\n        in place, don't specify out_path, or set it to None.\n\n        Args:\n            in_path: str or path-like. Must refer to a single existing file.\n            out_path: str or path-like. Must refer to a single destination file\n                      location. will create directory structure if it doesn't\n                      exist.\n                      If out_path is not specified or None, will in-place edit\n                      and overwrite the in-files.\n\n        Returns:\n            None.\n\n        \"\"\"\n        if is_same_file(in_path, out_path):\n            logger.debug(\n                \"in path and out path are the same file. writing to temp \"\n                \"file and then replacing in path with the temp file.\")\n            out_path = None\n\n        logger.debug(f\"opening source file: {in_path}\")\n        with open(in_path) as infile:\n            obj = self.object_representer.load(infile)\n\n        if out_path:\n            logger.debug(\n                f\"opening destination file for writing: {out_path}\")\n            ensure_dir(out_path)\n            with open(out_path, 'w') as outfile:\n                self.object_representer.dump(outfile, self.formatter(obj))\n            return\n        else:\n            logger.debug(\"opening temp file for writing...\")\n            with NamedTemporaryFile(mode='w+t',\n                                    dir=os.path.dirname(in_path),\n                                    delete=False) as outfile:\n                self.object_representer.dump(outfile, self.formatter(obj))\n\n            logger.debug(f\"moving temp file to: {in_path}\")\n\n            move_temp_file(outfile.name, infile.name)",
    "reference": "Load file into object, formats, writes object to out.\n\n        If in_path and out_path point to the same thing it will in-place edit\n        and overwrite the in path. Even easier, if you do want to edit a file\n        in place, don't specify out_path, or set it to None.\n\n        Args:\n            in_path: str or path-like. Must refer to a single existing file.\n            out_path: str or path-like. Must refer to a single destination file\n                      location. will create directory structure if it doesn't\n                      exist.\n                      If out_path is not specified or None, will in-place edit\n                      and overwrite the in-files.\n\n        Returns:\n            None.",
    "generated": "Load file into object, formats, writes object to out.\n\n        If in_path and out_path point to the same thing it will in-place edit\n        and overwrite the in path. Even easier, if you do want to edit a file\n        in place, don't specify out_path, or set it to None."
  },
  {
    "code": "def in_to_out(self, in_path, out_path=None):\n        \"\"\"Write a single file in to out, running self.formatter on each line.\n\n        If in_path and out_path point to the same thing it will in-place edit\n        and overwrite the in path. Even easier, if you do want to edit a file\n        in place, don't specify out_path, or set it to None.\n\n        Args:\n            in_path: str or path-like. Must refer to a single existing file.\n            out_path: str or path-like. Must refer to a single destination file\n                      location. will create directory structure if it doesn't\n                      exist.\n                      If out_path is not specified or None, will in-place edit\n                      and overwrite the in-files.\n\n        Returns:\n            None.\n\n        \"\"\"\n        is_in_place_edit = False\n        if is_same_file(in_path, out_path):\n            logger.debug(\n                \"in path and out path are the same file. writing to temp \"\n                \"file and then replacing in path with the temp file.\")\n            out_path = None\n            is_in_place_edit = True\n\n        logger.debug(f\"opening source file: {in_path}\")\n        with open(in_path) as infile:\n            if out_path:\n                logger.debug(\n                    f\"opening destination file for writing: {out_path}\")\n                ensure_dir(out_path)\n                with open(out_path, 'w') as outfile:\n                    outfile.writelines(self.formatter(infile))\n                return\n            else:\n                logger.debug(\"opening temp file for writing...\")\n                with NamedTemporaryFile(mode='w+t',\n                                        dir=os.path.dirname(in_path),\n                                        delete=False) as outfile:\n                    outfile.writelines(self.formatter(infile))\n\n                is_in_place_edit = True\n\n        # only replace infile AFTER it's closed, outside the with.\n        # pragma exclude because func actually returns on 287 in if out_path,\n        # and cov not smart enough to realize that !is_in_place_edit won't ever\n        # happen here (the function will have exited already)\n        if is_in_place_edit:    # pragma: no branch\n            logger.debug(f\"moving temp file to: {in_path}\")\n            move_temp_file(outfile.name, infile.name)",
    "reference": "Write a single file in to out, running self.formatter on each line.\n\n        If in_path and out_path point to the same thing it will in-place edit\n        and overwrite the in path. Even easier, if you do want to edit a file\n        in place, don't specify out_path, or set it to None.\n\n        Args:\n            in_path: str or path-like. Must refer to a single existing file.\n            out_path: str or path-like. Must refer to a single destination file\n                      location. will create directory structure if it doesn't\n                      exist.\n                      If out_path is not specified or None, will in-place edit\n                      and overwrite the in-files.\n\n        Returns:\n            None.",
    "generated": "Write a single file in to out, running self.formatter on each line.\n\n        If in_path and out_path point to the same thing it will in-place edit\n        and overwrite the in-files.\n\n        Returns:\n            None."
  },
  {
    "code": "def dump(self, file, payload):\n        \"\"\"Dump json oject to open file output.\n\n        Writes json with 2 spaces indentation.\n\n        Args:\n            file: Open file-like object. Must be open for writing.\n            payload: The Json object to write to file.\n\n        Returns:\n            None.\n\n        \"\"\"\n        json.dump(payload, file, indent=2, ensure_ascii=False)",
    "reference": "Dump json oject to open file output.\n\n        Writes json with 2 spaces indentation.\n\n        Args:\n            file: Open file-like object. Must be open for writing.\n            payload: The Json object to write to file.\n\n        Returns:\n            None.",
    "generated": "Dump oject to open file output.\n\n        Writes json with 2 spaces indentation.\n\n        Args:\n            file: Open file-like object. Must be open for writing.\n            payload: The Json object to write to file."
  },
  {
    "code": "def run_step(context):\n    \"\"\"Parse input file and replace a search string.\n\n    This also does string substitutions from context on the fileReplacePairs.\n    It does this before it search & replaces the in file.\n\n    Be careful of order. If fileReplacePairs is not an ordered collection,\n    replacements could evaluate in any given order. If this is coming in from\n    pipeline yaml it will be an ordered dictionary, so life is good.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                 The following context keys expected:\n                - fileReplace\n                    - in. mandatory.\n                      str, path-like, or an iterable (list/tuple) of\n                      strings/paths. Each str/path can be a glob, relative or\n                      absolute path.\n                    - out. optional. path-like.\n                      Can refer to a file or a directory.\n                      will create directory structure if it doesn't exist. If\n                      in-path refers to >1 file (e.g it's a glob or list), out\n                      path can only be a directory - it doesn't make sense to\n                      write >1 file to the same single file (this is not an\n                      appender.) To ensure out_path is read as a directory and\n                      not a file, be sure to have the path separator (/) at the\n                      end.\n                      If out_path is not specified or None, will in-place edit\n                      and overwrite the in-files.\n                    - replacePairs. mandatory. Dictionary where items are:\n                      'find_string': 'replace_string'\n\n    Returns:\n        None.\n\n    Raises:\n        FileNotFoundError: take a guess\n        pypyr.errors.KeyNotInContextError: Any of the required keys missing in\n                                          context.\n        pypyr.errors.KeyInContextHasNoValueError: Any of the required keys\n                                                  exists but is None.\n\n    \"\"\"\n    logger.debug(\"started\")\n    deprecated(context)\n    StreamReplacePairsRewriterStep(__name__, 'fileReplace', context).run_step()\n\n    logger.debug(\"done\")",
    "reference": "Parse input file and replace a search string.\n\n    This also does string substitutions from context on the fileReplacePairs.\n    It does this before it search & replaces the in file.\n\n    Be careful of order. If fileReplacePairs is not an ordered collection,\n    replacements could evaluate in any given order. If this is coming in from\n    pipeline yaml it will be an ordered dictionary, so life is good.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                 The following context keys expected:\n                - fileReplace\n                    - in. mandatory.\n                      str, path-like, or an iterable (list/tuple) of\n                      strings/paths. Each str/path can be a glob, relative or\n                      absolute path.\n                    - out. optional. path-like.\n                      Can refer to a file or a directory.\n                      will create directory structure if it doesn't exist. If\n                      in-path refers to >1 file (e.g it's a glob or list), out\n                      path can only be a directory - it doesn't make sense to\n                      write >1 file to the same single file (this is not an\n                      appender.) To ensure out_path is read as a directory and\n                      not a file, be sure to have the path separator (/) at the\n                      end.\n                      If out_path is not specified or None, will in-place edit\n                      and overwrite the in-files.\n                    - replacePairs. mandatory. Dictionary where items are:\n                      'find_string': 'replace_string'\n\n    Returns:\n        None.\n\n    Raises:\n        FileNotFoundError: take a guess\n        pypyr.errors.KeyNotInContextError: Any of the required keys missing in\n                                          context.\n        pypyr.errors.KeyInContextHasNoValueError: Any of the required keys\n                                                  exists but is None.",
    "generated": "Parse input file and replace a search string.\n\n    This also does string substitutions from context on the fileReplacePairs.\n    It does this before it search & replaces the in file.\n\n    Be careful of order. If fileReplacePairs is not an ordered collection,\n    replacements could evaluate in any given order. If this is coming in from\n    pipeline yaml it will be an ordered dictionary, so life is good.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                 The following context keys expected:\n                - fileReplace\n                    - in."
  },
  {
    "code": "def set_logging_config(log_level, handlers):\n    \"\"\"Set python logging library config.\n\n    Run this ONCE at the start of your process. It formats the python logging\n    module's output.\n    Defaults logging level to INFO = 20)\n    \"\"\"\n    logging.basicConfig(\n        format='%(asctime)s %(levelname)s:%(name)s:%(funcName)s: %(message)s',\n        datefmt='%Y-%m-%d %H:%M:%S',\n        level=log_level,\n        handlers=handlers)",
    "reference": "Set python logging library config.\n\n    Run this ONCE at the start of your process. It formats the python logging\n    module's output.\n    Defaults logging level to INFO = 20)",
    "generated": "Set python logging library config.\n\n    Run this ONCE at the start of your process. It formats the python logging\n    module's output."
  },
  {
    "code": "def set_root_logger(root_log_level, log_path=None):\n    \"\"\"Set the root logger 'pypyr'. Do this before you do anything else.\n\n    Run once and only once at initialization.\n    \"\"\"\n    handlers = []\n\n    console_handler = logging.StreamHandler()\n    handlers.append(console_handler)\n    if log_path:\n        file_handler = logging.FileHandler(log_path)\n        handlers.append(file_handler)\n\n    set_logging_config(root_log_level, handlers=handlers)\n    root_logger = logging.getLogger(\"pypyr\")\n    root_logger.debug(\n        f\"Root logger {root_logger.name} configured with level \"\n        f\"{root_log_level}\")",
    "reference": "Set the root logger 'pypyr'. Do this before you do anything else.\n\n    Run once and only once at initialization.",
    "generated": "Set the root logger 'pypyr'. Do this before you do anything else.\n\n    Run once and only once at initialization."
  },
  {
    "code": "def get_parsed_context(pipeline, context_in_string):\n    \"\"\"Execute get_parsed_context handler if specified.\n\n    Dynamically load the module specified by the context_parser key in pipeline\n    dict and execute the get_parsed_context function on that module.\n\n    Args:\n        pipeline: dict. Pipeline object.\n        context_in_string: string. Argument string used to initialize context.\n\n    Returns:\n        pypyr.context.Context() instance.\n\n    Raises:\n        AttributeError: parser specified on pipeline missing get_parsed_context\n                        function.\n\n    \"\"\"\n    logger.debug(\"starting\")\n\n    if 'context_parser' in pipeline:\n        parser_module_name = pipeline['context_parser']\n        logger.debug(f\"context parser found: {parser_module_name}\")\n        parser_module = pypyr.moduleloader.get_module(parser_module_name)\n\n        try:\n            logger.debug(f\"running parser {parser_module_name}\")\n            result_context = parser_module.get_parsed_context(\n                context_in_string)\n            logger.debug(f\"step {parser_module_name} done\")\n            # Downstream steps likely to expect context not to be None, hence\n            # empty rather than None.\n            if result_context is None:\n                logger.debug(f\"{parser_module_name} returned None. Using \"\n                             \"empty context instead\")\n                return pypyr.context.Context()\n            else:\n                return pypyr.context.Context(result_context)\n        except AttributeError:\n            logger.error(f\"The parser {parser_module_name} doesn't have a \"\n                         \"get_parsed_context(context) function.\")\n            raise\n    else:\n        logger.debug(\"pipeline does not have custom context parser. Using \"\n                     \"empty context.\")\n        logger.debug(\"done\")\n        # initialize to an empty dictionary because you want to be able to run\n        # with no context.\n        return pypyr.context.Context()",
    "reference": "Execute get_parsed_context handler if specified.\n\n    Dynamically load the module specified by the context_parser key in pipeline\n    dict and execute the get_parsed_context function on that module.\n\n    Args:\n        pipeline: dict. Pipeline object.\n        context_in_string: string. Argument string used to initialize context.\n\n    Returns:\n        pypyr.context.Context() instance.\n\n    Raises:\n        AttributeError: parser specified on pipeline missing get_parsed_context\n                        function.",
    "generated": "Execute get_parsed_context handler if specified.\n\n    Dynamically load the module specified by the context_parser key in pipeline\n    dict and execute the get_parsed_context function on that module.\n\n    Args:\n        pipeline: dict. Pipeline object.\n        context_in_string: string. Argument string used to initialize context."
  },
  {
    "code": "def main(\n    pipeline_name,\n    pipeline_context_input,\n    working_dir,\n    log_level,\n    log_path,\n):\n    \"\"\"Entry point for pypyr pipeline runner.\n\n    Call this once per pypyr run. Call me if you want to run a pypyr pipeline\n    from your own code. This function does some one-off 1st time initialization\n    before running the actual pipeline.\n\n    pipeline_name.yaml should be in the working_dir/pipelines/ directory.\n\n    Args:\n        pipeline_name: string. Name of pipeline, sans .yaml at end.\n        pipeline_context_input: string. Initialize the pypyr context with this\n                                string.\n        working_dir: path. looks for ./pipelines and modules in this directory.\n        log_level: int. Standard python log level enumerated value.\n        log_path: os.path. Append log to this path.\n\n    Returns:\n        None\n\n    \"\"\"\n    pypyr.log.logger.set_root_logger(log_level, log_path)\n\n    logger.debug(\"starting pypyr\")\n\n    # pipelines specify steps in python modules that load dynamically.\n    # make it easy for the operator so that the cwd is automatically included\n    # without needing to pip install a package 1st.\n    pypyr.moduleloader.set_working_directory(working_dir)\n\n    load_and_run_pipeline(pipeline_name=pipeline_name,\n                          pipeline_context_input=pipeline_context_input,\n                          working_dir=working_dir)\n\n    logger.debug(\"pypyr done\")",
    "reference": "Entry point for pypyr pipeline runner.\n\n    Call this once per pypyr run. Call me if you want to run a pypyr pipeline\n    from your own code. This function does some one-off 1st time initialization\n    before running the actual pipeline.\n\n    pipeline_name.yaml should be in the working_dir/pipelines/ directory.\n\n    Args:\n        pipeline_name: string. Name of pipeline, sans .yaml at end.\n        pipeline_context_input: string. Initialize the pypyr context with this\n                                string.\n        working_dir: path. looks for ./pipelines and modules in this directory.\n        log_level: int. Standard python log level enumerated value.\n        log_path: os.path. Append log to this path.\n\n    Returns:\n        None",
    "generated": "Entry point for pypyr pipeline runner.\n\n    Call this once per pypyr run. Call me if you want to run a pypyr pipeline\n    from your own code. This function does some one-off 1st time initialization\n    before running the actual pipeline.\n\n    pipeline_name.yaml should be in the working_dir/pipelines/ directory.\n\n    Args:\n        pipeline_name: string. Name of pipeline, sans .yaml at end.\n        pipeline_context_input: string. Initialize the pypyr context with this\n                                string."
  },
  {
    "code": "def prepare_context(pipeline, context_in_string, context):\n    \"\"\"Prepare context for pipeline run.\n\n    Args:\n        pipeline: dict. Dictionary representing the pipeline.\n        context_in_string: string. Argument string used to initialize context.\n        context: pypyr.context.Context. Merge any new context generated from\n                 context_in_string into this context instance.\n\n    Returns:\n        None. The context instance to use for the pipeline run is contained\n              in the context arg, it's not passed back as a function return.\n\n    \"\"\"\n    logger.debug(\"starting\")\n\n    parsed_context = get_parsed_context(\n        pipeline=pipeline,\n        context_in_string=context_in_string)\n\n    context.update(parsed_context)\n\n    logger.debug(\"done\")",
    "reference": "Prepare context for pipeline run.\n\n    Args:\n        pipeline: dict. Dictionary representing the pipeline.\n        context_in_string: string. Argument string used to initialize context.\n        context: pypyr.context.Context. Merge any new context generated from\n                 context_in_string into this context instance.\n\n    Returns:\n        None. The context instance to use for the pipeline run is contained\n              in the context arg, it's not passed back as a function return.",
    "generated": "Prepare context for pipeline run.\n\n    Args:\n        pipeline: dict. Dictionary representing the pipeline.\n        context_in_string: string. Argument string used to initialize context.\n        context: pypyr.context.Context. Merge any new context generated from\n                 context_in_string into this context instance."
  },
  {
    "code": "def load_and_run_pipeline(pipeline_name,\n                          pipeline_context_input=None,\n                          working_dir=None,\n                          context=None,\n                          parse_input=True,\n                          loader=None):\n    \"\"\"Load and run the specified pypyr pipeline.\n\n    This function runs the actual pipeline by name. If you are running another\n    pipeline from within a pipeline, call this, not main(). Do call main()\n    instead for your 1st pipeline if there are pipelines calling pipelines.\n\n    By default pypyr uses file loader. This means that pipeline_name.yaml\n    should be in the working_dir/pipelines/ directory.\n\n    Args:\n        pipeline_name (str): Name of pipeline, sans .yaml at end.\n        pipeline_context_input (str): Initialize the pypyr context with this\n                                 string.\n        working_dir (path): Look for pipelines and modules in this directory.\n                     If context arg passed, will use context.working_dir and\n                     ignore this argument. If context is None, working_dir\n                     must be specified.\n        context (pypyr.context.Context): Use if you already have a\n                 Context object, such as if you are running a pipeline from\n                 within a pipeline and you want to re-use the same context\n                 object for the child pipeline. Any mutations of the context by\n                 the pipeline will be against this instance of it.\n        parse_input (bool): run context_parser in pipeline.\n        loader (str): str. optional. Absolute name of pipeline loader module.\n                If not specified will use pypyr.pypeloaders.fileloader.\n\n    Returns:\n        None\n\n    \"\"\"\n    logger.debug(f\"you asked to run pipeline: {pipeline_name}\")\n    if loader:\n        logger.debug(f\"you set the pype loader to: {loader}\")\n    else:\n        loader = 'pypyr.pypeloaders.fileloader'\n        logger.debug(f\"use default pype loader: {loader}\")\n\n    logger.debug(f\"you set the initial context to: {pipeline_context_input}\")\n\n    if context is None:\n        context = pypyr.context.Context()\n        context.working_dir = working_dir\n    else:\n        working_dir = context.working_dir\n\n    # pipeline loading deliberately outside of try catch. The try catch will\n    # try to run a failure-handler from the pipeline, but if the pipeline\n    # doesn't exist there is no failure handler that can possibly run so this\n    # is very much a fatal stop error.\n    loader_module = pypyr.moduleloader.get_module(loader)\n\n    try:\n        get_pipeline_definition = getattr(\n            loader_module, 'get_pipeline_definition'\n        )\n    except AttributeError:\n        logger.error(\n            f\"The pipeline loader {loader_module} doesn't have a \"\n            \"get_pipeline_definition(pipeline_name, working_dir) function.\")\n        raise\n\n    logger.debug(f\"loading the pipeline definition with {loader_module}\")\n    pipeline_definition = get_pipeline_definition(\n        pipeline_name=pipeline_name,\n        working_dir=working_dir\n    )\n    logger.debug(f\"{loader_module} done\")\n\n    run_pipeline(\n        pipeline=pipeline_definition,\n        pipeline_context_input=pipeline_context_input,\n        context=context,\n        parse_input=parse_input\n    )",
    "reference": "Load and run the specified pypyr pipeline.\n\n    This function runs the actual pipeline by name. If you are running another\n    pipeline from within a pipeline, call this, not main(). Do call main()\n    instead for your 1st pipeline if there are pipelines calling pipelines.\n\n    By default pypyr uses file loader. This means that pipeline_name.yaml\n    should be in the working_dir/pipelines/ directory.\n\n    Args:\n        pipeline_name (str): Name of pipeline, sans .yaml at end.\n        pipeline_context_input (str): Initialize the pypyr context with this\n                                 string.\n        working_dir (path): Look for pipelines and modules in this directory.\n                     If context arg passed, will use context.working_dir and\n                     ignore this argument. If context is None, working_dir\n                     must be specified.\n        context (pypyr.context.Context): Use if you already have a\n                 Context object, such as if you are running a pipeline from\n                 within a pipeline and you want to re-use the same context\n                 object for the child pipeline. Any mutations of the context by\n                 the pipeline will be against this instance of it.\n        parse_input (bool): run context_parser in pipeline.\n        loader (str): str. optional. Absolute name of pipeline loader module.\n                If not specified will use pypyr.pypeloaders.fileloader.\n\n    Returns:\n        None",
    "generated": "Load and run the specified pypyr pipeline.\n\n    This function runs the actual pipeline by name. If you are running another\n    pipeline from within a pipeline, call this, not main(). Do call main()\n    instead for your 1st pipeline if there are pipelines calling pipelines.\n\n    By default pypyr uses file loader. This means that pipeline_name.yaml\n    should be in the working_dir/pipelines/ directory.\n\n    Args:\n        pipeline_name.yaml\n    should be in the working_dir/pipelines/ directory."
  },
  {
    "code": "def run_pipeline(pipeline,\n                 context,\n                 pipeline_context_input=None,\n                 parse_input=True):\n    \"\"\"Run the specified pypyr pipeline.\n\n    This function runs the actual pipeline. If you are running another\n    pipeline from within a pipeline, call this, not main(). Do call main()\n    instead for your 1st pipeline if there are pipelines calling pipelines.\n\n    Pipeline and context should be already loaded.\n\n    Args:\n        pipeline (dict): Dictionary representing the pipeline.\n        context (pypyr.context.Context): Reusable context object.\n        pipeline_context_input (str): Initialize the pypyr context with this\n                                string.\n        parse_input (bool): run context_parser in pipeline.\n\n    Returns:\n        None\n\n    \"\"\"\n    logger.debug(\"starting\")\n\n    try:\n        if parse_input:\n            logger.debug(\"executing context_parser\")\n            prepare_context(pipeline=pipeline,\n                            context_in_string=pipeline_context_input,\n                            context=context)\n        else:\n            logger.debug(\"skipping context_parser\")\n\n        # run main steps\n        pypyr.stepsrunner.run_step_group(\n            pipeline_definition=pipeline,\n            step_group_name='steps',\n            context=context)\n\n        # if nothing went wrong, run on_success\n        logger.debug(\"pipeline steps complete. Running on_success steps now.\")\n        pypyr.stepsrunner.run_step_group(\n            pipeline_definition=pipeline,\n            step_group_name='on_success',\n            context=context)\n    except Exception:\n        # yes, yes, don't catch Exception. Have to, though, to run the failure\n        # handler. Also, it does raise it back up.\n        logger.error(\"Something went wrong. Will now try to run on_failure.\")\n\n        # failure_step_group will log but swallow any errors\n        pypyr.stepsrunner.run_failure_step_group(\n            pipeline=pipeline,\n            context=context)\n        logger.debug(\"Raising original exception to caller.\")\n        raise\n\n    logger.debug(\"done\")",
    "reference": "Run the specified pypyr pipeline.\n\n    This function runs the actual pipeline. If you are running another\n    pipeline from within a pipeline, call this, not main(). Do call main()\n    instead for your 1st pipeline if there are pipelines calling pipelines.\n\n    Pipeline and context should be already loaded.\n\n    Args:\n        pipeline (dict): Dictionary representing the pipeline.\n        context (pypyr.context.Context): Reusable context object.\n        pipeline_context_input (str): Initialize the pypyr context with this\n                                string.\n        parse_input (bool): run context_parser in pipeline.\n\n    Returns:\n        None",
    "generated": "Run the specified pypyr pipeline.\n\n    This function runs the actual pipeline. If you are running another\n    pipeline from within a pipeline, call this, not main(). Do call main()\n    instead for your 1st pipeline if there are pipelines calling pipelines.\n\n    Pipeline and context should be already loaded.\n\n    Args:\n        pipeline (dict): Dictionary representing the pipeline.\n        context (pypyr.context.Context): Reusable context object."
  },
  {
    "code": "def run_step(context):\n    \"\"\"Write payload out to yaml file.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                 The following context keys expected:\n                - fileWriteYaml\n                    - path. mandatory. path-like. Write output file to\n                      here. Will create directories in path for you.\n                    - payload. optional. Write this to output file. If not\n                      specified, output entire context.\n\n    Returns:\n        None.\n\n    Raises:\n        pypyr.errors.KeyNotInContextError: fileWriteYaml or\n            fileWriteYaml['path'] missing in context.\n        pypyr.errors.KeyInContextHasNoValueError: fileWriteYaml or\n            fileWriteYaml['path'] exists but is None.\n\n    \"\"\"\n    logger.debug(\"started\")\n    context.assert_child_key_has_value('fileWriteYaml', 'path', __name__)\n\n    out_path = context.get_formatted_string(context['fileWriteYaml']['path'])\n    # doing it like this to safeguard against accidentally dumping all context\n    # with potentially sensitive values in it to disk if payload exists but is\n    # None.\n    is_payload_specified = 'payload' in context['fileWriteYaml']\n\n    yaml_writer = pypyr.yaml.get_yaml_parser_roundtrip_for_context()\n\n    logger.debug(f\"opening destination file for writing: {out_path}\")\n    os.makedirs(os.path.abspath(os.path.dirname(out_path)), exist_ok=True)\n    with open(out_path, 'w') as outfile:\n        if is_payload_specified:\n            payload = context['fileWriteYaml']['payload']\n            formatted_iterable = context.get_formatted_iterable(payload)\n        else:\n            formatted_iterable = context.get_formatted_iterable(context)\n\n        yaml_writer.dump(formatted_iterable, outfile)\n\n    logger.info(f\"formatted context content and wrote to {out_path}\")\n    logger.debug(\"done\")",
    "reference": "Write payload out to yaml file.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                 The following context keys expected:\n                - fileWriteYaml\n                    - path. mandatory. path-like. Write output file to\n                      here. Will create directories in path for you.\n                    - payload. optional. Write this to output file. If not\n                      specified, output entire context.\n\n    Returns:\n        None.\n\n    Raises:\n        pypyr.errors.KeyNotInContextError: fileWriteYaml or\n            fileWriteYaml['path'] missing in context.\n        pypyr.errors.KeyInContextHasNoValueError: fileWriteYaml or\n            fileWriteYaml['path'] exists but is None.",
    "generated": "Write payload out to yaml file.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                 The following context keys expected:\n                - fileWriteYaml\n                    - path. mandatory. path-like. Write this to output file. If not\n                      specified, output entire context.\n\n    Returns:\n        None."
  },
  {
    "code": "def run_step(context):\n    \"\"\"Print debug info to console.\n\n    context is a dictionary or dictionary-like.\n\n    If you use pypyr.steps.debug as a simple step (i.e you do NOT specify the\n    debug input context), it will just dump the entire context to stdout.\n\n    Configure the debug step with the following optional context item:\n        debug:\n            keys: str (for single key) or list (of str keys). Only dump the\n                  specified keys.\n            format: bool. Defaults False. Applies formatting expressions on\n                    dump.\n    \"\"\"\n    logger.debug(\"started\")\n\n    debug = context.get('debug', None)\n\n    if debug:\n        keys = debug.get('keys', None)\n        format = debug.get('format', False)\n\n        if keys:\n            logger.debug(f\"Writing to output: {keys}\")\n            if isinstance(keys, str):\n                payload = {keys: context[keys]}\n            else:\n                payload = {k: context[k] for k in keys}\n        else:\n            logger.debug(\n                \"No keys specified. Writing entire context to output.\")\n            payload = context\n\n        if format:\n            payload = context.get_formatted_iterable(payload)\n    else:\n        payload = context\n\n    logger.info(f'\\n{json.dumps(payload, indent=2, ensure_ascii=False)}')\n\n    logger.debug(\"done\")",
    "reference": "Print debug info to console.\n\n    context is a dictionary or dictionary-like.\n\n    If you use pypyr.steps.debug as a simple step (i.e you do NOT specify the\n    debug input context), it will just dump the entire context to stdout.\n\n    Configure the debug step with the following optional context item:\n        debug:\n            keys: str (for single key) or list (of str keys). Only dump the\n                  specified keys.\n            format: bool. Defaults False. Applies formatting expressions on\n                    dump.",
    "generated": "Print debug info to console.\n\n    context is a dictionary or dictionary-like.\n\n    If you use pypyr.steps.debug as a simple step (i.e you do NOT specify the\n    debug input context), it will just dump the entire context to stdout.\n\n    Configure the debug step with the following optional context item:\n        debug:\n            keys: str (for single key) or list (of str keys). Only dump the\n                  specified keys.\n            format: bool. Defaults False. Applies formatting expressions on\n                    dump."
  },
  {
    "code": "def get_error_name(error):\n    \"\"\"Return canonical error name as string.\n\n    For builtin errors like ValueError or Exception, will return the bare\n    name, like ValueError or Exception.\n\n    For all other exceptions, will return modulename.errorname, such as\n    arbpackage.mod.myerror\n\n    Args:\n        error: Exception object.\n\n    Returns:\n        str. Canonical error name.\n\n    \"\"\"\n    error_type = type(error)\n    if error_type.__module__ in ['__main__', 'builtins']:\n        return error_type.__name__\n    else:\n        return f'{error_type.__module__}.{error_type.__name__}'",
    "reference": "Return canonical error name as string.\n\n    For builtin errors like ValueError or Exception, will return the bare\n    name, like ValueError or Exception.\n\n    For all other exceptions, will return modulename.errorname, such as\n    arbpackage.mod.myerror\n\n    Args:\n        error: Exception object.\n\n    Returns:\n        str. Canonical error name.",
    "generated": "Return canonical error name as string.\n\n    For builtin errors like ValueError or Exception, will return the bare\n    name, like ValueError or Exception."
  },
  {
    "code": "def get_module(module_abs_import):\n    \"\"\"Use importlib to get the module dynamically.\n\n    Get instance of the module specified by the module_abs_import.\n    This means that module_abs_import must be resolvable from this package.\n\n    Args:\n        module_abs_import: string. Absolute name of module to import.\n\n    Raises:\n        PyModuleNotFoundError: if module not found.\n\n    \"\"\"\n    logger.debug(\"starting\")\n    logger.debug(f\"loading module {module_abs_import}\")\n    try:\n        imported_module = importlib.import_module(module_abs_import)\n        logger.debug(\"done\")\n        return imported_module\n    except ModuleNotFoundError as err:\n        msg = (\"The module doesn't exist. Looking for a file like this: \"\n               f\"{module_abs_import}\")\n\n        extended_msg = (f\"{module_abs_import}.py should be in your working \"\n                        \"dir or it should be installed to the python path.\"\n                        \"\\nIf you have 'package.sub.mod' your current working \"\n                        \"dir should contain ./package/sub/mod.py\\n\"\n                        \"If you specified 'mymodulename', your current \"\n                        \"working dir should contain ./mymodulename.py\\n\"\n                        \"If the module is not in your current working dir, it \"\n                        \"must exist in your current python path - so you \"\n                        \"should have run pip install or setup.py\")\n        logger.error(msg)\n        raise PyModuleNotFoundError(extended_msg) from err",
    "reference": "Use importlib to get the module dynamically.\n\n    Get instance of the module specified by the module_abs_import.\n    This means that module_abs_import must be resolvable from this package.\n\n    Args:\n        module_abs_import: string. Absolute name of module to import.\n\n    Raises:\n        PyModuleNotFoundError: if module not found.",
    "generated": "Use importlib to get the module dynamically.\n\n    Get instance of the module specified by the module_abs_import.\n    This means that module_abs_import must be resolvable from this package."
  },
  {
    "code": "def set_working_directory(working_directory):\n    \"\"\"Add working_directory to sys.paths.\n\n    This allows dynamic loading of arbitrary python modules in cwd.\n\n    Args:\n        working_directory: string. path to add to sys.paths\n\n    \"\"\"\n    logger.debug(\"starting\")\n\n    logger.debug(f\"adding {working_directory} to sys.paths\")\n    sys.path.append(working_directory)\n\n    logger.debug(\"done\")",
    "reference": "Add working_directory to sys.paths.\n\n    This allows dynamic loading of arbitrary python modules in cwd.\n\n    Args:\n        working_directory: string. path to add to sys.paths",
    "generated": "Add working_directory to sys.paths.\n\n    This allows dynamic loading of arbitrary python modules in cwd."
  },
  {
    "code": "def assert_child_key_has_value(self, parent, child, caller):\n        \"\"\"Assert that context contains key that has child which has a value.\n\n        Args:\n            parent: parent key\n            child: validate this sub-key of parent exists AND isn't None.\n            caller: string. calling function name - this used to construct\n                    error messages\n\n        Raises:\n            KeyNotInContextError: Key doesn't exist\n            KeyInContextHasNoValueError: context[key] is None\n            AssertionError: if key is None\n\n        \"\"\"\n        assert parent, (\"parent parameter must be specified.\")\n        assert child, (\"child parameter must be specified.\")\n        self.assert_key_has_value(parent, caller)\n\n        try:\n            child_exists = child in self[parent]\n        except TypeError as err:\n            # This happens if parent isn't iterable\n            raise ContextError(\n                f\"context['{parent}'] must be iterable and contain '{child}' \"\n                f\"for {caller}. {err}\") from err\n\n        if child_exists:\n            if self[parent][child] is None:\n                raise KeyInContextHasNoValueError(\n                    f\"context['{parent}']['{child}'] must have a value for \"\n                    f\"{caller}.\")\n        else:\n            raise KeyNotInContextError(\n                f\"context['{parent}']['{child}'] doesn't \"\n                f\"exist. It must exist for {caller}.\")",
    "reference": "Assert that context contains key that has child which has a value.\n\n        Args:\n            parent: parent key\n            child: validate this sub-key of parent exists AND isn't None.\n            caller: string. calling function name - this used to construct\n                    error messages\n\n        Raises:\n            KeyNotInContextError: Key doesn't exist\n            KeyInContextHasNoValueError: context[key] is None\n            AssertionError: if key is None",
    "generated": "Assert that context contains key that has child which has a value.\n\n        Args:\n            parent: parent key\n            child: validate this sub-key of parent exists AND isn't None.\n            caller: string. calling function name - this used to construct\n                    error messages\n\n        Raises:\n            KeyNotInContextError: Key doesn't exist\n            KeyInContextHasNoValueError: context[key] is None\n            AssertionError: if key is None"
  },
  {
    "code": "def assert_key_has_value(self, key, caller):\n        \"\"\"Assert that context contains key which also has a value.\n\n        Args:\n            key: validate this key exists in context AND has a value that isn't\n                 None.\n            caller: string. calling function name - this used to construct\n                    error messages\n\n        Raises:\n            KeyNotInContextError: Key doesn't exist\n            KeyInContextHasNoValueError: context[key] is None\n            AssertionError: if key is None\n\n        \"\"\"\n        assert key, (\"key parameter must be specified.\")\n        self.assert_key_exists(key, caller)\n\n        if self[key] is None:\n            raise KeyInContextHasNoValueError(\n                f\"context['{key}'] must have a value for {caller}.\")",
    "reference": "Assert that context contains key which also has a value.\n\n        Args:\n            key: validate this key exists in context AND has a value that isn't\n                 None.\n            caller: string. calling function name - this used to construct\n                    error messages\n\n        Raises:\n            KeyNotInContextError: Key doesn't exist\n            KeyInContextHasNoValueError: context[key] is None\n            AssertionError: if key is None",
    "generated": "Assert that context contains key which also also has a value that isn't\n                 None.\n            caller: string. calling function name - this used to construct\n                    error messages\n\n        Raises:\n            KeyNotInContextError: Key doesn't exist\n            KeyInContextHasNoValueError: context[key] is None\n            AssertionError: if key is None"
  },
  {
    "code": "def assert_keys_exist(self, caller, *keys):\n        \"\"\"Assert that context contains keys.\n\n        Args:\n            keys: validates that these keys exists in context\n            caller: string. calling function or module name - this used to\n                    construct error messages\n\n        Raises:\n            KeyNotInContextError: When key doesn't exist in context.\n\n        \"\"\"\n        assert keys, (\"*keys parameter must be specified.\")\n        for key in keys:\n            self.assert_key_exists(key, caller)",
    "reference": "Assert that context contains keys.\n\n        Args:\n            keys: validates that these keys exists in context\n            caller: string. calling function or module name - this used to\n                    construct error messages\n\n        Raises:\n            KeyNotInContextError: When key doesn't exist in context.",
    "generated": "Assert that context contains keys.\n\n        Args:\n            keys: validates that these keys exists in context\n            caller: string. calling function or module name - this used to\n                    construct error messages\n\n        Raises:\n            KeyNotInContextError: When key doesn't exist in context."
  },
  {
    "code": "def assert_keys_have_values(self, caller, *keys):\n        \"\"\"Check that keys list are all in context and all have values.\n\n        Args:\n            *keys: Will check each of these keys in context\n            caller: string. Calling function name - just used for informational\n                    messages\n\n        Raises:\n            KeyNotInContextError: Key doesn't exist\n            KeyInContextHasNoValueError: context[key] is None\n            AssertionError: if *keys is None\n\n        \"\"\"\n        for key in keys:\n            self.assert_key_has_value(key, caller)",
    "reference": "Check that keys list are all in context and all have values.\n\n        Args:\n            *keys: Will check each of these keys in context\n            caller: string. Calling function name - just used for informational\n                    messages\n\n        Raises:\n            KeyNotInContextError: Key doesn't exist\n            KeyInContextHasNoValueError: context[key] is None\n            AssertionError: if *keys is None",
    "generated": "Check that keys list are all in context and all have values.\n\n        Args:\n            *keys: Will check each of these keys in context\n            caller: string. Calling function name - just used for informational\n                    messages\n        Raises:\n            KeyNotInContextError: Key doesn't exist\n            KeyInContextHasNoValueError: context[key] is None"
  },
  {
    "code": "def get_formatted_iterable(self, obj, memo=None):\n        \"\"\"Recursively loop through obj, formatting as it goes.\n\n        Interpolates strings from the context dictionary.\n\n        This is not a full on deepcopy, and it's on purpose not a full on\n        deepcopy. It will handle dict, list, set, tuple for iteration, without\n        any especial cuteness for other types or types not derived from these.\n\n        For lists: if value is a string, format it.\n        For dicts: format key. If value str, format it.\n        For sets/tuples: if type str, format it.\n\n        This is what formatting or interpolating a string means:\n        So where a string like this 'Piping {key1} the {key2} wild'\n        And context={'key1': 'down', 'key2': 'valleys', 'key3': 'value3'}\n\n        Then this will return string: \"Piping down the valleys wild\"\n\n        Args:\n            obj: iterable. Recurse through and format strings found in\n                           dicts, lists, tuples. Does not mutate the input\n                           iterable.\n            memo: dict. Don't use. Used internally on recursion to optimize\n                        recursive loops.\n\n        Returns:\n            Iterable identical in structure to the input iterable.\n\n        \"\"\"\n        if memo is None:\n            memo = {}\n\n        obj_id = id(obj)\n        already_done = memo.get(obj_id, None)\n        if already_done is not None:\n            return already_done\n\n        if isinstance(obj, str):\n            new = self.get_formatted_string(obj)\n        elif isinstance(obj, SpecialTagDirective):\n            new = obj.get_value(self)\n        elif isinstance(obj, (bytes, bytearray)):\n            new = obj\n        elif isinstance(obj, Mapping):\n            # dicts\n            new = obj.__class__()\n            for k, v in obj.items():\n                new[self.get_formatted_string(\n                    k)] = self.get_formatted_iterable(v, memo)\n        elif isinstance(obj, (Sequence, Set)):\n            # list, set, tuple. Bytes and str won't fall into this branch coz\n            # they're expicitly checked further up in the if.\n            new = obj.__class__(self.get_formatted_iterable(v, memo)\n                                for v in obj)\n        else:\n            # int, float, bool, function, et.\n            return obj\n\n        # If is its own copy, don't memoize.\n        if new is not obj:\n            memo[obj_id] = new\n\n        return new",
    "reference": "Recursively loop through obj, formatting as it goes.\n\n        Interpolates strings from the context dictionary.\n\n        This is not a full on deepcopy, and it's on purpose not a full on\n        deepcopy. It will handle dict, list, set, tuple for iteration, without\n        any especial cuteness for other types or types not derived from these.\n\n        For lists: if value is a string, format it.\n        For dicts: format key. If value str, format it.\n        For sets/tuples: if type str, format it.\n\n        This is what formatting or interpolating a string means:\n        So where a string like this 'Piping {key1} the {key2} wild'\n        And context={'key1': 'down', 'key2': 'valleys', 'key3': 'value3'}\n\n        Then this will return string: \"Piping down the valleys wild\"\n\n        Args:\n            obj: iterable. Recurse through and format strings found in\n                           dicts, lists, tuples. Does not mutate the input\n                           iterable.\n            memo: dict. Don't use. Used internally on recursion to optimize\n                        recursive loops.\n\n        Returns:\n            Iterable identical in structure to the input iterable.",
    "generated": "Recursively loop through obj, formatting as it goes.\n\n        Interpolates strings from the context dictionary.\n\n        This is not a full on deepcopy, and it's on purpose not a full on\n        deepcopy. It will handle dict, list, set, tuple for iteration, without\n        any especial cuteness for other types or types not derived from these.\n\n        For lists: if value is a string, format it.\n        For dicts: format key. If value str, format it.\n        For sets/tuples: if type str, format it.\n        For dicts: format"
  },
  {
    "code": "def get_formatted_string(self, input_string):\n        \"\"\"Return formatted value for input_string.\n\n        get_formatted gets a context[key] value.\n        get_formatted_string is for any arbitrary string that is not in the\n        context.\n\n        Only valid if input_string is a type string.\n        Return a string interpolated from the context dictionary.\n\n        If input_string='Piping {key1} the {key2} wild'\n        And context={'key1': 'down', 'key2': 'valleys', 'key3': 'value3'}\n\n        Then this will return string: \"Piping down the valleys wild\"\n\n        Args:\n            input_string: string to parse for substitutions.\n\n        Returns:\n            Formatted string.\n\n        Raises:\n            KeyNotInContextError: context[key] has {somekey} where somekey does\n                                  not exist in context dictionary.\n            TypeError: Attempt operation on a non-string type.\n\n        \"\"\"\n        if isinstance(input_string, str):\n            try:\n                return self.get_processed_string(input_string)\n            except KeyNotInContextError as err:\n                # Wrapping the KeyError into a less cryptic error for end-user\n                # friendliness\n                raise KeyNotInContextError(\n                    f'Unable to format \\'{input_string}\\' because {err}'\n                ) from err\n        elif isinstance(input_string, SpecialTagDirective):\n            return input_string.get_value(self)\n        else:\n            raise TypeError(f\"can only format on strings. {input_string} is a \"\n                            f\"{type(input_string)} instead.\")",
    "reference": "Return formatted value for input_string.\n\n        get_formatted gets a context[key] value.\n        get_formatted_string is for any arbitrary string that is not in the\n        context.\n\n        Only valid if input_string is a type string.\n        Return a string interpolated from the context dictionary.\n\n        If input_string='Piping {key1} the {key2} wild'\n        And context={'key1': 'down', 'key2': 'valleys', 'key3': 'value3'}\n\n        Then this will return string: \"Piping down the valleys wild\"\n\n        Args:\n            input_string: string to parse for substitutions.\n\n        Returns:\n            Formatted string.\n\n        Raises:\n            KeyNotInContextError: context[key] has {somekey} where somekey does\n                                  not exist in context dictionary.\n            TypeError: Attempt operation on a non-string type.",
    "generated": "Return formatted value for input_string.\n\n        get_formatted gets a context[key] value.\n        get_formatted_string is for any arbitrary string that is not in the\n        context.\n\n        Only valid if input_string is a type string.\n        Return a string interpolated from the context dictionary.\n            TypeError: Attempt operation on a non-string type."
  },
  {
    "code": "def get_formatted_as_type(self, value, default=None, out_type=str):\n        \"\"\"Return formatted value for input value, returns as out_type.\n\n        Caveat emptor: if out_type is bool and value a string,\n        return will be True if str is 'True'. It will be False for all other\n        cases.\n\n        Args:\n            value: the value to format\n            default: if value is None, set to this\n            out_type: cast return as this type\n\n        Returns:\n            Formatted value of type out_type\n\n        \"\"\"\n        if value is None:\n            value = default\n\n        if isinstance(value, SpecialTagDirective):\n            result = value.get_value(self)\n            return types.cast_to_type(result, out_type)\n        if isinstance(value, str):\n            result = self.get_formatted_string(value)\n            result_type = type(result)\n            if out_type is result_type:\n                # get_formatted_string result is already a string\n                return result\n            elif out_type is bool and result_type is str:\n                # casting a str to bool is always True, hence special case. If\n                # the str value is 'False'/'false', presumably user can\n                # reasonably expect a bool False response.\n                return result.lower() in ['true', '1', '1.0']\n            else:\n                return out_type(result)\n        else:\n            return out_type(value)",
    "reference": "Return formatted value for input value, returns as out_type.\n\n        Caveat emptor: if out_type is bool and value a string,\n        return will be True if str is 'True'. It will be False for all other\n        cases.\n\n        Args:\n            value: the value to format\n            default: if value is None, set to this\n            out_type: cast return as this type\n\n        Returns:\n            Formatted value of type out_type",
    "generated": "Return formatted value for input value, returns as out_type.\n\n        Caveat emptor: if value is None, set to this\n            out_type: cast return as this type\n\n        Returns:\n            Formatted value of type out_type"
  },
  {
    "code": "def get_processed_string(self, input_string):\n        \"\"\"Run token substitution on input_string against context.\n\n        You probably don't want to call this directly yourself - rather use\n        get_formatted, get_formatted_iterable, or get_formatted_string because\n        these contain more friendly error handling plumbing and context logic.\n\n        If you do want to call it yourself, go for it, it doesn't touch state.\n\n        If input_string='Piping {key1} the {key2} wild'\n        And context={'key1': 'down', 'key2': 'valleys', 'key3': 'value3'}\n\n        An input string with a single formatting expression and nothing else\n        will return the object at that context path: input_string='{key1}'.\n        This means that the return obj will be the same type as the source\n        object. This return object in itself has token substitions run on it\n        iteratively.\n\n        By comparison, multiple formatting expressions and/or the inclusion of\n        literal text will result in a string return type:\n        input_string='{key1} literal text {key2}'\n\n        Then this will return string: \"Piping down the valleys wild\"\n\n        Args:\n            input_string: string to Parse\n\n        Returns:\n            any given type: Formatted string with {substitutions} made from\n            context. If it's a !sic string, x from !sic x, with no\n            substitutions made on x. If input_string was a single expression\n            (e.g '{field}'), then returns the object with {substitutions} made\n            for its attributes.\n\n        Raises:\n            KeyNotInContextError: input_string is not a sic string and has\n                                  {somekey} where somekey does not exist in\n                                  context dictionary.\n\n        \"\"\"\n        # arguably, this doesn't really belong here, or at least it makes a\n        # nonsense of the function name. given how py and strings\n        # look and feel pretty much like strings from user's perspective, and\n        # given legacy code back when sic strings were in fact just strings,\n        # keep in here for backwards compatibility.\n        if isinstance(input_string, SpecialTagDirective):\n            return input_string.get_value(self)\n        else:\n            # is this a special one field formatstring? i.e \"{field}\", with\n            # nothing else?\n            out = None\n            is_out_set = False\n            expr_count = 0\n            # parse finds field format expressions and/or literals in input\n            for expression in formatter.parse(input_string):\n                # parse tuple:\n                # (literal_text, field_name, format_spec, conversion)\n                # it's a single '{field}' if no literal_text but field_name\n                # no literal, field name exists, and no previous expr found\n                if (not expression[0] and expression[1] and not expr_count):\n                    # get_field tuple: (obj, used_key)\n                    out = formatter.get_field(expression[1], None, self)[0]\n                    # second flag necessary because a literal with no format\n                    # expression will still result in expr_count == 1\n                    is_out_set = True\n\n                expr_count += 1\n\n                # this is a little bit clumsy, but you have to consume the\n                # iterator to get the count. Interested in 1 and only 1 field\n                # expressions with no literal text: have to loop to see if\n                # there is >1.\n                if expr_count > 1:\n                    break\n\n            if is_out_set and expr_count == 1:\n                # found 1 and only 1. but this could be an iterable obj\n                # that needs formatting rules run on it in itself\n                return self.get_formatted_iterable(out)\n            else:\n                return input_string.format_map(self)",
    "reference": "Run token substitution on input_string against context.\n\n        You probably don't want to call this directly yourself - rather use\n        get_formatted, get_formatted_iterable, or get_formatted_string because\n        these contain more friendly error handling plumbing and context logic.\n\n        If you do want to call it yourself, go for it, it doesn't touch state.\n\n        If input_string='Piping {key1} the {key2} wild'\n        And context={'key1': 'down', 'key2': 'valleys', 'key3': 'value3'}\n\n        An input string with a single formatting expression and nothing else\n        will return the object at that context path: input_string='{key1}'.\n        This means that the return obj will be the same type as the source\n        object. This return object in itself has token substitions run on it\n        iteratively.\n\n        By comparison, multiple formatting expressions and/or the inclusion of\n        literal text will result in a string return type:\n        input_string='{key1} literal text {key2}'\n\n        Then this will return string: \"Piping down the valleys wild\"\n\n        Args:\n            input_string: string to Parse\n\n        Returns:\n            any given type: Formatted string with {substitutions} made from\n            context. If it's a !sic string, x from !sic x, with no\n            substitutions made on x. If input_string was a single expression\n            (e.g '{field}'), then returns the object with {substitutions} made\n            for its attributes.\n\n        Raises:\n            KeyNotInContextError: input_string is not a sic string and has\n                                  {somekey} where somekey does not exist in\n                                  context dictionary.",
    "generated": "Run token substitution on input_string against context.\n\n        You probably don't want to call this directly yourself - rather use\n        get_formatted, get_formatted_iterable, or get_formatted_string because\n        these contain more friendly error handling plumbing and context logic.\n\n        If you do want to call it yourself, go for it, it doesn't touch state.\n\n        If input_string='Piping {key1} the {key2} wild'\n        And context={'key1': 'down', 'key2': 'down', 'key3':"
  },
  {
    "code": "def keys_of_type_exist(self, *keys):\n        \"\"\"Check if keys exist in context and if types are as expected.\n\n        Args:\n            *keys: *args for keys to check in context.\n                   Each arg is a tuple (str, type)\n\n        Returns:\n            Tuple of namedtuple ContextItemInfo, same order as *keys.\n            ContextItemInfo(key,\n                            key_in_context,\n                            expected_type,\n                            is_expected_type)\n\n            Remember if there is only one key in keys, the return assignment\n            needs an extra comma to remind python that it's a tuple:\n            # one\n            a, = context.keys_of_type_exist('a')\n            # > 1\n            a, b = context.keys_of_type_exist('a', 'b')\n\n        \"\"\"\n        # k[0] = key name, k[1] = exists, k2 = expected type\n        keys_exist = [(key, key in self.keys(), expected_type)\n                      for key, expected_type in keys]\n\n        return tuple(ContextItemInfo(\n            key=k[0],\n            key_in_context=k[1],\n            expected_type=k[2],\n            is_expected_type=isinstance(self[k[0]], k[2])\n            if k[1] else None,\n            has_value=k[1] and not self[k[0]] is None\n        ) for k in keys_exist)",
    "reference": "Check if keys exist in context and if types are as expected.\n\n        Args:\n            *keys: *args for keys to check in context.\n                   Each arg is a tuple (str, type)\n\n        Returns:\n            Tuple of namedtuple ContextItemInfo, same order as *keys.\n            ContextItemInfo(key,\n                            key_in_context,\n                            expected_type,\n                            is_expected_type)\n\n            Remember if there is only one key in keys, the return assignment\n            needs an extra comma to remind python that it's a tuple:\n            # one\n            a, = context.keys_of_type_exist('a')\n            # > 1\n            a, b = context.keys_of_type_exist('a', 'b')",
    "generated": "Check if keys exist in context and if types are as expected.\n\n        Args:\n            *keys: *args for keys to check in context.\n                   Each arg is a tuple (str, type)"
  },
  {
    "code": "def merge(self, add_me):\n        \"\"\"Merge add_me into context and applies interpolation.\n\n        Bottom-up merge where add_me merges into context. Applies string\n        interpolation where the type is a string. Where a key exists in\n        context already, add_me's value will overwrite what's in context\n        already.\n\n        Supports nested hierarchy. add_me can contains dicts/lists/enumerables\n        that contain other enumerables et. It doesn't restrict levels of\n        nesting, so if you really want to go crazy with the levels you can, but\n        you might blow your stack.\n\n        If something from add_me exists in context already, but add_me's value\n        is of a different type, add_me will overwrite context. Do note this.\n        i.e if you had context['int_key'] == 1 and\n        add_me['int_key'] == 'clearly not a number', the end result would be\n        context['int_key'] == 'clearly not a number'\n\n        If add_me contains lists/sets/tuples, this merges these\n        additively, meaning it appends values from add_me to the existing\n        sequence.\n\n        Args:\n            add_me: dict. Merge this dict into context.\n\n        Returns:\n            None. All operations mutate this instance of context.\n\n        \"\"\"\n        def merge_recurse(current, add_me):\n            \"\"\"Walk the current context tree in recursive inner function.\n\n            On 1st iteration, current = self (i.e root of context)\n            On subsequent recursive iterations, current is wherever you're at\n            in the nested context hierarchy.\n\n            Args:\n                current: dict. Destination of merge.\n                add_me: dict. Merge this to current.\n            \"\"\"\n            for k, v in add_me.items():\n                # key supports interpolation\n                k = self.get_formatted_string(k)\n\n                # str not mergable, so it doesn't matter if it exists in dest\n                if isinstance(v, str):\n                    # just overwrite dest - str adds/edits indiscriminately\n                    current[k] = self.get_formatted_string(v)\n                elif isinstance(v, (bytes, bytearray)):\n                    # bytes aren't mergable or formattable\n                    # only here to prevent the elif on enumerables catching it\n                    current[k] = v\n                # deal with things that are mergable - exists already in dest\n                elif k in current:\n                    if types.are_all_this_type(Mapping, current[k], v):\n                        # it's dict-y, thus recurse through it to merge since\n                        # it exists in dest\n                        merge_recurse(current[k], v)\n                    elif types.are_all_this_type(list, current[k], v):\n                        # it's list-y. Extend mutates existing list since it\n                        # exists in dest\n                        current[k].extend(\n                            self.get_formatted_iterable(v))\n                    elif types.are_all_this_type(tuple, current[k], v):\n                        # concatenate tuples\n                        current[k] = (\n                            current[k] + self.get_formatted_iterable(v))\n                    elif types.are_all_this_type(Set, current[k], v):\n                        # join sets\n                        current[k] = (\n                            current[k] | self.get_formatted_iterable(v))\n                    else:\n                        # at this point it's not mergable nor a known iterable\n                        current[k] = v\n                else:\n                    # at this point it's not mergable, nor in context\n                    current[k] = self.get_formatted_iterable(v)\n\n        # first iteration starts at context dict root\n        merge_recurse(self, add_me)",
    "reference": "Merge add_me into context and applies interpolation.\n\n        Bottom-up merge where add_me merges into context. Applies string\n        interpolation where the type is a string. Where a key exists in\n        context already, add_me's value will overwrite what's in context\n        already.\n\n        Supports nested hierarchy. add_me can contains dicts/lists/enumerables\n        that contain other enumerables et. It doesn't restrict levels of\n        nesting, so if you really want to go crazy with the levels you can, but\n        you might blow your stack.\n\n        If something from add_me exists in context already, but add_me's value\n        is of a different type, add_me will overwrite context. Do note this.\n        i.e if you had context['int_key'] == 1 and\n        add_me['int_key'] == 'clearly not a number', the end result would be\n        context['int_key'] == 'clearly not a number'\n\n        If add_me contains lists/sets/tuples, this merges these\n        additively, meaning it appends values from add_me to the existing\n        sequence.\n\n        Args:\n            add_me: dict. Merge this dict into context.\n\n        Returns:\n            None. All operations mutate this instance of context.",
    "generated": "Merge add_me into context and applies interpolation.\n\n        Bottom-up merge where add_me merges into context. Applies string\n        interpolation where the type is a string. Where a key exists in\n        context already, add_me's value\n        is of a different type, add_me will overwrite context. Do note this.\n        i.e if you had context['int_key'] == 1 and\n        add_me['int_key'] == 1 and\n        add_me['int_key'] == 1 and\n        add_me's value\n        is of"
  },
  {
    "code": "def set_defaults(self, defaults):\n        \"\"\"Set defaults in context if keys do not exist already.\n\n        Adds the input dict (defaults) into the context, only where keys in\n        defaults do not already exist in context. Supports nested hierarchies.\n\n        Example:\n        Given a context like this:\n            key1: value1\n            key2:\n                key2.1: value2.1\n            key3: None\n\n        And defaults input like this:\n            key1: 'updated value here won't overwrite since it already exists'\n            key2:\n                key2.2: value2.2\n            key3: 'key 3 exists so I won't overwrite\n\n        Will result in context:\n            key1: value1\n            key2:\n                key2.1: value2.1\n                key2.2: value2.2\n            key3: None\n\n        Args:\n            defaults: dict. Add this dict into context.\n\n        Returns:\n            None. All operations mutate this instance of context.\n\n        \"\"\"\n        def defaults_recurse(current, defaults):\n            \"\"\"Walk the current context tree in recursive inner function.\n\n            On 1st iteration, current = self (i.e root of context)\n            On subsequent recursive iterations, current is wherever you're at\n            in the nested context hierarchy.\n\n            Args:\n                current: dict. Destination of merge.\n                defaults: dict. Add this to current if keys don't exist\n                                already.\n\n            \"\"\"\n            for k, v in defaults.items():\n                # key supports interpolation\n                k = self.get_formatted_string(k)\n\n                if k in current:\n                    if types.are_all_this_type(Mapping, current[k], v):\n                        # it's dict-y, thus recurse through it to check if it\n                        # contains child items that don't exist in dest\n                        defaults_recurse(current[k], v)\n                else:\n                    # since it's not in context already, add the default\n                    current[k] = self.get_formatted_iterable(v)\n\n        # first iteration starts at context dict root\n        defaults_recurse(self, defaults)",
    "reference": "Set defaults in context if keys do not exist already.\n\n        Adds the input dict (defaults) into the context, only where keys in\n        defaults do not already exist in context. Supports nested hierarchies.\n\n        Example:\n        Given a context like this:\n            key1: value1\n            key2:\n                key2.1: value2.1\n            key3: None\n\n        And defaults input like this:\n            key1: 'updated value here won't overwrite since it already exists'\n            key2:\n                key2.2: value2.2\n            key3: 'key 3 exists so I won't overwrite\n\n        Will result in context:\n            key1: value1\n            key2:\n                key2.1: value2.1\n                key2.2: value2.2\n            key3: None\n\n        Args:\n            defaults: dict. Add this dict into context.\n\n        Returns:\n            None. All operations mutate this instance of context.",
    "generated": "Set defaults in context if keys do not exist already.\n\n        Adds the input dict (defaults) into the context, only where keys in\n        defaults do not already exist in context. Supports nested hierarchies.\n\n        Example:\n        Given a context like this:\n            key1: value1\n            key2:\n                key2.1: value2.1\n            key3: None\n\n        And defaults input like this:\n            key1: 'updated value here won't overwrite since it already exists'\n            key2: value2.2\n            key3"
  },
  {
    "code": "def run_step(self, rewriter):\n        \"\"\"Do the file in to out rewrite.\n\n        Doesn't do anything more crazy than call files_in_to_out on the\n        rewriter.\n\n        Args:\n            rewriter: pypyr.filesystem.FileRewriter instance.\n        \"\"\"\n        assert rewriter, (\"FileRewriter instance required to run \"\n                          \"FileInRewriterStep.\")\n        rewriter.files_in_to_out(in_path=self.path_in, out_path=self.path_out)",
    "reference": "Do the file in to out rewrite.\n\n        Doesn't do anything more crazy than call files_in_to_out on the\n        rewriter.\n\n        Args:\n            rewriter: pypyr.filesystem.FileRewriter instance.",
    "generated": "Do the file in to out rewrite.\n\n        Doesn't do anything more crazy than call files_in_to_out on the\n        rewriter.\n\n        Args:\n            rewriter: pypyr.filesystem.FileRewriter instance."
  },
  {
    "code": "def run_step(self, representer):\n        \"\"\"Do the object in-out rewrite.\n\n        Args:\n            representer: A pypyr.filesystem.ObjectRepresenter instance.\n\n        \"\"\"\n        assert representer, (\"ObjectRepresenter instance required to run \"\n                             \"ObjectRewriterStep.\")\n        rewriter = ObjectRewriter(self.context.get_formatted_iterable,\n                                  representer)\n        super().run_step(rewriter)",
    "reference": "Do the object in-out rewrite.\n\n        Args:\n            representer: A pypyr.filesystem.ObjectRepresenter instance.",
    "generated": "Do the object in-out rewrite.\n\n        Args:\n            representer: A pypyr.filesystem.ObjectRepresenter instance."
  },
  {
    "code": "def run_step(self):\n        \"\"\"Do the file in-out rewrite.\"\"\"\n        rewriter = StreamRewriter(self.context.iter_formatted_strings)\n        super().run_step(rewriter)",
    "reference": "Do the file in-out rewrite.",
    "generated": "Do the file in-out rewrite."
  },
  {
    "code": "def run_step(self):\n        \"\"\"Write in to out, replacing strings per the replace_pairs.\"\"\"\n        formatted_replacements = self.context.get_formatted_iterable(\n            self.replace_pairs)\n\n        iter = StreamReplacePairsRewriterStep.iter_replace_strings(\n            formatted_replacements)\n        rewriter = StreamRewriter(iter)\n        super().run_step(rewriter)",
    "reference": "Write in to out, replacing strings per the replace_pairs.",
    "generated": "Write in to out, replacing strings per the replace_pairs"
  },
  {
    "code": "def iter_replace_strings(replacements):\n        \"\"\"Create a function that uses replacement pairs to process a string.\n\n        The returned function takes an iterator and yields on each processed\n        line.\n\n        Args:\n            replacements: Dict containing 'find_string': 'replace_string' pairs\n\n        Returns:\n            function with signature: iterator of strings = function(iterable)\n\n        \"\"\"\n        def function_iter_replace_strings(iterable_strings):\n            \"\"\"Yield a formatted string from iterable_strings using a generator.\n\n            Args:\n                iterable_strings: Iterable containing strings. E.g a file-like\n                                  object.\n\n            Returns:\n                Yields formatted line.\n\n            \"\"\"\n            for string in iterable_strings:\n                yield reduce((lambda s, kv: s.replace(*kv)),\n                             replacements.items(),\n                             string)\n\n        return function_iter_replace_strings",
    "reference": "Create a function that uses replacement pairs to process a string.\n\n        The returned function takes an iterator and yields on each processed\n        line.\n\n        Args:\n            replacements: Dict containing 'find_string': 'replace_string' pairs\n\n        Returns:\n            function with signature: iterator of strings = function(iterable)",
    "generated": "Create a function that uses replacement pairs to process a string.\n\n        The returned function takes an iterator and yields on each processed\n        line.\n\n        Args:\n            replacements: Dict containing 'find_string': 'replace_string' pairs\n\n        Returns:\n            function with signature: iterator of strings"
  },
  {
    "code": "def run_step(context):\n    \"\"\"Set new context keys from formatting expressions with substitutions.\n\n    Context is a dictionary or dictionary-like.\n    context['contextSetf'] must exist. It's a dictionary.\n    Will iterate context['contextSetf'] and save the values as new keys to the\n    context.\n\n    For example, say input context is:\n        key1: value1\n        key2: value2\n        key3: value3\n        contextSetf:\n            key2: 'aaa_{key1}_zzz'\n            key4: 'bbb_{key3}_yyy'\n\n    This will result in return context:\n        key1: value1\n        key2: aaa_value1_zzz\n        key3: bbb_value3_yyy\n        key4: value3\n    \"\"\"\n    logger.debug(\"started\")\n    context.assert_key_has_value(key='contextSetf', caller=__name__)\n\n    for k, v in context['contextSetf'].items():\n        logger.debug(f\"setting context {k} to value from context {v}\")\n        context[context.get_formatted_iterable(\n            k)] = context.get_formatted_iterable(v)\n\n    logger.info(f\"Set {len(context['contextSetf'])} context items.\")\n\n    logger.debug(\"done\")",
    "reference": "Set new context keys from formatting expressions with substitutions.\n\n    Context is a dictionary or dictionary-like.\n    context['contextSetf'] must exist. It's a dictionary.\n    Will iterate context['contextSetf'] and save the values as new keys to the\n    context.\n\n    For example, say input context is:\n        key1: value1\n        key2: value2\n        key3: value3\n        contextSetf:\n            key2: 'aaa_{key1}_zzz'\n            key4: 'bbb_{key3}_yyy'\n\n    This will result in return context:\n        key1: value1\n        key2: aaa_value1_zzz\n        key3: bbb_value3_yyy\n        key4: value3",
    "generated": "Set new context keys from formatting expressions with substitutions.\n\n    Context is a dictionary or dictionary-like.\n    context['contextSetf'] must exist. It's a dictionary."
  },
  {
    "code": "def cast_to_type(obj, out_type):\n    \"\"\"Cast obj to out_type if it's not out_type already.\n\n    If the obj happens to be out_type already, it just returns obj as is.\n\n    Args:\n        obj: input object\n        out_type: type.\n\n    Returns:\n        obj cast to out_type. Usual python conversion / casting rules apply.\n\n    \"\"\"\n    in_type = type(obj)\n    if out_type is in_type:\n        # no need to cast.\n        return obj\n    else:\n        return out_type(obj)",
    "reference": "Cast obj to out_type if it's not out_type already.\n\n    If the obj happens to be out_type already, it just returns obj as is.\n\n    Args:\n        obj: input object\n        out_type: type.\n\n    Returns:\n        obj cast to out_type. Usual python conversion / casting rules apply.",
    "generated": "Cast obj to out_type if it's not out_type already."
  },
  {
    "code": "def get_pipeline_yaml(file):\n    \"\"\"Return pipeline yaml from open file object.\n\n    Use specific custom representers to model the custom pypyr pipeline yaml\n    format, to load in special literal types like py and sic strings.\n\n    If looking to extend the pypyr pipeline syntax with special types, add\n    these to the tag_representers list.\n\n    Args:\n        file: open file-like object.\n\n    Returns:\n        dict-like representation of loaded yaml.\n\n    \"\"\"\n    tag_representers = [PyString, SicString]\n\n    yaml_loader = get_yaml_parser_safe()\n\n    for representer in tag_representers:\n        yaml_loader.register_class(representer)\n\n    pipeline_definition = yaml_loader.load(file)\n    return pipeline_definition",
    "reference": "Return pipeline yaml from open file object.\n\n    Use specific custom representers to model the custom pypyr pipeline yaml\n    format, to load in special literal types like py and sic strings.\n\n    If looking to extend the pypyr pipeline syntax with special types, add\n    these to the tag_representers list.\n\n    Args:\n        file: open file-like object.\n\n    Returns:\n        dict-like representation of loaded yaml.",
    "generated": "Return pipeline yaml from open file object.\n\n    Use specific custom representers to model the custom pypyr pipeline yaml\n    format, to load in special literal types like py and sic strings."
  },
  {
    "code": "def get_yaml_parser_roundtrip():\n    \"\"\"Create the yaml parser object with this factory method.\n\n    The round-trip parser preserves:\n    - comments\n    - block style and key ordering are kept, so you can diff the round-tripped\n      source\n    - flow style sequences ( \u2018a: b, c, d\u2019) (based on request and test by\n      Anthony Sottile)\n    - anchor names that are hand-crafted (i.e. not of the form``idNNN``)\n    - merges in dictionaries are preserved\n\n    Returns:\n        ruamel.yaml.YAML object with round-trip loader\n\n    \"\"\"\n    yaml_writer = yamler.YAML(typ='rt', pure=True)\n    # if this isn't here the yaml doesn't format nicely indented for humans\n    yaml_writer.indent(mapping=2, sequence=4, offset=2)\n    return yaml_writer",
    "reference": "Create the yaml parser object with this factory method.\n\n    The round-trip parser preserves:\n    - comments\n    - block style and key ordering are kept, so you can diff the round-tripped\n      source\n    - flow style sequences ( \u2018a: b, c, d\u2019) (based on request and test by\n      Anthony Sottile)\n    - anchor names that are hand-crafted (i.e. not of the form``idNNN``)\n    - merges in dictionaries are preserved\n\n    Returns:\n        ruamel.yaml.YAML object with round-trip loader",
    "generated": "Create the yaml parser with this factory method.\n\n    The round-trip parser preserves:\n    - comments\n    - block style and key ordering are kept"
  },
  {
    "code": "def get_yaml_parser_roundtrip_for_context():\n    \"\"\"Create a yaml parser that can serialize the pypyr Context.\n\n    Create yaml parser with get_yaml_parser_roundtrip, adding Context.\n    This allows the yaml parser to serialize the pypyr Context.\n    \"\"\"\n    yaml_writer = get_yaml_parser_roundtrip()\n\n    # Context is a dict data structure, so can just use a dict representer\n    yaml_writer.Representer.add_representer(\n        Context,\n        yamler.representer.RoundTripRepresenter.represent_dict)\n\n    return yaml_writer",
    "reference": "Create a yaml parser that can serialize the pypyr Context.\n\n    Create yaml parser with get_yaml_parser_roundtrip, adding Context.\n    This allows the yaml parser to serialize the pypyr Context.",
    "generated": "Create a yaml parser that can serialize the pypyr Context.\n\n    Create yaml parser with get_yaml_parser_roundtrip, adding Context.\n    This allows the yaml parser to serialize the pypyr Context."
  },
  {
    "code": "def run_step(context):\n    \"\"\"Load a json file into the pypyr context.\n\n    json parsed from the file will be merged into the pypyr context. This will\n    overwrite existing values if the same keys are already in there.\n    I.e if file json has {'eggs' : 'boiled'} and context {'eggs': 'fried'}\n    already exists, returned context['eggs'] will be 'boiled'.\n\n    The json should not be an array [] on the top level, but rather an Object.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                 The following context key must exist\n                - fetchJson\n                    - path. path-like. Path to file on disk.\n                    - key. string. If exists, write json structure to this\n                      context key. Else json writes to context root.\n\n    Also supports a passing path as string to fetchJson, but in this case you\n    won't be able to specify a key.\n\n    All inputs support formatting expressions.\n\n    Returns:\n        None. updates context arg.\n\n    Raises:\n        FileNotFoundError: take a guess\n        pypyr.errors.KeyNotInContextError: fetchJson.path missing in context.\n        pypyr.errors.KeyInContextHasNoValueError: fetchJson.path exists but is\n                                                  None.\n\n    \"\"\"\n    logger.debug(\"started\")\n\n    deprecated(context)\n\n    context.assert_key_has_value(key='fetchJson', caller=__name__)\n\n    fetch_json_input = context.get_formatted('fetchJson')\n\n    if isinstance(fetch_json_input, str):\n        file_path = fetch_json_input\n        destination_key_expression = None\n    else:\n        context.assert_child_key_has_value(parent='fetchJson',\n                                           child='path',\n                                           caller=__name__)\n        file_path = fetch_json_input['path']\n        destination_key_expression = fetch_json_input.get('key', None)\n\n    logger.debug(f\"attempting to open file: {file_path}\")\n    with open(file_path) as json_file:\n        payload = json.load(json_file)\n\n    if destination_key_expression:\n        destination_key = context.get_formatted_iterable(\n            destination_key_expression)\n        logger.debug(f\"json file loaded. Writing to context {destination_key}\")\n        context[destination_key] = payload\n    else:\n        if not isinstance(payload, MutableMapping):\n            raise TypeError(\n                'json input should describe an object at the top '\n                'level when fetchJsonKey isn\\'t specified. You should have '\n                'something like {\"key1\": \"value1\", \"key2\": \"value2\"} '\n                'in the json top-level, not [\"value1\", \"value2\"]')\n\n        logger.debug(\"json file loaded. Merging into pypyr context. . .\")\n        context.update(payload)\n\n    logger.info(f\"json file written into pypyr context. Count: {len(payload)}\")\n    logger.debug(\"done\")",
    "reference": "Load a json file into the pypyr context.\n\n    json parsed from the file will be merged into the pypyr context. This will\n    overwrite existing values if the same keys are already in there.\n    I.e if file json has {'eggs' : 'boiled'} and context {'eggs': 'fried'}\n    already exists, returned context['eggs'] will be 'boiled'.\n\n    The json should not be an array [] on the top level, but rather an Object.\n\n    Args:\n        context: pypyr.context.Context. Mandatory.\n                 The following context key must exist\n                - fetchJson\n                    - path. path-like. Path to file on disk.\n                    - key. string. If exists, write json structure to this\n                      context key. Else json writes to context root.\n\n    Also supports a passing path as string to fetchJson, but in this case you\n    won't be able to specify a key.\n\n    All inputs support formatting expressions.\n\n    Returns:\n        None. updates context arg.\n\n    Raises:\n        FileNotFoundError: take a guess\n        pypyr.errors.KeyNotInContextError: fetchJson.path missing in context.\n        pypyr.errors.KeyInContextHasNoValueError: fetchJson.path exists but is\n                                                  None.",
    "generated": "Load a json file into the pypyr context.\n\n    json parsed from the file will be merged into the pypyr context. This will\n    overwrite existing values if the same keys are already in there.\n    I.e if file json has {'eggs': 'boiled'} and context {'eggs': 'fried'}\n    already exists, returned context['eggs'] will be 'boiled'.\n\n    The json should not be an array [] on the top level.\n\n   "
  },
  {
    "code": "def _ignore_request(self, path):\n        \"\"\"Check to see if we should ignore the request.\"\"\"\n        return any([\n            re.match(pattern, path) for pattern in QC_SETTINGS['IGNORE_REQUEST_PATTERNS']\n        ])",
    "reference": "Check to see if we should ignore the request.",
    "generated": "Check to see if we should ignore the request."
  },
  {
    "code": "def _ignore_sql(self, query):\n        \"\"\"Check to see if we should ignore the sql query.\"\"\"\n        return any([\n            re.search(pattern, query.get('sql')) for pattern in QC_SETTINGS['IGNORE_SQL_PATTERNS']\n        ])",
    "reference": "Check to see if we should ignore the sql query.",
    "generated": "Check to see if we should ignore the sql query."
  },
  {
    "code": "def _duplicate_queries(self, output):\n        \"\"\"Appends the most common duplicate queries to the given output.\"\"\"\n        if QC_SETTINGS['DISPLAY_DUPLICATES']:\n            for query, count in self.queries.most_common(QC_SETTINGS['DISPLAY_DUPLICATES']):\n                lines = ['\\nRepeated {0} times.'.format(count)]\n                lines += wrap(query)\n                lines = \"\\n\".join(lines) + \"\\n\"\n                output += self._colorize(lines, count)\n        return output",
    "reference": "Appends the most common duplicate queries to the given output.",
    "generated": "Appends the most common duplicate queries to the given output."
  },
  {
    "code": "def _calculate_num_queries(self):\n        \"\"\"\n        Calculate the total number of request and response queries.\n        Used for count header and count table.\n        \"\"\"\n        request_totals = self._totals(\"request\")\n        response_totals = self._totals(\"response\")\n\n        return request_totals[2] + response_totals[2]",
    "reference": "Calculate the total number of request and response queries.\n        Used for count header and count table.",
    "generated": "Calculate the total number of request and response queries.\n        Used for count header and count table."
  },
  {
    "code": "def _process_settings(**kwargs):\n    \"\"\"\n    Apply user supplied settings.\n    \"\"\"\n\n    # If we are in this method due to a signal, only reload for our settings\n    setting_name = kwargs.get('setting', None) \n    if setting_name is not None and setting_name != 'QUERYCOUNT':\n        return\n\n    # Support the old-style settings\n    if getattr(settings, 'QUERYCOUNT_THRESHOLDS', False):\n        QC_SETTINGS['THRESHOLDS'] = settings.QUERYCOUNT_THRESHOLDS\n\n    # Apply new-style settings\n    if not getattr(settings, 'QUERYCOUNT', False):\n        return\n\n    # Duplicate display is a special case, configure it specifically\n    if 'DISPLAY_DUPLICATES' in settings.QUERYCOUNT:\n        duplicate_settings = settings.QUERYCOUNT['DISPLAY_DUPLICATES']\n        if duplicate_settings is not None:\n            duplicate_settings = int(duplicate_settings)\n        QC_SETTINGS['DISPLAY_DUPLICATES'] = duplicate_settings\n\n    # Apply the rest of the setting overrides\n    for key in ['THRESHOLDS',\n                'IGNORE_REQUEST_PATTERNS',\n                'IGNORE_SQL_PATTERNS',\n                'IGNORE_PATTERNS',\n                'RESPONSE_HEADER']:\n        if key in settings.QUERYCOUNT:\n            QC_SETTINGS[key] = settings.QUERYCOUNT[key]",
    "reference": "Apply user supplied settings.",
    "generated": "Apply user supplied settings.\n\n   "
  },
  {
    "code": "def _get_webapi_requests(self):\n        \"\"\"Update headers of webapi for Requests.\"\"\"\n\n        headers = {\n            'Accept':\n            '*/*',\n            'Accept-Language':\n            'zh-CN,zh;q=0.8,gl;q=0.6,zh-TW;q=0.4',\n            'Connection':\n            'keep-alive',\n            'Content-Type':\n            'application/x-www-form-urlencoded',\n            'Referer':\n            'http://music.163.com',\n            'Host':\n            'music.163.com',\n            'User-Agent':\n            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/33.0.1750.152 Safari/537.36'\n        }\n        NCloudBot.req.headers.update(headers)\n        return NCloudBot.req",
    "reference": "Update headers of webapi for Requests.",
    "generated": "Update headers of webapi for Requests."
  },
  {
    "code": "def _build_response(self, resp):\n        \"\"\"Build internal Response object from given response.\"\"\"\n        # rememberLogin\n        # if self.method is 'LOGIN' and resp.json().get('code') == 200:\n        #     cookiesJar.save_cookies(resp, NCloudBot.username)\n        self.response.content = resp.content\n        self.response.status_code = resp.status_code\n        self.response.headers = resp.headers",
    "reference": "Build internal Response object from given response.",
    "generated": "Build internal Response object from given response."
  },
  {
    "code": "def send(self):\n        \"\"\"Sens the request.\"\"\"\n        success = False\n        if self.method is None:\n            raise ParamsError()\n        try:\n            if self.method == 'SEARCH':\n                req = self._get_requests()\n                _url = self.__NETEAST_HOST + self._METHODS[self.method]\n                resp = req.post(_url, data=self.data)\n                self._build_response(resp)\n                self.response.ok = True\n            else:\n                if isinstance(self.data, dict):\n                    data = encrypted_request(self.data)\n\n                req = self._get_webapi_requests()\n                _url = self.__NETEAST_HOST + self._METHODS[self.method]\n\n                if self.method in ('USER_DJ', 'USER_FOLLOWS', 'USER_EVENT'):\n                    _url = _url % self.params['uid']\n\n                if self.method in ('LYRIC', 'MUSIC_COMMENT'):\n                    _url = _url % self.params['id']\n                # GET\n                if self.method in ('LYRIC'):\n                    resp = req.get(_url)\n                else:\n                    resp = req.post(_url, data=data)\n                self._build_response(resp)\n                self.response.ok = True\n        except Exception as why:\n            traceback.print_exc()\n            print 'Requests Exception', why\n            # self._build_response(why)\n            self.response.error = why",
    "reference": "Sens the request.",
    "generated": "Sens the request."
  },
  {
    "code": "def set_option(name, value):\n    \"\"\"\n    Set plydata option\n\n    Parameters\n    ----------\n    name : str\n        Name of the option\n    value : object\n        New value of the option\n\n    Returns\n    -------\n    old : object\n        Old value of the option\n\n    See also\n    --------\n    :class:`options`\n    \"\"\"\n    old = get_option(name)\n    globals()[name] = value\n    return old",
    "reference": "Set plydata option\n\n    Parameters\n    ----------\n    name : str\n        Name of the option\n    value : object\n        New value of the option\n\n    Returns\n    -------\n    old : object\n        Old value of the option\n\n    See also\n    --------\n    :class:`options`",
    "generated": "Set plydata option\n    Parameters\n    ----------\n    name : str\n        Name of the option\n    value : object\n        New value of the option\n    Returns\n    -------\n    : class:`options`"
  },
  {
    "code": "def group_indices(self):\n        \"\"\"\n        Return group indices\n        \"\"\"\n        # No groups\n        if not self.plydata_groups:\n            return np.ones(len(self), dtype=int)\n\n        grouper = self.groupby()\n        indices = np.empty(len(self), dtype=int)\n        for i, (_, idx) in enumerate(sorted(grouper.indices.items())):\n            indices[idx] = i\n        return indices",
    "reference": "Return group indices",
    "generated": "Return group indices"
  },
  {
    "code": "def _make_verb_helper(verb_func, add_groups=False):\n    \"\"\"\n    Create function that prepares verb for the verb function\n\n    The functions created add expressions to be evaluated to\n    the verb, then call the core verb function\n\n    Parameters\n    ----------\n    verb_func : function\n        Core verb function. This is the function called after\n        expressions created and added to the verb. The core\n        function should be one of those that implement verbs that\n        evaluate expressions.\n    add_groups : bool\n        If True, a groups attribute is added to the verb. The\n        groups are the columns created after evaluating the\n        expressions.\n\n    Returns\n    -------\n    out : function\n        A function that implements a helper verb.\n    \"\"\"\n\n    @wraps(verb_func)\n    def _verb_func(verb):\n        verb.expressions, new_columns = build_expressions(verb)\n        if add_groups:\n            verb.groups = new_columns\n        return verb_func(verb)\n\n    return _verb_func",
    "reference": "Create function that prepares verb for the verb function\n\n    The functions created add expressions to be evaluated to\n    the verb, then call the core verb function\n\n    Parameters\n    ----------\n    verb_func : function\n        Core verb function. This is the function called after\n        expressions created and added to the verb. The core\n        function should be one of those that implement verbs that\n        evaluate expressions.\n    add_groups : bool\n        If True, a groups attribute is added to the verb. The\n        groups are the columns created after evaluating the\n        expressions.\n\n    Returns\n    -------\n    out : function\n        A function that implements a helper verb.",
    "generated": "Create function that prepares verb for the verb function\n\n    The functions created add expressions to be evaluated to\n    the verb, then call the core verb function\n\n    Parameters\n    ----------\n    verb_func : function\n        Core verb function. This is the function called after\n        expressions created and added to the verb. The core\n        function should be one of those that implement verbs that\n        evaluate expressions.\n    add_groups : bool\n        If True, a groups attribute is added to the verb. The\n        groups are the columns created after evaluating the\n        expressions."
  },
  {
    "code": "def _get_base_dataframe(df):\n    \"\"\"\n    Remove all columns other than those grouped on\n    \"\"\"\n    if isinstance(df, GroupedDataFrame):\n        base_df = GroupedDataFrame(\n            df.loc[:, df.plydata_groups], df.plydata_groups,\n            copy=True)\n    else:\n        base_df = pd.DataFrame(index=df.index)\n    return base_df",
    "reference": "Remove all columns other than those grouped on",
    "generated": "Remove all columns other than those grouped on"
  },
  {
    "code": "def _add_group_columns(data, gdf):\n    \"\"\"\n    Add group columns to data with a value from the grouped dataframe\n\n    It is assumed that the grouped dataframe contains a single group\n\n    >>> data = pd.DataFrame({\n    ...     'x': [5, 6, 7]})\n    >>> gdf = GroupedDataFrame({\n    ...     'g': list('aaa'),\n    ...     'x': range(3)}, groups=['g'])\n    >>> _add_group_columns(data, gdf)\n       g  x\n    0  a  5\n    1  a  6\n    2  a  7\n    \"\"\"\n    n = len(data)\n    if isinstance(gdf, GroupedDataFrame):\n        for i, col in enumerate(gdf.plydata_groups):\n            if col not in data:\n                group_values = [gdf[col].iloc[0]] * n\n                # Need to be careful and maintain the dtypes\n                # of the group columns\n                if pdtypes.is_categorical_dtype(gdf[col]):\n                    col_values = pd.Categorical(\n                        group_values,\n                        categories=gdf[col].cat.categories,\n                        ordered=gdf[col].cat.ordered\n                    )\n                else:\n                    col_values = pd.Series(\n                        group_values,\n                        index=data.index,\n                        dtype=gdf[col].dtype\n                    )\n                # Group columns come first\n                data.insert(i, col, col_values)\n    return data",
    "reference": "Add group columns to data with a value from the grouped dataframe\n\n    It is assumed that the grouped dataframe contains a single group\n\n    >>> data = pd.DataFrame({\n    ...     'x': [5, 6, 7]})\n    >>> gdf = GroupedDataFrame({\n    ...     'g': list('aaa'),\n    ...     'x': range(3)}, groups=['g'])\n    >>> _add_group_columns(data, gdf)\n       g  x\n    0  a  5\n    1  a  6\n    2  a  7",
    "generated": "Add group columns to data with a value from the grouped dataframe\n\n    It is assumed that the grouped dataframe contains a single group\n\n   "
  },
  {
    "code": "def _create_column(data, col, value):\n    \"\"\"\n    Create column in dataframe\n\n    Helper method meant to deal with problematic\n    column values. e.g When the series index does\n    not match that of the data.\n\n    Parameters\n    ----------\n    data : pandas.DataFrame\n        dataframe in which to insert value\n    col : column label\n        Column name\n    value : object\n        Value to assign to column\n\n    Returns\n    -------\n    data : pandas.DataFrame\n        Modified original dataframe\n\n    >>> df = pd.DataFrame({'x': [1, 2, 3]})\n    >>> y = pd.Series([11, 12, 13], index=[21, 22, 23])\n\n    Data index and value index do not match\n\n    >>> _create_column(df, 'y', y)\n       x   y\n    0  1  11\n    1  2  12\n    2  3  13\n\n    Non-empty dataframe, scalar value\n\n    >>> _create_column(df, 'z', 3)\n       x   y  z\n    0  1  11  3\n    1  2  12  3\n    2  3  13  3\n\n    Empty dataframe, scalar value\n\n    >>> df = pd.DataFrame()\n    >>> _create_column(df, 'w', 3)\n       w\n    0  3\n    >>> _create_column(df, 'z', 'abc')\n       w    z\n    0  3  abc\n    \"\"\"\n    with suppress(AttributeError):\n        # If the index of a series and the dataframe\n        # in which the series will be assigned to a\n        # column do not match, missing values/NaNs\n        # are created. We do not want that.\n        if not value.index.equals(data.index):\n            if len(value) == len(data):\n                value.index = data.index\n            else:\n                value.reset_index(drop=True, inplace=True)\n\n    # You cannot assign a scalar value to a dataframe\n    # without an index. You need an interable value.\n    if data.index.empty:\n        try:\n            len(value)\n        except TypeError:\n            scalar = True\n        else:\n            scalar = isinstance(value, str)\n\n        if scalar:\n            value = [value]\n\n    data[col] = value\n    return data",
    "reference": "Create column in dataframe\n\n    Helper method meant to deal with problematic\n    column values. e.g When the series index does\n    not match that of the data.\n\n    Parameters\n    ----------\n    data : pandas.DataFrame\n        dataframe in which to insert value\n    col : column label\n        Column name\n    value : object\n        Value to assign to column\n\n    Returns\n    -------\n    data : pandas.DataFrame\n        Modified original dataframe\n\n    >>> df = pd.DataFrame({'x': [1, 2, 3]})\n    >>> y = pd.Series([11, 12, 13], index=[21, 22, 23])\n\n    Data index and value index do not match\n\n    >>> _create_column(df, 'y', y)\n       x   y\n    0  1  11\n    1  2  12\n    2  3  13\n\n    Non-empty dataframe, scalar value\n\n    >>> _create_column(df, 'z', 3)\n       x   y  z\n    0  1  11  3\n    1  2  12  3\n    2  3  13  3\n\n    Empty dataframe, scalar value\n\n    >>> df = pd.DataFrame()\n    >>> _create_column(df, 'w', 3)\n       w\n    0  3\n    >>> _create_column(df, 'z', 'abc')\n       w    z\n    0  3  abc",
    "generated": "Create column in dataframe\n\n    Helper method meant to deal with problematic\n    column values. e.g When the series index does\n    not match that of the data.\n\n    Parameters\n    ----------\n    data : pandas.DataFrame\n        dataframe in which to insert value\n    col : column label\n        Column name\n    value : object\n        Value to assign to column\n\n    Returns\n    ----------\n    data : pandas.DataFrame\n        Modified original dataframe\n\n   "
  },
  {
    "code": "def build_expressions(verb):\n    \"\"\"\n    Build expressions for helper verbs\n\n    Parameters\n    ----------\n    verb : verb\n        A verb with a *functions* attribute.\n\n    Returns\n    -------\n    out : tuple\n        (List of Expressions, New columns). The expressions and the\n        new columns in which the results of those expressions will\n        be stored. Even when a result will stored in a column with\n        an existing label, that column is still considered new,\n        i.e An expression ``x='x+1'``, will create a new_column `x`\n        to replace an old column `x`.\n    \"\"\"\n    def partial(func, col, *args, **kwargs):\n        \"\"\"\n        Make a function that acts on a column in a dataframe\n\n        Parameters\n        ----------\n        func : callable\n            Function\n        col : str\n            Column\n        args : tuple\n            Arguments to pass to func\n        kwargs : dict\n            Keyword arguments to func\n\n        Results\n        -------\n        new_func : callable\n            Function that takes a dataframe, and calls the\n            original function on a column in the dataframe.\n        \"\"\"\n        def new_func(gdf):\n            return func(gdf[col], *args, **kwargs)\n\n        return new_func\n\n    def make_statement(func, col):\n        \"\"\"\n        A statement of function called on a column in a dataframe\n\n        Parameters\n        ----------\n        func : str or callable\n            Function to call on a dataframe column\n        col : str\n            Column\n        \"\"\"\n        if isinstance(func, str):\n            expr = '{}({})'.format(func, col)\n        elif callable(func):\n            expr = partial(func, col, *verb.args, **verb.kwargs)\n        else:\n            raise TypeError(\"{} is not a function\".format(func))\n        return expr\n\n    def func_name(func):\n        \"\"\"\n        Return name of a function.\n\n        If the function is `np.sin`, we return `sin`.\n        \"\"\"\n        if isinstance(func, str):\n            return func\n\n        try:\n            return func.__name__\n        except AttributeError:\n            return ''\n\n    # Generate function names. They act as identifiers (postfixed\n    # to the original columns) in the new_column names.\n    if isinstance(verb.functions, (tuple, list)):\n        names = (func_name(func) for func in verb.functions)\n        names_and_functions = zip(names, verb.functions)\n    else:\n        names_and_functions = verb.functions.items()\n\n    # Create statements for the expressions\n    # and postfix identifiers\n    columns = Selector.get(verb)  # columns to act on\n    postfixes = []\n    stmts = []\n    for name, func in names_and_functions:\n        postfixes.append(name)\n        for col in columns:\n            stmts.append(make_statement(func, col))\n\n    if not stmts:\n        stmts = columns\n\n    # Names of the new columns\n    # e.g col1_mean, col2_mean, col1_std, col2_std\n    add_postfix = (isinstance(verb.functions, dict) or\n                   len(verb.functions) > 1)\n    if add_postfix:\n        fmt = '{}_{}'.format\n        new_columns = [fmt(c, p) for p in postfixes for c in columns]\n    else:\n        new_columns = columns\n\n    expressions = [Expression(stmt, col)\n                   for stmt, col in zip(stmts, new_columns)]\n    return expressions, new_columns",
    "reference": "Build expressions for helper verbs\n\n    Parameters\n    ----------\n    verb : verb\n        A verb with a *functions* attribute.\n\n    Returns\n    -------\n    out : tuple\n        (List of Expressions, New columns). The expressions and the\n        new columns in which the results of those expressions will\n        be stored. Even when a result will stored in a column with\n        an existing label, that column is still considered new,\n        i.e An expression ``x='x+1'``, will create a new_column `x`\n        to replace an old column `x`.",
    "generated": "Build expressions for helper verbs\n\n    Parameters\n    ----------\n    verb : verb\n        A verb with a *functions* attribute.\n\n    Returns\n    -------\n    out : tuple\n        (List of Expressions, New columns). The expressions and the\n        new columns in which the results of those expressions will\n        be stored. Even when a result will stored in a column with\n        an existing label, that column is still considered new,\n        i.e An expression ``x='x+1'``, will create a new_column `x`\n        a existing label,"
  },
  {
    "code": "def process(self):\n        \"\"\"\n        Run the expressions\n\n        Returns\n        -------\n        out : pandas.DataFrame\n            Resulting data\n        \"\"\"\n        # Short cut\n        if self._all_expressions_evaluated():\n            if self.drop:\n                # Drop extra columns. They do not correspond to\n                # any expressions.\n                columns = [expr.column for expr in self.expressions]\n                self.data = self.data.loc[:, columns]\n            return self.data\n\n        # group_by\n        # evaluate expressions\n        # combine columns\n        # concat evalutated group data and clean up index and group\n        gdfs = self._get_group_dataframes()\n        egdfs = self._evaluate_expressions(gdfs)\n        edata = self._concat(egdfs)\n        return edata",
    "reference": "Run the expressions\n\n        Returns\n        -------\n        out : pandas.DataFrame\n            Resulting data",
    "generated": "Run the expressions\n        Returns\n        -------\n        out : pandas.DataFrame\n            Resulting data\n       "
  },
  {
    "code": "def _all_expressions_evaluated(self):\n        \"\"\"\n        Return True all expressions match with the columns\n\n        Saves some processor cycles\n        \"\"\"\n        def present(expr):\n            return expr.stmt == expr.column and expr.column in self.data\n        return all(present(expr) for expr in self.expressions)",
    "reference": "Return True all expressions match with the columns\n\n        Saves some processor cycles",
    "generated": "Return True all expressions match with the columns\n\n        Saves some processor cycles\n        \"\"\"\n        def _all_expressions_evaluated(self):\n        Return True all expressions match with the columns\n\n        Saves some processor cycles"
  },
  {
    "code": "def _get_group_dataframes(self):\n        \"\"\"\n        Get group dataframes\n\n        Returns\n        -------\n        out : tuple or generator\n            Group dataframes\n        \"\"\"\n        if isinstance(self.data, GroupedDataFrame):\n            grouper = self.data.groupby()\n            # groupby on categorical columns uses the categories\n            # even if they are not present in the data. This\n            # leads to empty groups. We exclude them.\n            return (gdf for _, gdf in grouper if not gdf.empty)\n        else:\n            return (self.data, )",
    "reference": "Get group dataframes\n\n        Returns\n        -------\n        out : tuple or generator\n            Group dataframes",
    "generated": "Get group dataframes\n        Returns\n        -------\n        out : tuple or generator\n            Group dataframes\n       "
  },
  {
    "code": "def _evaluate_group_dataframe(self, gdf):\n        \"\"\"\n        Evaluate a single group dataframe\n\n        Parameters\n        ----------\n        gdf : pandas.DataFrame\n            Input group dataframe\n\n        Returns\n        -------\n        out : pandas.DataFrame\n            Result data\n        \"\"\"\n        gdf._is_copy = None\n        result_index = gdf.index if self.keep_index else []\n        data = pd.DataFrame(index=result_index)\n        for expr in self.expressions:\n            value = expr.evaluate(gdf, self.env)\n            if isinstance(value, pd.DataFrame):\n                data = value\n                break\n            else:\n                _create_column(data, expr.column, value)\n        data = _add_group_columns(data, gdf)\n        return data",
    "reference": "Evaluate a single group dataframe\n\n        Parameters\n        ----------\n        gdf : pandas.DataFrame\n            Input group dataframe\n\n        Returns\n        -------\n        out : pandas.DataFrame\n            Result data",
    "generated": "Evaluate a single group dataframe\n\n        Parameters\n        ----------\n        gdf : pandas.DataFrame\n            Input group dataframe\n        Returns\n        -------\n        out : pandas.DataFrame\n            Result data"
  },
  {
    "code": "def _concat(self, egdfs):\n        \"\"\"\n        Concatenate evaluated group dataframes\n\n        Parameters\n        ----------\n        egdfs : iterable\n            Evaluated dataframes\n\n        Returns\n        -------\n        edata : pandas.DataFrame\n            Evaluated data\n        \"\"\"\n        egdfs = list(egdfs)\n        edata = pd.concat(egdfs, axis=0, ignore_index=False, copy=False)\n\n        # groupby can mixup the rows. We try to maintain the original\n        # order, but we can only do that if the result has a one to\n        # one relationship with the original\n        one2one = (\n            self.keep_index and\n            not any(edata.index.duplicated()) and\n            len(edata.index) == len(self.data.index))\n        if one2one:\n            edata = edata.sort_index()\n        else:\n            edata.reset_index(drop=True, inplace=True)\n\n        # Maybe this should happen in the verb functions\n        if self.keep_groups and self.groups:\n            edata = GroupedDataFrame(edata, groups=self.groups)\n        return edata",
    "reference": "Concatenate evaluated group dataframes\n\n        Parameters\n        ----------\n        egdfs : iterable\n            Evaluated dataframes\n\n        Returns\n        -------\n        edata : pandas.DataFrame\n            Evaluated data",
    "generated": "Concatenate evaluated group dataframes\n\n        Parameters\n        ----------\n        egdfs : iterable\n            Evaluated dataframes\n        Returns\n        -------\n        edata : pandas.DataFrame\n            Evaluated data\n        Returns\n        -------\n        edata : pandas.DataFrame\n            Evaluated data\n       "
  },
  {
    "code": "def _resolve_slices(data_columns, names):\n        \"\"\"\n        Convert any slices into column names\n\n        Parameters\n        ----------\n        data_columns : pandas.Index\n            Dataframe columns\n        names : tuple\n            Names (including slices) of columns in the\n            dataframe.\n\n        Returns\n        -------\n        out : tuple\n            Names of columns in the dataframe. Has no\n            slices.\n        \"\"\"\n        def _get_slice_cols(sc):\n            \"\"\"\n            Convert slice to list of names\n            \"\"\"\n            # Just like pandas.DataFrame.loc the stop\n            # column is included\n            idx_start = data_columns.get_loc(sc.start)\n            idx_stop = data_columns.get_loc(sc.stop) + 1\n            return data_columns[idx_start:idx_stop:sc.step]\n\n        result = []\n        for col in names:\n            if isinstance(col, slice):\n                result.extend(_get_slice_cols(col))\n            else:\n                result.append(col)\n        return tuple(result)",
    "reference": "Convert any slices into column names\n\n        Parameters\n        ----------\n        data_columns : pandas.Index\n            Dataframe columns\n        names : tuple\n            Names (including slices) of columns in the\n            dataframe.\n\n        Returns\n        -------\n        out : tuple\n            Names of columns in the dataframe. Has no\n            slices.",
    "generated": "Convert any slices into column names\n\n        Parameters\n        ----------\n        data_columns : pandas.Index\n            Dataframe columns\n        names : tuple\n            Names (including slices) of columns in the\n            dataframe."
  },
  {
    "code": "def select(cls, verb):\n        \"\"\"\n        Return selected columns for the select verb\n\n        Parameters\n        ----------\n        verb : object\n            verb with the column selection attributes:\n\n                - names\n                - startswith\n                - endswith\n                - contains\n                - matches\n        \"\"\"\n        columns = verb.data.columns\n        contains = verb.contains\n        matches = verb.matches\n\n        groups = _get_groups(verb)\n        names = cls._resolve_slices(columns, verb.names)\n        names_set = set(names)\n        groups_set = set(groups)\n        lst = [[]]\n\n        if names or groups:\n            # group variable missing from the selection are prepended\n            missing = [g for g in groups if g not in names_set]\n            missing_set = set(missing)\n            c1 = missing + [x for x in names if x not in missing_set]\n            lst.append(c1)\n\n        if verb.startswith:\n            c2 = [x for x in columns\n                  if isinstance(x, str) and x.startswith(verb.startswith)]\n            lst.append(c2)\n\n        if verb.endswith:\n            c3 = [x for x in columns if\n                  isinstance(x, str) and x.endswith(verb.endswith)]\n            lst.append(c3)\n\n        if contains:\n            c4 = []\n            for col in columns:\n                if (isinstance(col, str) and\n                        any(s in col for s in contains)):\n                    c4.append(col)\n            lst.append(c4)\n\n        if matches:\n            c5 = []\n            patterns = [x if hasattr(x, 'match') else re.compile(x)\n                        for x in matches]\n            for col in columns:\n                if isinstance(col, str):\n                    if any(bool(p.match(col)) for p in patterns):\n                        c5.append(col)\n\n            lst.append(c5)\n\n        selected = unique(list(itertools.chain(*lst)))\n\n        if verb.drop:\n            to_drop = [col for col in selected if col not in groups_set]\n            selected = [col for col in columns if col not in to_drop]\n\n        return selected",
    "reference": "Return selected columns for the select verb\n\n        Parameters\n        ----------\n        verb : object\n            verb with the column selection attributes:\n\n                - names\n                - startswith\n                - endswith\n                - contains\n                - matches",
    "generated": "Return selected columns for the select verb\n\n        Parameters\n        ----------\n        verb : object\n            verb with the column selection attributes:\n\n                - names\n                - startswith\n                - endswith\n                - contains\n                - matches\n       "
  },
  {
    "code": "def _at(cls, verb):\n        \"\"\"\n        A verb with a select text match\n        \"\"\"\n        # Named (listed) columns are always included\n        columns = cls.select(verb)\n        final_columns_set = set(cls.select(verb))\n        groups_set = set(_get_groups(verb))\n        final_columns_set -= groups_set - set(verb.names)\n\n        def pred(col):\n            if col not in verb.data:\n                raise KeyError(\n                    \"Unknown column name, {!r}\".format(col))\n            return col in final_columns_set\n\n        return [col for col in columns if pred(col)]",
    "reference": "A verb with a select text match",
    "generated": "A verb with a select text match\n       "
  },
  {
    "code": "def _if(cls, verb):\n        \"\"\"\n        A verb with a predicate function\n        \"\"\"\n        pred = verb.predicate\n        data = verb.data\n        groups = set(_get_groups(verb))\n\n        # force predicate\n        if isinstance(pred, str):\n            if not pred.endswith('_dtype'):\n                pred = '{}_dtype'.format(pred)\n            pred = getattr(pdtypes, pred)\n        elif pdtypes.is_bool_dtype(np.array(pred)):\n            # Turn boolean array into a predicate function\n            it = iter(pred)\n\n            def pred(col):\n                return next(it)\n\n        return [col for col in data\n                if pred(data[col]) and col not in groups]",
    "reference": "A verb with a predicate function",
    "generated": "A verb with a predicate function\n        \"\"\"\n        A verb with a predicate function\n        \"\"\""
  },
  {
    "code": "def get_verb_function(data, verb):\n    \"\"\"\n    Return function that implements the verb for given data type\n    \"\"\"\n    try:\n        module = type_lookup[type(data)]\n    except KeyError:\n        # Some guess work for subclasses\n        for type_, mod in type_lookup.items():\n            if isinstance(data, type_):\n                module = mod\n                break\n    try:\n        return getattr(module, verb)\n    except (NameError, AttributeError):\n        msg = \"Data source of type '{}' is not supported.\"\n        raise TypeError(msg.format(type(data)))",
    "reference": "Return function that implements the verb for given data type",
    "generated": "Return function that implements the verb for given data type\n   "
  },
  {
    "code": "def Expression(*args, **kwargs):\n    \"\"\"\n    Return an appropriate Expression given the arguments\n\n    Parameters\n    ----------\n    args : tuple\n        Positional arguments passed to the Expression class\n    kwargs : dict\n        Keyword arguments passed to the Expression class\n    \"\"\"\n    # dispatch\n    if not hasattr(args[0], '_Expression'):\n        return BaseExpression(*args, *kwargs)\n    else:\n        return args[0]._Expression(*args, **kwargs)",
    "reference": "Return an appropriate Expression given the arguments\n\n    Parameters\n    ----------\n    args : tuple\n        Positional arguments passed to the Expression class\n    kwargs : dict\n        Keyword arguments passed to the Expression class",
    "generated": "Return an appropriate Expression given the arguments\n\n    Parameters\n    ----------\n    args : tuple\n        Positional arguments passed to the Expression class\n    kwargs : dict\n        Keyword arguments passed to the Expression class\n   "
  },
  {
    "code": "def with_outer_namespace(self, outer_namespace):\n        \"\"\"Return a new EvalEnvironment with an extra namespace added.\n        This namespace will be used only for variables that are not found in\n        any existing namespace, i.e., it is \"outside\" them all.\"\"\"\n        return self.__class__(self._namespaces + [outer_namespace],\n                              self.flags)",
    "reference": "Return a new EvalEnvironment with an extra namespace added.\n        This namespace will be used only for variables that are not found in\n        any existing namespace, i.e., it is \"outside\" them all.",
    "generated": "Return a new EvalEnvironment with an extra namespace added.\n        This namespace will be used only for variables that are not found in\n        any existing namespace, i.e., it is \"outside\" them all, i.e., it is \"outside\" them all, i.e., it is \"outside\" them all"
  },
  {
    "code": "def subset(self, names):\n        \"\"\"Creates a new, flat EvalEnvironment that contains only\n        the variables specified.\"\"\"\n        vld = VarLookupDict(self._namespaces)\n        new_ns = dict((name, vld[name]) for name in names)\n        return EvalEnvironment([new_ns], self.flags)",
    "reference": "Creates a new, flat EvalEnvironment that contains only\n        the variables specified.",
    "generated": "Creates a new, flat EvalEnvironment that contains only\n        the variables specified."
  },
  {
    "code": "def Q(name):\n    \"\"\"\n    Quote a variable name\n\n    A way to 'quote' variable names, especially ones that do not otherwise\n    meet Python's variable name rules.\n\n    Parameters\n    ----------\n    name : str\n        Name of variable\n\n    Returns\n    -------\n    value : object\n        Value of variable\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> from plydata import define\n    >>> df = pd.DataFrame({'class': [10, 20, 30]})\n\n    Since ``class`` is a reserved python keyword it cannot be a variable\n    name, and therefore cannot be used in an expression without quoting it.\n\n    >>> df >> define(y='class+1')\n    Traceback (most recent call last):\n      File \"<string>\", line 1\n        class+1\n            ^\n    SyntaxError: invalid syntax\n\n    >>> df >> define(y='Q(\"class\")+1')\n       class   y\n    0     10  11\n    1     20  21\n    2     30  31\n\n    Note that it is ``'Q(\"some name\")'`` and not ``'Q(some name)'``.\n    As in the above example, you do not need to ``import`` ``Q`` before\n    you can use it.\n    \"\"\"\n    env = EvalEnvironment.capture(1)\n    try:\n        return env.namespace[name]\n    except KeyError:\n        raise NameError(\"No data named {!r} found\".format(name))",
    "reference": "Quote a variable name\n\n    A way to 'quote' variable names, especially ones that do not otherwise\n    meet Python's variable name rules.\n\n    Parameters\n    ----------\n    name : str\n        Name of variable\n\n    Returns\n    -------\n    value : object\n        Value of variable\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> from plydata import define\n    >>> df = pd.DataFrame({'class': [10, 20, 30]})\n\n    Since ``class`` is a reserved python keyword it cannot be a variable\n    name, and therefore cannot be used in an expression without quoting it.\n\n    >>> df >> define(y='class+1')\n    Traceback (most recent call last):\n      File \"<string>\", line 1\n        class+1\n            ^\n    SyntaxError: invalid syntax\n\n    >>> df >> define(y='Q(\"class\")+1')\n       class   y\n    0     10  11\n    1     20  21\n    2     30  31\n\n    Note that it is ``'Q(\"some name\")'`` and not ``'Q(some name)'``.\n    As in the above example, you do not need to ``import`` ``Q`` before\n    you can use it.",
    "generated": "Quote a variable name\n\n    A way to 'quote' variable names, especially ones that do not otherwise\n    meet Python's variable name rules.\n\n    Parameters\n    ----------\n    name : str\n        Name of variable\n    Returns\n    -------\n    value : object\n        Value of variable\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> df = pd.DataFrame({'class': [10, 20, 30])\n\n    Since ``class`` is a reserved python keyword it cannot be a variable\n    name, and therefore cannot be used in an expression without"
  },
  {
    "code": "def regular_index(*dfs):\n    \"\"\"\n    Change & restore the indices of dataframes\n\n    Dataframe with duplicate values can be hard to work with.\n    When split and recombined, you cannot restore the row order.\n    This can be the case even if the index has unique but\n    irregular/unordered. This contextmanager resets the unordered\n    indices of any dataframe passed to it, on exit it restores\n    the original index.\n\n    A regular index is of the form::\n\n        RangeIndex(start=0, stop=n, step=1)\n\n    Parameters\n    ----------\n    dfs : tuple\n        Dataframes\n\n    Yields\n    ------\n    dfs : tuple\n        Dataframe\n\n    Examples\n    --------\n    Create dataframes with different indices\n\n    >>> df1 = pd.DataFrame([4, 3, 2, 1])\n    >>> df2 = pd.DataFrame([3, 2, 1], index=[3, 0, 0])\n    >>> df3 = pd.DataFrame([11, 12, 13], index=[11, 12, 13])\n\n    Within the contexmanager all frames have nice range indices\n\n    >>> with regular_index(df1, df2, df3):\n    ...     print(df1.index)\n    ...     print(df2.index)\n    ...     print(df3.index)\n    RangeIndex(start=0, stop=4, step=1)\n    RangeIndex(start=0, stop=3, step=1)\n    RangeIndex(start=0, stop=3, step=1)\n\n    Indices restored\n\n    >>> df1.index\n    RangeIndex(start=0, stop=4, step=1)\n    >>> df2.index\n    Int64Index([3, 0, 0], dtype='int64')\n    >>> df3.index\n    Int64Index([11, 12, 13], dtype='int64')\n    \"\"\"\n    original_index = [df.index for df in dfs]\n    have_bad_index = [not isinstance(df.index, pd.RangeIndex)\n                      for df in dfs]\n\n    for df, bad in zip(dfs, have_bad_index):\n        if bad:\n            df.reset_index(drop=True, inplace=True)\n\n    try:\n        yield dfs\n    finally:\n        for df, bad, idx in zip(dfs, have_bad_index, original_index):\n            if bad and len(df.index) == len(idx):\n                df.index = idx",
    "reference": "Change & restore the indices of dataframes\n\n    Dataframe with duplicate values can be hard to work with.\n    When split and recombined, you cannot restore the row order.\n    This can be the case even if the index has unique but\n    irregular/unordered. This contextmanager resets the unordered\n    indices of any dataframe passed to it, on exit it restores\n    the original index.\n\n    A regular index is of the form::\n\n        RangeIndex(start=0, stop=n, step=1)\n\n    Parameters\n    ----------\n    dfs : tuple\n        Dataframes\n\n    Yields\n    ------\n    dfs : tuple\n        Dataframe\n\n    Examples\n    --------\n    Create dataframes with different indices\n\n    >>> df1 = pd.DataFrame([4, 3, 2, 1])\n    >>> df2 = pd.DataFrame([3, 2, 1], index=[3, 0, 0])\n    >>> df3 = pd.DataFrame([11, 12, 13], index=[11, 12, 13])\n\n    Within the contexmanager all frames have nice range indices\n\n    >>> with regular_index(df1, df2, df3):\n    ...     print(df1.index)\n    ...     print(df2.index)\n    ...     print(df3.index)\n    RangeIndex(start=0, stop=4, step=1)\n    RangeIndex(start=0, stop=3, step=1)\n    RangeIndex(start=0, stop=3, step=1)\n\n    Indices restored\n\n    >>> df1.index\n    RangeIndex(start=0, stop=4, step=1)\n    >>> df2.index\n    Int64Index([3, 0, 0], dtype='int64')\n    >>> df3.index\n    Int64Index([11, 12, 13], dtype='int64')",
    "generated": "Change & restore the indices of dataframes\n\n    Dataframe with duplicate values can be hard to work with.\n    When split and recombined, you cannot restore the row order.\n    This can be the case even if the index has unique but\n    irregular/unordered. This contextmanager resets the unordered\n    indices of any dataframe passed to it, on exit it restores\n    the original index.\n\n    A regular index is of the form::\n\n        RangeIndex(start, stop, step=1)"
  },
  {
    "code": "def unique(lst):\n    \"\"\"\n    Return unique elements\n\n    :class:`pandas.unique` and :class:`numpy.unique` cast\n    mixed type lists to the same type. They are faster, but\n    some times we want to maintain the type.\n\n    Parameters\n    ----------\n    lst : list-like\n        List of items\n\n    Returns\n    -------\n    out : list\n        Unique items in the order that they appear in the\n        input.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> import numpy as np\n    >>> lst = ['one', 'two', 123, 'three']\n    >>> pd.unique(lst)\n    array(['one', 'two', '123', 'three'], dtype=object)\n    >>> np.unique(lst)\n    array(['123', 'one', 'three', 'two'],\n          dtype='<U5')\n    >>> unique(lst)\n    ['one', 'two', 123, 'three']\n\n    pandas and numpy cast 123 to a string!, and numpy does not\n    even maintain the order.\n    \"\"\"\n    seen = set()\n\n    def make_seen(x):\n        seen.add(x)\n        return x\n\n    return [make_seen(x) for x in lst if x not in seen]",
    "reference": "Return unique elements\n\n    :class:`pandas.unique` and :class:`numpy.unique` cast\n    mixed type lists to the same type. They are faster, but\n    some times we want to maintain the type.\n\n    Parameters\n    ----------\n    lst : list-like\n        List of items\n\n    Returns\n    -------\n    out : list\n        Unique items in the order that they appear in the\n        input.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> import numpy as np\n    >>> lst = ['one', 'two', 123, 'three']\n    >>> pd.unique(lst)\n    array(['one', 'two', '123', 'three'], dtype=object)\n    >>> np.unique(lst)\n    array(['123', 'one', 'three', 'two'],\n          dtype='<U5')\n    >>> unique(lst)\n    ['one', 'two', 123, 'three']\n\n    pandas and numpy cast 123 to a string!, and numpy does not\n    even maintain the order.",
    "generated": "Return unique elements\n\n    :class:`pandas.unique` and :class:`numpy.unique` cast\n    mixed type lists to the same type. They are faster, but\n    some times we want to maintain the type."
  },
  {
    "code": "def _nth(arr, n):\n    \"\"\"\n    Return the nth value of array\n\n    If it is missing return NaN\n    \"\"\"\n    try:\n        return arr.iloc[n]\n    except (KeyError, IndexError):\n        return np.nan",
    "reference": "Return the nth value of array\n\n    If it is missing return NaN",
    "generated": "Return the nth value of array\n\n    If it is missing return NaN"
  },
  {
    "code": "def make_time(h=0, m=0, s=0, ms=0, frames=None, fps=None):\n    \"\"\"\n    Convert time to milliseconds.\n\n    See :func:`pysubs2.time.times_to_ms()`. When both frames and fps are specified,\n    :func:`pysubs2.time.frames_to_ms()` is called instead.\n\n    Raises:\n        ValueError: Invalid fps, or one of frames/fps is missing.\n\n    Example:\n        >>> make_time(s=1.5)\n        1500\n        >>> make_time(frames=50, fps=25)\n        2000\n\n    \"\"\"\n    if frames is None and fps is None:\n        return times_to_ms(h, m, s, ms)\n    elif frames is not None and fps is not None:\n        return frames_to_ms(frames, fps)\n    else:\n        raise ValueError(\"Both fps and frames must be specified\")",
    "reference": "Convert time to milliseconds.\n\n    See :func:`pysubs2.time.times_to_ms()`. When both frames and fps are specified,\n    :func:`pysubs2.time.frames_to_ms()` is called instead.\n\n    Raises:\n        ValueError: Invalid fps, or one of frames/fps is missing.\n\n    Example:\n        >>> make_time(s=1.5)\n        1500\n        >>> make_time(frames=50, fps=25)\n        2000",
    "generated": "Convert time to milliseconds.\n\n    See :func:`pysubs2.time.times_to_ms()`. When both frames and fps are specified,\n    :func:`pysubs2.time.frames_to_ms()` is called instead."
  },
  {
    "code": "def shift(self, h=0, m=0, s=0, ms=0, frames=None, fps=None):\n        \"\"\"\n        Shift start and end times.\n\n        See :meth:`SSAFile.shift()` for full description.\n\n        \"\"\"\n        delta = make_time(h=h, m=m, s=s, ms=ms, frames=frames, fps=fps)\n        self.start += delta\n        self.end += delta",
    "reference": "Shift start and end times.\n\n        See :meth:`SSAFile.shift()` for full description.",
    "generated": "Shift start and end times.\n\n        See :meth:`SSAFile.shift()` for full description."
  },
  {
    "code": "def equals(self, other):\n        \"\"\"Field-based equality for SSAEvents.\"\"\"\n        if isinstance(other, SSAEvent):\n            return self.as_dict() == other.as_dict()\n        else:\n            raise TypeError(\"Cannot compare to non-SSAEvent object\")",
    "reference": "Field-based equality for SSAEvents.",
    "generated": "Field-based equality for SSAEvents."
  },
  {
    "code": "def load(cls, path, encoding=\"utf-8\", format_=None, fps=None, **kwargs):\n        \"\"\"\n        Load subtitle file from given path.\n\n        Arguments:\n            path (str): Path to subtitle file.\n            encoding (str): Character encoding of input file.\n                Defaults to UTF-8, you may need to change this.\n            format_ (str): Optional, forces use of specific parser\n                (eg. `\"srt\"`, `\"ass\"`). Otherwise, format is detected\n                automatically from file contents. This argument should\n                be rarely needed.\n            fps (float): Framerate for frame-based formats (MicroDVD),\n                for other formats this argument is ignored. Framerate might\n                be detected from the file, in which case you don't need\n                to specify it here (when given, this argument overrides\n                autodetection).\n            kwargs: Extra options for the parser.\n\n        Returns:\n            SSAFile\n\n        Raises:\n            IOError\n            UnicodeDecodeError\n            pysubs2.exceptions.UnknownFPSError\n            pysubs2.exceptions.UnknownFormatIdentifierError\n            pysubs2.exceptions.FormatAutodetectionError\n\n        Note:\n            pysubs2 may autodetect subtitle format and/or framerate. These\n            values are set as :attr:`SSAFile.format` and :attr:`SSAFile.fps`\n            attributes.\n\n        Example:\n            >>> subs1 = pysubs2.load(\"subrip-subtitles.srt\")\n            >>> subs2 = pysubs2.load(\"microdvd-subtitles.sub\", fps=23.976)\n\n        \"\"\"\n        with open(path, encoding=encoding) as fp:\n            return cls.from_file(fp, format_, fps=fps, **kwargs)",
    "reference": "Load subtitle file from given path.\n\n        Arguments:\n            path (str): Path to subtitle file.\n            encoding (str): Character encoding of input file.\n                Defaults to UTF-8, you may need to change this.\n            format_ (str): Optional, forces use of specific parser\n                (eg. `\"srt\"`, `\"ass\"`). Otherwise, format is detected\n                automatically from file contents. This argument should\n                be rarely needed.\n            fps (float): Framerate for frame-based formats (MicroDVD),\n                for other formats this argument is ignored. Framerate might\n                be detected from the file, in which case you don't need\n                to specify it here (when given, this argument overrides\n                autodetection).\n            kwargs: Extra options for the parser.\n\n        Returns:\n            SSAFile\n\n        Raises:\n            IOError\n            UnicodeDecodeError\n            pysubs2.exceptions.UnknownFPSError\n            pysubs2.exceptions.UnknownFormatIdentifierError\n            pysubs2.exceptions.FormatAutodetectionError\n\n        Note:\n            pysubs2 may autodetect subtitle format and/or framerate. These\n            values are set as :attr:`SSAFile.format` and :attr:`SSAFile.fps`\n            attributes.\n\n        Example:\n            >>> subs1 = pysubs2.load(\"subrip-subtitles.srt\")\n            >>> subs2 = pysubs2.load(\"microdvd-subtitles.sub\", fps=23.976)",
    "generated": "Load subtitle file from given path.\n\n        Arguments:\n            path (str): Path to subtitle file.\n            encoding (str): Character encoding of input file.\n                Defaults to UTF-8, you may need to change this.\n            format_ (str): Optional, forces use of specific parser\n                (eg. `\"srt\"`, `\"ass\"`). Otherwise, format is detected\n                automatically from file contents. This argument should\n                be rarely needed.\n            fps (float): Framerate for frame-based formats (MicroDVD),\n                for other formats this argument"
  },
  {
    "code": "def from_string(cls, string, format_=None, fps=None, **kwargs):\n        \"\"\"\n        Load subtitle file from string.\n\n        See :meth:`SSAFile.load()` for full description.\n\n        Arguments:\n            string (str): Subtitle file in a string. Note that the string\n                must be Unicode (in Python 2).\n\n        Returns:\n            SSAFile\n\n        Example:\n            >>> text = '''\n            ... 1\n            ... 00:00:00,000 --> 00:00:05,000\n            ... An example SubRip file.\n            ... '''\n            >>> subs = SSAFile.from_string(text)\n\n        \"\"\"\n        fp = io.StringIO(string)\n        return cls.from_file(fp, format_, fps=fps, **kwargs)",
    "reference": "Load subtitle file from string.\n\n        See :meth:`SSAFile.load()` for full description.\n\n        Arguments:\n            string (str): Subtitle file in a string. Note that the string\n                must be Unicode (in Python 2).\n\n        Returns:\n            SSAFile\n\n        Example:\n            >>> text = '''\n            ... 1\n            ... 00:00:00,000 --> 00:00:05,000\n            ... An example SubRip file.\n            ... '''\n            >>> subs = SSAFile.from_string(text)",
    "generated": "Load subtitle file from string.\n\n        See :meth:`SSAFile.load()` for full description."
  },
  {
    "code": "def from_file(cls, fp, format_=None, fps=None, **kwargs):\n        \"\"\"\n        Read subtitle file from file object.\n\n        See :meth:`SSAFile.load()` for full description.\n\n        Note:\n            This is a low-level method. Usually, one of :meth:`SSAFile.load()`\n            or :meth:`SSAFile.from_string()` is preferable.\n\n        Arguments:\n            fp (file object): A file object, ie. :class:`io.TextIOBase` instance.\n                Note that the file must be opened in text mode (as opposed to binary).\n\n        Returns:\n            SSAFile\n\n        \"\"\"\n        if format_ is None:\n            # Autodetect subtitle format, then read again using correct parser.\n            # The file might be a pipe and we need to read it twice,\n            # so just buffer everything.\n            text = fp.read()\n            fragment = text[:10000]\n            format_ = autodetect_format(fragment)\n            fp = io.StringIO(text)\n\n        impl = get_format_class(format_)\n        subs = cls() # an empty subtitle file\n        subs.format = format_\n        subs.fps = fps\n        impl.from_file(subs, fp, format_, fps=fps, **kwargs)\n        return subs",
    "reference": "Read subtitle file from file object.\n\n        See :meth:`SSAFile.load()` for full description.\n\n        Note:\n            This is a low-level method. Usually, one of :meth:`SSAFile.load()`\n            or :meth:`SSAFile.from_string()` is preferable.\n\n        Arguments:\n            fp (file object): A file object, ie. :class:`io.TextIOBase` instance.\n                Note that the file must be opened in text mode (as opposed to binary).\n\n        Returns:\n            SSAFile",
    "generated": "Read subtitle file from file object.\n\n        See :meth:`SSAFile.load()` for full description.\n\n        Note:\n            This is a low-level method. Usually, one of :meth:`SSAFile.load()`\n            or :meth:`SSAFile.from_string()` is preferable.\n\n        Arguments:\n            fp (file object): A file object, ie. :class:`io.TextIOBase` instance.\n                Note that the file must be opened in text mode (as opposed to binary)."
  },
  {
    "code": "def save(self, path, encoding=\"utf-8\", format_=None, fps=None, **kwargs):\n        \"\"\"\n        Save subtitle file to given path.\n\n        Arguments:\n            path (str): Path to subtitle file.\n            encoding (str): Character encoding of output file.\n                Defaults to UTF-8, which should be fine for most purposes.\n            format_ (str): Optional, specifies desired subtitle format\n                (eg. `\"srt\"`, `\"ass\"`). Otherwise, format is detected\n                automatically from file extension. Thus, this argument\n                is rarely needed.\n            fps (float): Framerate for frame-based formats (MicroDVD),\n                for other formats this argument is ignored. When omitted,\n                :attr:`SSAFile.fps` value is used (ie. the framerate used\n                for loading the file, if any). When the :class:`SSAFile`\n                wasn't loaded from MicroDVD, or if you wish save it with\n                different framerate, use this argument. See also\n                :meth:`SSAFile.transform_framerate()` for fixing bad\n                frame-based to time-based conversions.\n            kwargs: Extra options for the writer.\n\n        Raises:\n            IOError\n            UnicodeEncodeError\n            pysubs2.exceptions.UnknownFPSError\n            pysubs2.exceptions.UnknownFormatIdentifierError\n            pysubs2.exceptions.UnknownFileExtensionError\n\n        \"\"\"\n        if format_ is None:\n            ext = os.path.splitext(path)[1].lower()\n            format_ = get_format_identifier(ext)\n\n        with open(path, \"w\", encoding=encoding) as fp:\n            self.to_file(fp, format_, fps=fps, **kwargs)",
    "reference": "Save subtitle file to given path.\n\n        Arguments:\n            path (str): Path to subtitle file.\n            encoding (str): Character encoding of output file.\n                Defaults to UTF-8, which should be fine for most purposes.\n            format_ (str): Optional, specifies desired subtitle format\n                (eg. `\"srt\"`, `\"ass\"`). Otherwise, format is detected\n                automatically from file extension. Thus, this argument\n                is rarely needed.\n            fps (float): Framerate for frame-based formats (MicroDVD),\n                for other formats this argument is ignored. When omitted,\n                :attr:`SSAFile.fps` value is used (ie. the framerate used\n                for loading the file, if any). When the :class:`SSAFile`\n                wasn't loaded from MicroDVD, or if you wish save it with\n                different framerate, use this argument. See also\n                :meth:`SSAFile.transform_framerate()` for fixing bad\n                frame-based to time-based conversions.\n            kwargs: Extra options for the writer.\n\n        Raises:\n            IOError\n            UnicodeEncodeError\n            pysubs2.exceptions.UnknownFPSError\n            pysubs2.exceptions.UnknownFormatIdentifierError\n            pysubs2.exceptions.UnknownFileExtensionError",
    "generated": "Save subtitle file to given path.\n\n        Arguments:\n            path (str): Path to subtitle file.\n            encoding (str): Character encoding of output file.\n                Defaults to UTF-8, which should be fine for most purposes.\n            format_ (str): Optional, specifies desired subtitle format\n                (eg. `\"srt\"`, `\"ass\"`). Otherwise, format is detected\n                automatically from file extension. Thus, this argument\n                is rarely needed.\n            fps (float): Framerate for frame-based formats (MicroDVD),\n                for other formats this"
  },
  {
    "code": "def to_string(self, format_, fps=None, **kwargs):\n        \"\"\"\n        Get subtitle file as a string.\n\n        See :meth:`SSAFile.save()` for full description.\n\n        Returns:\n            str\n\n        \"\"\"\n        fp = io.StringIO()\n        self.to_file(fp, format_, fps=fps, **kwargs)\n        return fp.getvalue()",
    "reference": "Get subtitle file as a string.\n\n        See :meth:`SSAFile.save()` for full description.\n\n        Returns:\n            str",
    "generated": "Get subtitle file as a string.\n\n        See :meth:`SSAFile.save()` for full description."
  },
  {
    "code": "def to_file(self, fp, format_, fps=None, **kwargs):\n        \"\"\"\n        Write subtitle file to file object.\n\n        See :meth:`SSAFile.save()` for full description.\n\n        Note:\n            This is a low-level method. Usually, one of :meth:`SSAFile.save()`\n            or :meth:`SSAFile.to_string()` is preferable.\n\n        Arguments:\n            fp (file object): A file object, ie. :class:`io.TextIOBase` instance.\n                Note that the file must be opened in text mode (as opposed to binary).\n\n        \"\"\"\n        impl = get_format_class(format_)\n        impl.to_file(self, fp, format_, fps=fps, **kwargs)",
    "reference": "Write subtitle file to file object.\n\n        See :meth:`SSAFile.save()` for full description.\n\n        Note:\n            This is a low-level method. Usually, one of :meth:`SSAFile.save()`\n            or :meth:`SSAFile.to_string()` is preferable.\n\n        Arguments:\n            fp (file object): A file object, ie. :class:`io.TextIOBase` instance.\n                Note that the file must be opened in text mode (as opposed to binary).",
    "generated": "Write subtitle file to file object.\n\n        See :meth:`SSAFile.save()` for full description.\n\n        Note:\n            This is a low-level method. Usually, one of :meth:`SSAFile.save()`\n            or :meth:`SSAFile.to_string()` is preferable."
  },
  {
    "code": "def rename_style(self, old_name, new_name):\n        \"\"\"\n        Rename a style, including references to it.\n\n        Arguments:\n            old_name (str): Style to be renamed.\n            new_name (str): New name for the style (must be unused).\n\n        Raises:\n            KeyError: No style named old_name.\n            ValueError: new_name is not a legal name (cannot use commas)\n                or new_name is taken.\n\n        \"\"\"\n        if old_name not in self.styles:\n            raise KeyError(\"Style %r not found\" % old_name)\n        if new_name in self.styles:\n            raise ValueError(\"There is already a style called %r\" % new_name)\n        if not is_valid_field_content(new_name):\n            raise ValueError(\"%r is not a valid name\" % new_name)\n\n        self.styles[new_name] = self.styles[old_name]\n        del self.styles[old_name]\n\n        for line in self:\n            # XXX also handle \\r override tag\n            if line.style == old_name:\n                line.style = new_name",
    "reference": "Rename a style, including references to it.\n\n        Arguments:\n            old_name (str): Style to be renamed.\n            new_name (str): New name for the style (must be unused).\n\n        Raises:\n            KeyError: No style named old_name.\n            ValueError: new_name is not a legal name (cannot use commas)\n                or new_name is taken.",
    "generated": "Rename a style, including references to it.\n\n        Arguments:\n            old_name (str): Style to be renamed.\n            new_name (str): New name for the style (must be unused).\n\n        Raises:\n            KeyError: No style named old_name.\n            ValueError: new_name is not a legal name (cannot use commas)\n                or new_name is taken."
  },
  {
    "code": "def import_styles(self, subs, overwrite=True):\n        \"\"\"\n        Merge in styles from other SSAFile.\n\n        Arguments:\n            subs (SSAFile): Subtitle file imported from.\n            overwrite (bool): On name conflict, use style from the other file\n                (default: True).\n\n        \"\"\"\n        if not isinstance(subs, SSAFile):\n            raise TypeError(\"Must supply an SSAFile.\")\n\n        for name, style in subs.styles.items():\n            if name not in self.styles or overwrite:\n                self.styles[name] = style",
    "reference": "Merge in styles from other SSAFile.\n\n        Arguments:\n            subs (SSAFile): Subtitle file imported from.\n            overwrite (bool): On name conflict, use style from the other file\n                (default: True).",
    "generated": "Merge in styles from other SSAFile.\n\n        Arguments:\n            subs (SSAFile): Subtitle file imported from.\n            overwrite (bool): On name conflict, use style from the other file\n                (default: True)."
  },
  {
    "code": "def equals(self, other):\n        \"\"\"\n        Equality of two SSAFiles.\n\n        Compares :attr:`SSAFile.info`, :attr:`SSAFile.styles` and :attr:`SSAFile.events`.\n        Order of entries in OrderedDicts does not matter. \"ScriptType\" key in info is\n        considered an implementation detail and thus ignored.\n\n        Useful mostly in unit tests. Differences are logged at DEBUG level.\n\n        \"\"\"\n\n        if isinstance(other, SSAFile):\n            for key in set(chain(self.info.keys(), other.info.keys())) - {\"ScriptType\"}:\n                sv, ov = self.info.get(key), other.info.get(key)\n                if sv is None:\n                    logging.debug(\"%r missing in self.info\", key)\n                    return False\n                elif ov is None:\n                    logging.debug(\"%r missing in other.info\", key)\n                    return False\n                elif sv != ov:\n                    logging.debug(\"info %r differs (self=%r, other=%r)\", key, sv, ov)\n                    return False\n\n            for key in set(chain(self.styles.keys(), other.styles.keys())):\n                sv, ov = self.styles.get(key), other.styles.get(key)\n                if sv is None:\n                    logging.debug(\"%r missing in self.styles\", key)\n                    return False\n                elif ov is None:\n                    logging.debug(\"%r missing in other.styles\", key)\n                    return False\n                elif sv != ov:\n                    for k in sv.FIELDS:\n                        if getattr(sv, k) != getattr(ov, k): logging.debug(\"difference in field %r\", k)\n                    logging.debug(\"style %r differs (self=%r, other=%r)\", key, sv.as_dict(), ov.as_dict())\n                    return False\n\n            if len(self) != len(other):\n                logging.debug(\"different # of subtitles (self=%d, other=%d)\", len(self), len(other))\n                return False\n\n            for i, (se, oe) in enumerate(zip(self.events, other.events)):\n                if not se.equals(oe):\n                    for k in se.FIELDS:\n                        if getattr(se, k) != getattr(oe, k): logging.debug(\"difference in field %r\", k)\n                    logging.debug(\"event %d differs (self=%r, other=%r)\", i, se.as_dict(), oe.as_dict())\n                    return False\n\n            return True\n        else:\n            raise TypeError(\"Cannot compare to non-SSAFile object\")",
    "reference": "Equality of two SSAFiles.\n\n        Compares :attr:`SSAFile.info`, :attr:`SSAFile.styles` and :attr:`SSAFile.events`.\n        Order of entries in OrderedDicts does not matter. \"ScriptType\" key in info is\n        considered an implementation detail and thus ignored.\n\n        Useful mostly in unit tests. Differences are logged at DEBUG level.",
    "generated": "Compare two SSAFiles.\n\n        Compares :attr:`SSAFile.info`, :attr:`SSAFile.styles` and :attr:`SSAFile.events`.\n        Order of entries in OrderedDicts does not matter. \"ScriptType\" key in info is\n        considered an implementation detail and thus ignored.\n\n        Useful mostly in unit tests. Differences are logged at DEBUG level."
  },
  {
    "code": "def get_file_extension(format_):\n    \"\"\"Format identifier -> file extension\"\"\"\n    if format_ not in FORMAT_IDENTIFIER_TO_FORMAT_CLASS:\n        raise UnknownFormatIdentifierError(format_)\n\n    for ext, f in FILE_EXTENSION_TO_FORMAT_IDENTIFIER.items():\n        if f == format_:\n            return ext\n\n    raise RuntimeError(\"No file extension for format %r\" % format_)",
    "reference": "Format identifier -> file extension",
    "generated": "Format identifier -> file extension"
  },
  {
    "code": "def autodetect_format(content):\n    \"\"\"Return format identifier for given fragment or raise FormatAutodetectionError.\"\"\"\n    formats = set()\n    for impl in FORMAT_IDENTIFIER_TO_FORMAT_CLASS.values():\n        guess = impl.guess_format(content)\n        if guess is not None:\n            formats.add(guess)\n\n    if len(formats) == 1:\n        return formats.pop()\n    elif not formats:\n        raise FormatAutodetectionError(\"No suitable formats\")\n    else:\n        raise FormatAutodetectionError(\"Multiple suitable formats (%r)\" % formats)",
    "reference": "Return format identifier for given fragment or raise FormatAutodetectionError.",
    "generated": "Return format identifier for given fragment or raise FormatAutodetectionError."
  },
  {
    "code": "async def src_reload(app, path: str = None):\n    \"\"\"\n    prompt each connected browser to reload by sending websocket message.\n\n    :param path: if supplied this must be a path relative to app['static_path'],\n        eg. reload of a single file is only supported for static resources.\n    :return: number of sources reloaded\n    \"\"\"\n    cli_count = len(app[WS])\n    if cli_count == 0:\n        return 0\n\n    is_html = None\n    if path:\n        path = str(Path(app['static_url']) / Path(path).relative_to(app['static_path']))\n        is_html = mimetypes.guess_type(path)[0] == 'text/html'\n\n    reloads = 0\n    aux_logger.debug('prompting source reload for %d clients', cli_count)\n    for ws, url in app[WS]:\n        if path and is_html and path not in {url, url + '.html', url.rstrip('/') + '/index.html'}:\n            aux_logger.debug('skipping reload for client at %s', url)\n            continue\n        aux_logger.debug('reload client at %s', url)\n        data = {\n            'command': 'reload',\n            'path': path or url,\n            'liveCSS': True,\n            'liveImg': True,\n        }\n        try:\n            await ws.send_str(json.dumps(data))\n        except RuntimeError as e:\n            # eg. \"RuntimeError: websocket connection is closing\"\n            aux_logger.error('Error broadcasting change to %s, RuntimeError: %s', path or url, e)\n        else:\n            reloads += 1\n\n    if reloads:\n        s = '' if reloads == 1 else 's'\n        aux_logger.info('prompted reload of %s on %d client%s', path or 'page', reloads, s)\n    return reloads",
    "reference": "prompt each connected browser to reload by sending websocket message.\n\n    :param path: if supplied this must be a path relative to app['static_path'],\n        eg. reload of a single file is only supported for static resources.\n    :return: number of sources reloaded",
    "generated": "Prompt each connected browser to reload by sending websocket message.\n\n    :param path: if supplied this must be a path relative to app['static_path'],\n        eg. reload of a single file is only supported for static resources.\n    :return: number of sources reloaded\n   "
  },
  {
    "code": "def substitute_environ(self):\n        \"\"\"\n        Substitute environment variables into settings.\n        \"\"\"\n        for attr_name in dir(self):\n            if attr_name.startswith('_') or attr_name.upper() != attr_name:\n                continue\n\n            orig_value = getattr(self, attr_name)\n            is_required = isinstance(orig_value, Required)\n            orig_type = orig_value.v_type if is_required else type(orig_value)\n            env_var_name = self._ENV_PREFIX + attr_name\n            env_var = os.getenv(env_var_name, None)\n            if env_var is not None:\n                if issubclass(orig_type, bool):\n                    env_var = env_var.upper() in ('1', 'TRUE')\n                elif issubclass(orig_type, int):\n                    env_var = int(env_var)\n                elif issubclass(orig_type, Path):\n                    env_var = Path(env_var)\n                elif issubclass(orig_type, bytes):\n                    env_var = env_var.encode()\n                # could do floats here and lists etc via json\n                setattr(self, attr_name, env_var)\n            elif is_required and attr_name not in self._custom_settings:\n                raise RuntimeError('The required environment variable \"{0}\" is currently not set, '\n                                   'you\\'ll need to run `source activate.settings.sh` '\n                                   'or you can set that single environment variable with '\n                                   '`export {0}=\"<value>\"`'.format(env_var_name))",
    "reference": "Substitute environment variables into settings.",
    "generated": "Substitute environment variables into settings.\n       "
  },
  {
    "code": "def serve(path, livereload, port, verbose):\n    \"\"\"\n    Serve static files from a directory.\n    \"\"\"\n    setup_logging(verbose)\n    run_app(*serve_static(static_path=path, livereload=livereload, port=port))",
    "reference": "Serve static files from a directory.",
    "generated": "Serve static files from a directory."
  },
  {
    "code": "def runserver(**config):\n    \"\"\"\n    Run a development server for an aiohttp apps.\n\n    Takes one argument \"app-path\" which should be a path to either a directory containing a recognized default file\n    (\"app.py\" or \"main.py\") or to a specific file. Defaults to the environment variable \"AIO_APP_PATH\" or \".\".\n\n    The app path is run directly, see the \"--app-factory\" option for details on how an app is loaded from a python\n    module.\n    \"\"\"\n    active_config = {k: v for k, v in config.items() if v is not None}\n    setup_logging(config['verbose'])\n    try:\n        run_app(*_runserver(**active_config))\n    except AiohttpDevException as e:\n        if config['verbose']:\n            tb = click.style(traceback.format_exc().strip('\\n'), fg='white', dim=True)\n            main_logger.warning('AiohttpDevException traceback:\\n%s', tb)\n        main_logger.error('Error: %s', e)\n        sys.exit(2)",
    "reference": "Run a development server for an aiohttp apps.\n\n    Takes one argument \"app-path\" which should be a path to either a directory containing a recognized default file\n    (\"app.py\" or \"main.py\") or to a specific file. Defaults to the environment variable \"AIO_APP_PATH\" or \".\".\n\n    The app path is run directly, see the \"--app-factory\" option for details on how an app is loaded from a python\n    module.",
    "generated": "Run a development server for an aiohttp apps.\n\n    Takes one argument \"app-path\" which should be a path to either a directory containing a recognized default file\n    (\"app.py\" or \"main.py\") or to a specific file. Defaults to the environment variable \"AIO_APP_PATH\" or \".\".\n\n    The app path is run directly, see the \"--app-factory\" option for details on how an app is loaded from a python\n    module."
  },
  {
    "code": "def scenario(weight=1, delay=0.0, name=None):\n    \"\"\"Decorator to register a function as a Molotov test.\n\n    Options:\n\n    - **weight** used by Molotov when the scenarii are randomly picked.\n      The functions with the highest values are more likely to be picked.\n      Integer, defaults to 1. This value is ignored when the\n      *scenario_picker* decorator is used.\n    - **delay** once the scenario is done, the worker will sleep\n      *delay* seconds. Float, defaults to 0.\n      The general --delay argument you can pass to Molotov\n      will be summed with this delay.\n    - **name** name of the scenario. If not provided, will use the\n      function __name___ attribute.\n\n    The decorated function receives an :class:`aiohttp.ClientSession` instance.\n    \"\"\"\n    def _scenario(func, *args, **kw):\n        _check_coroutine(func)\n        if weight > 0:\n            sname = name or func.__name__\n            data = {'name': sname,\n                    'weight': weight, 'delay': delay,\n                    'func': func, 'args': args, 'kw': kw}\n            _SCENARIO[sname] = data\n\n        @functools.wraps(func)\n        def __scenario(*args, **kw):\n            return func(*args, **kw)\n        return __scenario\n\n    return _scenario",
    "reference": "Decorator to register a function as a Molotov test.\n\n    Options:\n\n    - **weight** used by Molotov when the scenarii are randomly picked.\n      The functions with the highest values are more likely to be picked.\n      Integer, defaults to 1. This value is ignored when the\n      *scenario_picker* decorator is used.\n    - **delay** once the scenario is done, the worker will sleep\n      *delay* seconds. Float, defaults to 0.\n      The general --delay argument you can pass to Molotov\n      will be summed with this delay.\n    - **name** name of the scenario. If not provided, will use the\n      function __name___ attribute.\n\n    The decorated function receives an :class:`aiohttp.ClientSession` instance.",
    "generated": "Decorator to register a function as a Molotov test.\n\n    Options:\n\n    - **weight** used by Molotov when the scenarii are randomly picked.\n      The functions with the highest values are more likely to be picked.\n      Integer, defaults to 1. This value is ignored when the\n      *scenario_picker* decorator is used.\n    - **delay** once the scenario is done, the worker will sleep\n      *delay* seconds. Float, defaults to 0."
  },
  {
    "code": "def request(endpoint, verb='GET', session_options=None, **options):\n    \"\"\"Performs a synchronous request.\n\n    Uses a dedicated event loop and aiohttp.ClientSession object.\n\n    Options:\n\n    - endpoint: the endpoint to call\n    - verb: the HTTP verb to use (defaults: GET)\n    - session_options: a dict containing options to initialize the session\n      (defaults: None)\n    - options: extra options for the request (defaults: None)\n\n    Returns a dict object with the following keys:\n\n    - content: the content of the response\n    - status: the status\n    - headers: a dict with all the response headers\n    \"\"\"\n    req = functools.partial(_request, endpoint, verb, session_options,\n                            **options)\n    return _run_in_fresh_loop(req)",
    "reference": "Performs a synchronous request.\n\n    Uses a dedicated event loop and aiohttp.ClientSession object.\n\n    Options:\n\n    - endpoint: the endpoint to call\n    - verb: the HTTP verb to use (defaults: GET)\n    - session_options: a dict containing options to initialize the session\n      (defaults: None)\n    - options: extra options for the request (defaults: None)\n\n    Returns a dict object with the following keys:\n\n    - content: the content of the response\n    - status: the status\n    - headers: a dict with all the response headers",
    "generated": "Performs a synchronous request.\n\n    Uses a dedicated event loop and aiohttp.ClientSession object.\n\n    Options:\n\n    - endpoint: the endpoint to call\n    - verb: the HTTP verb to use (defaults: GET)"
  },
  {
    "code": "def get_var(name, factory=None):\n    \"\"\"Gets a global variable given its name.\n\n    If factory is not None and the variable is not set, factory\n    is a callable that will set the variable.\n\n    If not set, returns None.\n    \"\"\"\n    if name not in _VARS and factory is not None:\n        _VARS[name] = factory()\n    return _VARS.get(name)",
    "reference": "Gets a global variable given its name.\n\n    If factory is not None and the variable is not set, factory\n    is a callable that will set the variable.\n\n    If not set, returns None.",
    "generated": "Gets a global variable given its name.\n\n    If factory is not None, factory\n    is a callable that will set the variable."
  },
  {
    "code": "async def step(self, step_id, session, scenario=None):\n        \"\"\" single scenario call.\n\n        When it returns 1, it works. -1 the script failed,\n        0 the test is stopping or needs to stop.\n        \"\"\"\n        if scenario is None:\n            scenario = pick_scenario(self.wid, step_id)\n        try:\n            await self.send_event('scenario_start', scenario=scenario)\n\n            await scenario['func'](session, *scenario['args'],\n                                   **scenario['kw'])\n\n            await self.send_event('scenario_success', scenario=scenario)\n\n            if scenario['delay'] > 0.:\n                await cancellable_sleep(scenario['delay'])\n            return 1\n        except Exception as exc:\n            await self.send_event('scenario_failure',\n                                  scenario=scenario,\n                                  exception=exc)\n            if self.args.verbose > 0:\n                self.console.print_error(exc)\n                await self.console.flush()\n\n        return -1",
    "reference": "single scenario call.\n\n        When it returns 1, it works. -1 the script failed,\n        0 the test is stopping or needs to stop.",
    "generated": "single scenario call.\n\n        When it returns 1, it works. -1 the script failed,\n        0 the test is stopping or needs to stop."
  },
  {
    "code": "def main():\n    \"\"\"Moloslave clones a git repo and runs a molotov test\n    \"\"\"\n    parser = argparse.ArgumentParser(description='Github-based load test')\n\n    parser.add_argument('--version', action='store_true', default=False,\n                        help='Displays version and exits.')\n\n    parser.add_argument('--virtualenv', type=str, default='virtualenv',\n                        help='Virtualenv executable.')\n\n    parser.add_argument('--python', type=str, default=sys.executable,\n                        help='Python executable.')\n\n    parser.add_argument('--config', type=str, default='molotov.json',\n                        help='Path of the configuration file.')\n\n    parser.add_argument('repo', help='Github repo', type=str, nargs=\"?\")\n    parser.add_argument('run', help='Test to run', nargs=\"?\")\n\n    args = parser.parse_args()\n\n    if args.version:\n        print(__version__)\n        sys.exit(0)\n\n    tempdir = tempfile.mkdtemp()\n    curdir = os.getcwd()\n    os.chdir(tempdir)\n    print('Working directory is %s' % tempdir)\n    try:\n        clone_repo(args.repo)\n        config_file = os.path.join(tempdir, args.config)\n\n        with open(config_file) as f:\n            config = json.loads(f.read())\n\n        # creating the virtualenv\n        create_virtualenv(args.virtualenv, args.python)\n\n        # install deps\n        if 'requirements' in config['molotov']:\n            install_reqs(config['molotov']['requirements'])\n\n        # load deps into sys.path\n        pyver = '%d.%d' % (sys.version_info.major, sys.version_info.minor)\n        site_pkg = os.path.join(tempdir, 'venv', 'lib', 'python' + pyver,\n                                'site-packages')\n        site.addsitedir(site_pkg)\n        pkg_resources.working_set.add_entry(site_pkg)\n\n        # environment\n        if 'env' in config['molotov']:\n            for key, value in config['molotov']['env'].items():\n                os.environ[key] = value\n\n        run_test(**config['molotov']['tests'][args.run])\n    except Exception:\n        os.chdir(curdir)\n        shutil.rmtree(tempdir, ignore_errors=True)\n        raise",
    "reference": "Moloslave clones a git repo and runs a molotov test",
    "generated": "Moloslave clones a git repo and runs a molotov test\n   "
  },
  {
    "code": "def copy_files(source_files, target_directory, source_directory=None):\n    \"\"\"Copies a list of files to the specified directory.\n    If source_directory is provided, it will be prepended to each source file.\"\"\"\n    try:\n        os.makedirs(target_directory)\n    except:     # TODO: specific exception?\n        pass\n    for f in source_files:\n        source = os.path.join(source_directory, f) if source_directory else f\n        target = os.path.join(target_directory, f)\n        shutil.copy2(source, target)",
    "reference": "Copies a list of files to the specified directory.\n    If source_directory is provided, it will be prepended to each source file.",
    "generated": "Copies a list of files to the specified directory.\n    If source_directory is provided, it will be prepended to each source file."
  },
  {
    "code": "def yes_or_no(message):\n    \"\"\"Gets user input and returns True for yes and False for no.\"\"\"\n    while True:\n        print message, '(yes/no)',\n        line = raw_input()\n        if line is None:\n            return None\n        line = line.lower()\n        if line == 'y' or line == 'ye' or line == 'yes':\n            return True\n        if line == 'n' or line == 'no':\n            return False",
    "reference": "Gets user input and returns True for yes and False for no.",
    "generated": "Gets user input and returns True for yes and False for no."
  },
  {
    "code": "def add_plugin(plugin, directory=None):\n    \"\"\"Adds the specified plugin. This returns False if it was already added.\"\"\"\n    repo = require_repo(directory)\n    plugins = get_value(repo, 'plugins', expect_type=dict)\n    if plugin in plugins:\n        return False\n\n    plugins[plugin] = {}\n    set_value(repo, 'plugins', plugins)\n    return True",
    "reference": "Adds the specified plugin. This returns False if it was already added.",
    "generated": "Adds the specified plugin. This returns True if it was already added."
  },
  {
    "code": "def get_plugin_settings(plugin, directory=None):\n    \"\"\"Gets the settings for the specified plugin.\"\"\"\n    repo = require_repo(directory)\n    plugins = get_value(repo, 'plugins')\n    return plugins.get(plugin) if isinstance(plugins, dict) else None",
    "reference": "Gets the settings for the specified plugin.",
    "generated": "Gets the settings for the specified plugin."
  },
  {
    "code": "def preview(directory=None, host=None, port=None, watch=True):\n    \"\"\"Runs a local server to preview the working directory of a repository.\"\"\"\n    directory = directory or '.'\n    host = host or '127.0.0.1'\n    port = port or 5000\n\n    # TODO: admin interface\n\n    # TODO: use cache_only to keep from modifying output directly\n    out_directory = build(directory)\n\n    # Serve generated site\n    os.chdir(out_directory)\n    Handler = SimpleHTTPServer.SimpleHTTPRequestHandler\n    httpd = SocketServer.TCPServer((host, port), Handler)\n    print ' * Serving on http://%s:%s/' % (host, port)\n    httpd.serve_forever()",
    "reference": "Runs a local server to preview the working directory of a repository.",
    "generated": "Runs a local server to preview the working directory of a repository."
  },
  {
    "code": "def require_repo(directory=None):\n    \"\"\"Checks for a presentation repository and raises an exception if not found.\"\"\"\n    if directory and not os.path.isdir(directory):\n        raise ValueError('Directory not found: ' + repr(directory))\n    repo = repo_path(directory)\n    if not os.path.isdir(repo):\n        raise RepositoryNotFoundError(directory)\n    return repo",
    "reference": "Checks for a presentation repository and raises an exception if not found.",
    "generated": "Checks for a presentation repository and raises an exception if not found"
  },
  {
    "code": "def init(directory=None):\n    \"\"\"Initializes a Gitpress presentation repository at the specified directory.\"\"\"\n    repo = repo_path(directory)\n    if os.path.isdir(repo):\n        raise RepositoryAlreadyExistsError(directory, repo)\n\n    # Initialize repository with default template\n    shutil.copytree(default_template_path, repo)\n\n    message = '\"Default presentation content.\"'\n    subprocess.call(['git', 'init', '-q', repo])\n    subprocess.call(['git', 'add', '.'], cwd=repo)\n    subprocess.call(['git', 'commit', '-q', '-m', message], cwd=repo)\n\n    return repo",
    "reference": "Initializes a Gitpress presentation repository at the specified directory.",
    "generated": "Initializes a Gitpress presentation repository at the specified directory."
  },
  {
    "code": "def iterate_presentation_files(path=None, excludes=None, includes=None):\n    \"\"\"Iterates the repository presentation files relative to 'path',\n    not including themes. Note that 'includes' take priority.\"\"\"\n\n    # Defaults\n    if includes is None:\n        includes = []\n    if excludes is None:\n        excludes = []\n\n    # Transform glob patterns to regular expressions\n    includes_pattern = r'|'.join([fnmatch.translate(x) for x in includes]) or r'$.'\n    excludes_pattern = r'|'.join([fnmatch.translate(x) for x in excludes]) or r'$.'\n    includes_re = re.compile(includes_pattern)\n    excludes_re = re.compile(excludes_pattern)\n\n    def included(root, name):\n        \"\"\"Returns True if the specified file is a presentation file.\"\"\"\n        full_path = os.path.join(root, name)\n        # Explicitly included files takes priority\n        if includes_re.match(full_path):\n            return True\n        # Ignore special and excluded files\n        return (not specials_re.match(name)\n            and not excludes_re.match(full_path))\n\n    # Get a filtered list of paths to be built\n    for root, dirs, files in os.walk(path):\n        dirs[:] = [d for d in dirs if included(root, d)]\n        files = [f for f in files if included(root, f)]\n        for f in files:\n            yield os.path.relpath(os.path.join(root, f), path)",
    "reference": "Iterates the repository presentation files relative to 'path',\n    not including themes. Note that 'includes' take priority.",
    "generated": "Iterate the repository presentation files relative to 'path',\n    not including themes. Note that 'includes' take priority."
  },
  {
    "code": "def read_config_file(path):\n    \"\"\"Returns the configuration from the specified file.\"\"\"\n    try:\n        with open(path, 'r') as f:\n            return json.load(f, object_pairs_hook=OrderedDict)\n    except IOError as ex:\n        if ex != errno.ENOENT:\n            raise\n    return {}",
    "reference": "Returns the configuration from the specified file.",
    "generated": "Returns the configuration from the specified file."
  },
  {
    "code": "def write_config(repo_directory, config):\n    \"\"\"Writes the specified configuration to the presentation repository.\"\"\"\n    return write_config_file(os.path.join(repo_directory, config_file), config)",
    "reference": "Writes the specified configuration to the presentation repository.",
    "generated": "Writes the specified configuration to the presentation repository."
  },
  {
    "code": "def write_config_file(path, config):\n    \"\"\"Writes the specified configuration to the specified file.\"\"\"\n    contents = json.dumps(config, indent=4, separators=(',', ': ')) + '\\n'\n    try:\n        with open(path, 'w') as f:\n            f.write(contents)\n        return True\n    except IOError as ex:\n        if ex != errno.ENOENT:\n            raise\n    return False",
    "reference": "Writes the specified configuration to the specified file.",
    "generated": "Writes the specified configuration to the specified file."
  },
  {
    "code": "def get_value(repo_directory, key, expect_type=None):\n    \"\"\"Gets the value of the specified key in the config file.\"\"\"\n    config = read_config(repo_directory)\n    value = config.get(key)\n    if expect_type and value is not None and not isinstance(value, expect_type):\n        raise ConfigSchemaError('Expected config variable %s to be type %s, got %s'\n            % (repr(key), repr(expect_type), repr(type(value))))\n    return value",
    "reference": "Gets the value of the specified key in the config file.",
    "generated": "Gets the value of the specified key in the config file."
  },
  {
    "code": "def set_value(repo_directory, key, value, strict=True):\n    \"\"\"Sets the value of a particular key in the config file. This has no effect when setting to the same value.\"\"\"\n    if value is None:\n        raise ValueError('Argument \"value\" must not be None.')\n\n    # Read values and do nothing if not making any changes\n    config = read_config(repo_directory)\n    old = config.get(key)\n    if old == value:\n        return old\n\n    # Check schema\n    if strict and old is not None and not isinstance(old, type(value)):\n        raise ConfigSchemaError('Expected config variable %s to be type %s, got %s'\n            % (repr(key), repr(type(value)), repr(type(old))))\n\n    # Set new value and save results\n    config[key] = value\n    write_config(repo_directory, config)\n    return old",
    "reference": "Sets the value of a particular key in the config file. This has no effect when setting to the same value.",
    "generated": "Sets the value of a particular key in the config file. This has no effect when setting to the same value."
  },
  {
    "code": "def build(content_directory=None, out_directory=None):\n    \"\"\"Builds the site from its content and presentation repository.\"\"\"\n    content_directory = content_directory or '.'\n    out_directory = os.path.abspath(out_directory or default_out_directory)\n    repo = require_repo(content_directory)\n\n    # Prevent user mistakes\n    if out_directory == '.':\n        raise ValueError('Output directory must be different than the source directory: ' + repr(out_directory))\n    if os.path.basename(os.path.relpath(out_directory, content_directory)) == '..':\n        raise ValueError('Output directory must not contain the source directory: ' + repr(out_directory))\n\n    # TODO: read config\n    # TODO: use virtualenv\n    # TODO: init and run plugins\n    # TODO: process with active theme\n\n    # Collect and copy static files\n    files = presentation_files(repo)\n    remove_directory(out_directory)\n    copy_files(files, out_directory, repo)\n\n    return out_directory",
    "reference": "Builds the site from its content and presentation repository.",
    "generated": "Builds the site from its content and presentation repository."
  },
  {
    "code": "def gpp(argv=None):\n    \"\"\"Shortcut function for running the previewing command.\"\"\"\n    if argv is None:\n        argv = sys.argv[1:]\n    argv.insert(0, 'preview')\n    return main(argv)",
    "reference": "Shortcut function for running the previewing command.",
    "generated": "Shortcut function for running the previewing command."
  },
  {
    "code": "def use_theme(theme, directory=None):\n    \"\"\"Switches to the specified theme. This returns False if switching to the already active theme.\"\"\"\n    repo = require_repo(directory)\n    if theme not in list_themes(directory):\n        raise ThemeNotFoundError(theme)\n\n    old_theme = set_value(repo, 'theme', theme)\n    return old_theme != theme",
    "reference": "Switches to the specified theme. This returns False if switching to the already active theme.",
    "generated": "Switches to the specified theme. This returns True if switching to the already active theme."
  },
  {
    "code": "def data_type(data, grouped=False, columns=None, key_on='idx', iter_idx=None):\n    '''Data type check for automatic import'''\n    if iter_idx:\n        return Data.from_mult_iters(idx=iter_idx, **data)\n    if pd:\n        if isinstance(data, (pd.Series, pd.DataFrame)):\n            return Data.from_pandas(data, grouped=grouped, columns=columns,\n                                    key_on=key_on)\n    if isinstance(data, (list, tuple, dict)):\n            return Data.from_iter(data)\n    else:\n        raise ValueError('This data type is not supported by Vincent.')",
    "reference": "Data type check for automatic import",
    "generated": "Data type check for automatic import"
  },
  {
    "code": "def rebind(self, column=None, brew='GnBu'):\n        \"\"\"Bind a new column to the data map\n\n        Parameters\n        ----------\n        column: str, default None\n            Pandas DataFrame column name\n        brew: str, default None\n            Color brewer abbreviation. See colors.py\n\n        \"\"\"\n        self.data['table'] = Data.keypairs(\n            self.raw_data, columns=[self.data_key, column])\n        domain = [Data.serialize(self.raw_data[column].min()),\n                  Data.serialize(self.raw_data[column].quantile(0.95))]\n        scale = Scale(name='color', type='quantize', domain=domain,\n                      range=brews[brew])\n        self.scales['color'] = scale",
    "reference": "Bind a new column to the data map\n\n        Parameters\n        ----------\n        column: str, default None\n            Pandas DataFrame column name\n        brew: str, default None\n            Color brewer abbreviation. See colors.py",
    "generated": "Bind a new column to the data map\n\n        Parameters\n        ----------\n        column: str, default None\n            Pandas DataFrame column name\n        brew: str, default None\n            Color brewer abbreviation. See colors.py\n\n        \"\"\"Bind a new column to the data map\n\n        Parameters\n        ----------\n        column: str, default None\n            Color brewer abbreviation. See colors.py"
  },
  {
    "code": "def axis_titles(self, x=None, y=None):\n        \"\"\"Apply axis titles to the figure.\n\n        This is a convenience method for manually modifying the \"Axes\" mark.\n\n        Parameters\n        ----------\n        x: string, default 'null'\n            X-axis title\n        y: string, default 'null'\n            Y-axis title\n\n        Example\n        -------\n        >>>vis.axis_titles(y=\"Data 1\", x=\"Data 2\")\n\n        \"\"\"\n        keys = self.axes.get_keys()\n\n        if keys:\n            for key in keys:\n                if key == 'x':\n                    self.axes[key].title = x\n                elif key == 'y':\n                    self.axes[key].title = y\n        else:\n            self.axes.extend([Axis(type='x', title=x),\n                              Axis(type='y', title=y)])\n        return self",
    "reference": "Apply axis titles to the figure.\n\n        This is a convenience method for manually modifying the \"Axes\" mark.\n\n        Parameters\n        ----------\n        x: string, default 'null'\n            X-axis title\n        y: string, default 'null'\n            Y-axis title\n\n        Example\n        -------\n        >>>vis.axis_titles(y=\"Data 1\", x=\"Data 2\")",
    "generated": "Apply axis titles to the figure.\n\n        This is a convenience method for manually modifying the \"Axes\" mark.\n\n        Parameters\n        ----------\n        x: string, default 'null'\n            X-axis title\n        y: string, default 'null'\n            Y-axis title\n        Example\n        -------\n        >>>vis.axis_titles(x=\"Data 1\", x=\"Data 2)\""
  },
  {
    "code": "def _set_axis_properties(self, axis):\n        \"\"\"Set AxisProperties and PropertySets\"\"\"\n        if not getattr(axis, 'properties'):\n            axis.properties = AxisProperties()\n            for prop in ['ticks', 'axis', 'major_ticks', 'minor_ticks',\n                         'title', 'labels']:\n                setattr(axis.properties, prop, PropertySet())",
    "reference": "Set AxisProperties and PropertySets",
    "generated": "Set AxisProperties and PropertySets"
  },
  {
    "code": "def _set_all_axis_color(self, axis, color):\n        \"\"\"Set axis ticks, title, labels to given color\"\"\"\n        for prop in ['ticks', 'axis', 'major_ticks', 'minor_ticks', 'title',\n                     'labels']:\n            prop_set = getattr(axis.properties, prop)\n            if color and prop in ['title', 'labels']:\n                prop_set.fill = ValueRef(value=color)\n            elif color and prop in ['axis', 'major_ticks', 'minor_ticks',\n                                    'ticks']:\n                prop_set.stroke = ValueRef(value=color)",
    "reference": "Set axis ticks, title, labels to given color",
    "generated": "Set axis ticks, title, labels to given color"
  },
  {
    "code": "def _axis_properties(self, axis, title_size, title_offset, label_angle,\n                         label_align, color):\n        \"\"\"Assign axis properties\"\"\"\n        if self.axes:\n            axis = [a for a in self.axes if a.scale == axis][0]\n            self._set_axis_properties(axis)\n            self._set_all_axis_color(axis, color)\n\n            if title_size:\n                axis.properties.title.font_size = ValueRef(value=title_size)\n            if label_angle:\n                axis.properties.labels.angle = ValueRef(value=label_angle)\n            if label_align:\n                axis.properties.labels.align = ValueRef(value=label_align)\n            if title_offset:\n                axis.properties.title.dy = ValueRef(value=title_offset)\n        else:\n            raise ValueError('This Visualization has no axes!')",
    "reference": "Assign axis properties",
    "generated": "Assign axis properties."
  },
  {
    "code": "def common_axis_properties(self, color=None, title_size=None):\n        \"\"\"Set common axis properties such as color\n\n        Parameters\n        ----------\n        color: str, default None\n            Hex color str, etc\n        \"\"\"\n        if self.axes:\n            for axis in self.axes:\n                self._set_axis_properties(axis)\n                self._set_all_axis_color(axis, color)\n                if title_size:\n                    ref = ValueRef(value=title_size)\n                    axis.properties.title.font_size = ref\n        else:\n            raise ValueError('This Visualization has no axes!')\n        return self",
    "reference": "Set common axis properties such as color\n\n        Parameters\n        ----------\n        color: str, default None\n            Hex color str, etc",
    "generated": "Set common axis properties such as color\n\n        Parameters\n        ----------\n        color: str, default None\n            Hex color str, etc"
  },
  {
    "code": "def x_axis_properties(self, title_size=None, title_offset=None,\n                          label_angle=None, label_align=None, color=None):\n        \"\"\"Change x-axis title font size and label angle\n\n        Parameters\n        ----------\n        title_size: int, default None\n            Title size, in px\n        title_offset: int, default None\n            Pixel offset from given axis\n        label_angle: int, default None\n            label angle in degrees\n        label_align: str, default None\n            Label alignment\n        color: str, default None\n            Hex color\n        \"\"\"\n        self._axis_properties('x', title_size, title_offset, label_angle,\n                              label_align, color)\n        return self",
    "reference": "Change x-axis title font size and label angle\n\n        Parameters\n        ----------\n        title_size: int, default None\n            Title size, in px\n        title_offset: int, default None\n            Pixel offset from given axis\n        label_angle: int, default None\n            label angle in degrees\n        label_align: str, default None\n            Label alignment\n        color: str, default None\n            Hex color",
    "generated": "Change x-axis title font size and label angle\n\n        Parameters\n        ----------\n        title_size: int, default None\n            Title size, in px\n        title_offset: int, default None\n            Pixel offset from given axis\n        label_angle: int, default None\n            label angle in degrees\n        label_align: str, default None\n            Label alignment\n        color: str, default None\n            Hex color"
  },
  {
    "code": "def y_axis_properties(self, title_size=None, title_offset=None,\n                          label_angle=None, label_align=None, color=None):\n        \"\"\"Change y-axis title font size and label angle\n\n        Parameters\n        ----------\n        title_size: int, default None\n            Title size, in px\n        title_offset: int, default None\n            Pixel offset from given axis\n        label_angle: int, default None\n            label angle in degrees\n        label_align: str, default None\n            Label alignment\n        color: str, default None\n            Hex color\n        \"\"\"\n        self._axis_properties('y', title_size, title_offset, label_angle,\n                              label_align, color)\n        return self",
    "reference": "Change y-axis title font size and label angle\n\n        Parameters\n        ----------\n        title_size: int, default None\n            Title size, in px\n        title_offset: int, default None\n            Pixel offset from given axis\n        label_angle: int, default None\n            label angle in degrees\n        label_align: str, default None\n            Label alignment\n        color: str, default None\n            Hex color",
    "generated": "Change y-axis title font size and label angle\n\n        Parameters\n        ----------\n        title_size: int, default None\n            Title size, in px\n        title_offset: int, default None\n            Pixel offset from given axis\n        label_angle: int, default None\n            label angle in degrees\n        label_align: str, default None\n            Label alignment\n        color: str, default None\n            Hex color"
  },
  {
    "code": "def legend(self, title=None, scale='color', text_color=None):\n        \"\"\"Convience method for adding a legend to the figure.\n\n        Important: This defaults to the color scale that is generated with\n        Line, Area, Stacked Line, etc charts. For bar charts, the scale ref is\n        usually 'y'.\n\n        Parameters\n        ----------\n        title: string, default None\n            Legend Title\n        scale: string, default 'color'\n            Scale reference for legend\n        text_color: str, default None\n            Title and label color\n        \"\"\"\n\n        self.legends.append(Legend(title=title, fill=scale, offset=0,\n                                   properties=LegendProperties()))\n        if text_color:\n            color_props = PropertySet(fill=ValueRef(value=text_color))\n            self.legends[0].properties.labels = color_props\n            self.legends[0].properties.title = color_props\n        return self",
    "reference": "Convience method for adding a legend to the figure.\n\n        Important: This defaults to the color scale that is generated with\n        Line, Area, Stacked Line, etc charts. For bar charts, the scale ref is\n        usually 'y'.\n\n        Parameters\n        ----------\n        title: string, default None\n            Legend Title\n        scale: string, default 'color'\n            Scale reference for legend\n        text_color: str, default None\n            Title and label color",
    "generated": "Convience method for adding a legend to the figure.\n\n        Important: This defaults to the color scale that is generated with\n        Line, Area, Stacked Line, etc charts. For bar charts, the scale ref is\n        usually 'y'."
  },
  {
    "code": "def colors(self, brew=None, range_=None):\n        \"\"\"Convenience method for adding color brewer scales to charts with a\n        color scale, such as stacked or grouped bars.\n\n        See the colors here: http://colorbrewer2.org/\n\n        Or here: http://bl.ocks.org/mbostock/5577023\n\n        This assumes that a 'color' scale exists on your chart.\n\n        Parameters\n        ----------\n        brew: string, default None\n            Color brewer scheme (BuGn, YlOrRd, etc)\n        range: list, default None\n            List of colors. Ex: ['#ac4142', '#d28445', '#f4bf75']\n        \"\"\"\n        if brew:\n            self.scales['color'].range = brews[brew]\n        elif range_:\n            self.scales['color'].range = range_\n        return self",
    "reference": "Convenience method for adding color brewer scales to charts with a\n        color scale, such as stacked or grouped bars.\n\n        See the colors here: http://colorbrewer2.org/\n\n        Or here: http://bl.ocks.org/mbostock/5577023\n\n        This assumes that a 'color' scale exists on your chart.\n\n        Parameters\n        ----------\n        brew: string, default None\n            Color brewer scheme (BuGn, YlOrRd, etc)\n        range: list, default None\n            List of colors. Ex: ['#ac4142', '#d28445', '#f4bf75']",
    "generated": "Add color brewer scales to charts with a\n        color scale, such as stacked or grouped bars.\n\n        See the colors here: http://colorbrewer2.org/\n\n        Or here: http://colorbrewer2.org/\n\n        Or here: http://bl.ocks.org/mbostock/5577023\n\n        This assumes that a 'color' scale exists on your chart.\n\n        Parameters\n        ----------\n        brew: string, default None\n            Color brewer scheme (BuGn, YlOrRd"
  },
  {
    "code": "def validate(self, require_all=True, scale='colors'):\n        \"\"\"Validate the visualization contents.\n\n        Parameters\n        ----------\n        require_all : boolean, default True\n            If True (default), then all fields ``data``, ``scales``,\n            ``axes``, and ``marks`` must be defined. The user is allowed to\n            disable this if the intent is to define the elements\n            client-side.\n\n        If the contents of the visualization are not valid Vega, then a\n        :class:`ValidationError` is raised.\n        \"\"\"\n        super(self.__class__, self).validate()\n        required_attribs = ('data', 'scales', 'axes', 'marks')\n        for elem in required_attribs:\n            attr = getattr(self, elem)\n            if attr:\n                # Validate each element of the sets of data, etc\n                for entry in attr:\n                    entry.validate()\n                names = [a.name for a in attr]\n                if len(names) != len(set(names)):\n                    raise ValidationError(elem + ' has duplicate names')\n            elif require_all:\n                raise ValidationError(\n                    elem + ' must be defined for valid visualization')",
    "reference": "Validate the visualization contents.\n\n        Parameters\n        ----------\n        require_all : boolean, default True\n            If True (default), then all fields ``data``, ``scales``,\n            ``axes``, and ``marks`` must be defined. The user is allowed to\n            disable this if the intent is to define the elements\n            client-side.\n\n        If the contents of the visualization are not valid Vega, then a\n        :class:`ValidationError` is raised.",
    "generated": "Validate the visualization contents.\n\n        Parameters\n        ----------\n        require_all : boolean, default True\n            If True (default), then all fields ``data`, ``scales``,\n            ``axes``, and ``marks`` must be defined. The user is allowed to\n            disable this if the intent is to define the elements\n            client-side.\n\n        If the contents of the visualization are not valid Vega, then a\n        :class:`ValidationError` is raised."
  },
  {
    "code": "def display(self):\n        \"\"\"Display the visualization inline in the IPython notebook.\n\n        This is deprecated, use the following instead::\n\n            from IPython.display import display\n            display(viz)\n        \"\"\"\n        from IPython.core.display import display, HTML\n        display(HTML(self._repr_html_()))",
    "reference": "Display the visualization inline in the IPython notebook.\n\n        This is deprecated, use the following instead::\n\n            from IPython.display import display\n            display(viz)",
    "generated": "Display the visualization inline in the IPython notebook.\n\n        This is deprecated instead::\n\n            from IPython.display import display\n            display(viz)"
  },
  {
    "code": "def validate(self, *args):\n        \"\"\"Validate contents of class\n        \"\"\"\n        super(self.__class__, self).validate(*args)\n        if not self.name:\n            raise ValidationError('name is required for Data')",
    "reference": "Validate contents of class",
    "generated": "Validate contents of class\n       "
  },
  {
    "code": "def serialize(obj):\n        \"\"\"Convert an object into a JSON-serializable value\n\n        This is used by the ``from_pandas`` and ``from_numpy`` functions to\n        convert data to JSON-serializable types when loading.\n        \"\"\"\n        if isinstance(obj, str_types):\n            return obj\n        elif hasattr(obj, 'timetuple'):\n            return int(time.mktime(obj.timetuple())) * 1000\n        elif hasattr(obj, 'item'):\n            return obj.item()\n        elif hasattr(obj, '__float__'):\n            if isinstance(obj, int):\n                return int(obj)\n            else:\n                return float(obj)\n        elif hasattr(obj, '__int__'):\n            return int(obj)\n        else:\n            raise LoadError('cannot serialize index of type '\n                            + type(obj).__name__)",
    "reference": "Convert an object into a JSON-serializable value\n\n        This is used by the ``from_pandas`` and ``from_numpy`` functions to\n        convert data to JSON-serializable types when loading.",
    "generated": "Convert an object into a JSON-serializable value\n\n        This is used by the ``from_pandas`` and ``from_numpy`` functions to\n        convert data to JSON-serializable types when loading."
  },
  {
    "code": "def from_pandas(cls, data, columns=None, key_on='idx', name=None,\n                    series_key='data', grouped=False, records=False, **kwargs):\n        \"\"\"Load values from a pandas ``Series`` or ``DataFrame`` object\n\n        Parameters\n        ----------\n        data : pandas ``Series`` or ``DataFrame``\n            Pandas object to import data from.\n        columns: list, default None\n            DataFrame columns to convert to Data. Keys default to col names.\n            If columns are given and on_index is False, x-axis data will\n            default to the first column.\n        key_on: string, default 'index'\n            Value to key on for x-axis data. Defaults to index.\n        name : string, default None\n            Applies to the ``name`` attribute of the generated class. If\n            ``None`` (default), then the ``name`` attribute of ``pd_obj`` is\n            used if it exists, or ``'table'`` if it doesn't.\n        series_key : string, default 'data'\n            Applies only to ``Series``. If ``None`` (default), then defaults to\n            data.name. For example, if ``series_key`` is ``'x'``, then the\n            entries of the ``values`` list\n            will be ``{'idx': ..., 'col': 'x', 'val': ...}``.\n        grouped: boolean, default False\n            Pass true for an extra grouping parameter\n        records: boolean, defaule False\n            Requires Pandas 0.12 or greater. Writes the Pandas DataFrame\n            using the df.to_json(orient='records') formatting.\n        **kwargs : dict\n            Additional arguments passed to the :class:`Data` constructor.\n        \"\"\"\n        # Note: There's an experimental JSON encoder floating around in\n        # pandas land that hasn't made it into the main branch. This\n        # function should be revisited if it ever does.\n        if not pd:\n            raise LoadError('pandas could not be imported')\n        if not hasattr(data, 'index'):\n            raise ValueError('Please load a Pandas object.')\n\n        if name:\n            vega_data = cls(name=name, **kwargs)\n        else:\n            vega_data = cls(name='table', **kwargs)\n\n        pd_obj = data.copy()\n        if columns:\n            pd_obj = data[columns]\n        if key_on != 'idx':\n            pd_obj.index = data[key_on]\n        if records:\n            # The worst\n            vega_data.values = json.loads(pd_obj.to_json(orient='records'))\n            return vega_data\n\n        vega_data.values = []\n\n        if isinstance(pd_obj, pd.Series):\n            data_key = data.name or series_key\n            for i, v in pd_obj.iteritems():\n                value = {}\n                value['idx'] = cls.serialize(i)\n                value['col'] = data_key\n                value['val'] = cls.serialize(v)\n                vega_data.values.append(value)\n\n        elif isinstance(pd_obj, pd.DataFrame):\n            # We have to explicitly convert the column names to strings\n            # because the json serializer doesn't allow for integer keys.\n            for i, row in pd_obj.iterrows():\n                for num, (k, v) in enumerate(row.iteritems()):\n                    value = {}\n                    value['idx'] = cls.serialize(i)\n                    value['col'] = cls.serialize(k)\n                    value['val'] = cls.serialize(v)\n                    if grouped:\n                        value['group'] = num\n                    vega_data.values.append(value)\n        else:\n            raise ValueError('cannot load from data type '\n                             + type(pd_obj).__name__)\n        return vega_data",
    "reference": "Load values from a pandas ``Series`` or ``DataFrame`` object\n\n        Parameters\n        ----------\n        data : pandas ``Series`` or ``DataFrame``\n            Pandas object to import data from.\n        columns: list, default None\n            DataFrame columns to convert to Data. Keys default to col names.\n            If columns are given and on_index is False, x-axis data will\n            default to the first column.\n        key_on: string, default 'index'\n            Value to key on for x-axis data. Defaults to index.\n        name : string, default None\n            Applies to the ``name`` attribute of the generated class. If\n            ``None`` (default), then the ``name`` attribute of ``pd_obj`` is\n            used if it exists, or ``'table'`` if it doesn't.\n        series_key : string, default 'data'\n            Applies only to ``Series``. If ``None`` (default), then defaults to\n            data.name. For example, if ``series_key`` is ``'x'``, then the\n            entries of the ``values`` list\n            will be ``{'idx': ..., 'col': 'x', 'val': ...}``.\n        grouped: boolean, default False\n            Pass true for an extra grouping parameter\n        records: boolean, defaule False\n            Requires Pandas 0.12 or greater. Writes the Pandas DataFrame\n            using the df.to_json(orient='records') formatting.\n        **kwargs : dict\n            Additional arguments passed to the :class:`Data` constructor.",
    "generated": "Load values from a pandas ``Series`` or ``DataFrame``\n            Pandas object to import data from.\n        columns: list, default None\n            DataFrame columns to convert to Data. Keys default to col names.\n            If columns are given and on_index is False, x-axis data will\n            default to the first column.\n        key_on: string, default 'index'\n            Value to key on for x-axis data. Defaults to index.\n        key_on: string, default 'index'\n            Value to key on for x-axis data. Defaults to index."
  },
  {
    "code": "def from_numpy(cls, np_obj, name, columns, index=None, index_key=None,\n                   **kwargs):\n        \"\"\"Load values from a numpy array\n\n        Parameters\n        ----------\n        np_obj : numpy.ndarray\n            numpy array to load data from\n        name : string\n            ``name`` field for the data\n        columns : iterable\n            Sequence of column names, from left to right. Must have same\n            length as the number of columns of ``np_obj``.\n        index : iterable, default None\n            Sequence of indices from top to bottom. If ``None`` (default),\n            then the indices are integers starting at 0. Must have same\n            length as the number of rows of ``np_obj``.\n        index_key : string, default None\n            Key to use for the index. If ``None`` (default), ``idx`` is\n            used.\n        **kwargs : dict\n            Additional arguments passed to the :class:`Data` constructor\n\n        Notes\n        -----\n        The individual elements of ``np_obj``, ``columns``, and ``index``\n        must return valid values from :func:`Data.serialize`.\n        \"\"\"\n        if not np:\n            raise LoadError('numpy could not be imported')\n\n        _assert_is_type('numpy object', np_obj, np.ndarray)\n\n        # Integer index if none is provided\n        index = index or range(np_obj.shape[0])\n        # Explicitly map dict-keys to strings for JSON serializer.\n        columns = list(map(str, columns))\n\n        index_key = index_key or cls._default_index_key\n\n        if len(index) != np_obj.shape[0]:\n            raise LoadError(\n                'length of index must be equal to number of rows of array')\n        elif len(columns) != np_obj.shape[1]:\n            raise LoadError(\n                'length of columns must be equal to number of columns of '\n                'array')\n\n        data = cls(name=name, **kwargs)\n        data.values = [\n            dict([(index_key, cls.serialize(idx))] +\n                 [(col, x) for col, x in zip(columns, row)])\n            for idx, row in zip(index, np_obj.tolist())]\n\n        return data",
    "reference": "Load values from a numpy array\n\n        Parameters\n        ----------\n        np_obj : numpy.ndarray\n            numpy array to load data from\n        name : string\n            ``name`` field for the data\n        columns : iterable\n            Sequence of column names, from left to right. Must have same\n            length as the number of columns of ``np_obj``.\n        index : iterable, default None\n            Sequence of indices from top to bottom. If ``None`` (default),\n            then the indices are integers starting at 0. Must have same\n            length as the number of rows of ``np_obj``.\n        index_key : string, default None\n            Key to use for the index. If ``None`` (default), ``idx`` is\n            used.\n        **kwargs : dict\n            Additional arguments passed to the :class:`Data` constructor\n\n        Notes\n        -----\n        The individual elements of ``np_obj``, ``columns``, and ``index``\n        must return valid values from :func:`Data.serialize`.",
    "generated": "Load values from a numpy array\n\n        Parameters\n        ----------\n        np_obj : numpy.ndarray\n            numpy array to load data from\n        name : string\n            ``name`` field for the data\n        columns : iterable\n            Sequence of column names, from left to right. Must have same\n            length as the number of columns of ``np_obj``.\n        index : iterable, default None\n            Key to use for the index. If ``None`` (default), ``idx`` is\n            used."
  },
  {
    "code": "def from_mult_iters(cls, name=None, idx=None, **kwargs):\n        \"\"\"Load values from multiple iters\n\n        Parameters\n        ----------\n        name : string, default None\n            Name of the data set. If None (default), the name will be set to\n            ``'table'``.\n        idx: string, default None\n            Iterable to use for the data index\n        **kwargs : dict of iterables\n            The ``values`` field will contain dictionaries with keys for\n            each of the iterables provided. For example,\n\n                d = Data.from_iters(idx='x', x=[0, 1, 5], y=(10, 20, 30))\n\n            would result in ``d`` having a ``values`` field with\n\n                [{'idx': 0, 'col': 'y', 'val': 10},\n                 {'idx': 1, 'col': 'y', 'val': 20}\n\n            If the iterables are not the same length, then ValueError is\n            raised.\n        \"\"\"\n        if not name:\n            name = 'table'\n\n        lengths = [len(v) for v in kwargs.values()]\n\n        if len(set(lengths)) != 1:\n            raise ValueError('Iterables must all be same length')\n\n        if not idx:\n            raise ValueError('Must provide iter name index reference')\n\n        index = kwargs.pop(idx)\n        vega_vals = []\n        for k, v in sorted(kwargs.items()):\n            for idx, val in zip(index, v):\n                value = {}\n                value['idx'] = idx\n                value['col'] = k\n                value['val'] = val\n                vega_vals.append(value)\n\n        return cls(name, values=vega_vals)",
    "reference": "Load values from multiple iters\n\n        Parameters\n        ----------\n        name : string, default None\n            Name of the data set. If None (default), the name will be set to\n            ``'table'``.\n        idx: string, default None\n            Iterable to use for the data index\n        **kwargs : dict of iterables\n            The ``values`` field will contain dictionaries with keys for\n            each of the iterables provided. For example,\n\n                d = Data.from_iters(idx='x', x=[0, 1, 5], y=(10, 20, 30))\n\n            would result in ``d`` having a ``values`` field with\n\n                [{'idx': 0, 'col': 'y', 'val': 10},\n                 {'idx': 1, 'col': 'y', 'val': 20}\n\n            If the iterables are not the same length, then ValueError is\n            raised.",
    "generated": "Load values from multiple iters\n\n        Parameters\n        ----------\n        name : string, default None\n            Name of the data set. If None (default), the name will be set to\n            ``'table'``.\n        idx: string, default None\n            Iterable to use for the data index\n        **kwargs : dict of iterables\n            The ``values`` field will contain dictionaries with keys for\n            each of the iterables provided. For example,\n\n                d = Data.from_iters(idx='x', x=[0, 1, 5], y=(10, 20,"
  },
  {
    "code": "def from_iter(cls, data, name=None):\n        \"\"\"Convenience method for loading data from an iterable.\n\n        Defaults to numerical indexing for x-axis.\n\n        Parameters\n        ----------\n        data: iterable\n            An iterable of data (list, tuple, dict of key/val pairs)\n        name: string, default None\n            Name of the data set. If None (default), the name will be set to\n            ``'table'``.\n\n        \"\"\"\n\n        if not name:\n            name = 'table'\n        if isinstance(data, (list, tuple)):\n            data = {x: y for x, y in enumerate(data)}\n\n        values = [{'idx': k, 'col': 'data', 'val': v}\n                  for k, v in sorted(data.items())]\n        return cls(name, values=values)",
    "reference": "Convenience method for loading data from an iterable.\n\n        Defaults to numerical indexing for x-axis.\n\n        Parameters\n        ----------\n        data: iterable\n            An iterable of data (list, tuple, dict of key/val pairs)\n        name: string, default None\n            Name of the data set. If None (default), the name will be set to\n            ``'table'``.",
    "generated": "Convenience method for loading data from an iterable.\n\n        Defaults to numerical indexing for x-axis.\n\n        Parameters\n        ----------\n        data: iterable\n            An iterable of data (list, tuple, dict of key/val pairs)\n        name: string, default None\n            Name of the data set. If None (default), the name will be set to\n            ``'table'`."
  },
  {
    "code": "def _numpy_to_values(data):\n        '''Convert a NumPy array to values attribute'''\n        def to_list_no_index(xvals, yvals):\n            return [{\"x\": x, \"y\": np.asscalar(y)}\n                    for x, y in zip(xvals, yvals)]\n\n        if len(data.shape) == 1 or data.shape[1] == 1:\n            xvals = range(data.shape[0] + 1)\n            values = to_list_no_index(xvals, data)\n        elif len(data.shape) == 2:\n            if data.shape[1] == 2:\n                # NumPy arrays and matrices have different iteration rules.\n                if isinstance(data, np.matrix):\n                    xidx = (0, 0)\n                    yidx = (0, 1)\n                else:\n                    xidx = 0\n                    yidx = 1\n\n                xvals = [np.asscalar(row[xidx]) for row in data]\n                yvals = [np.asscalar(row[yidx]) for row in data]\n                values = [{\"x\": x, \"y\": y} for x, y in zip(xvals, yvals)]\n            else:\n                raise ValueError('arrays with > 2 columns not supported')\n        else:\n            raise ValueError('invalid dimensions for ndarray')\n\n        return values",
    "reference": "Convert a NumPy array to values attribute",
    "generated": "Convert a NumPy array to values attribute'''\n        def to_list_no_index(xvals, yvals):\n            return [{\"x\": x, \"y\": np.asscalar(y)}"
  },
  {
    "code": "def to_json(self, validate=False, pretty_print=True, data_path=None):\n        \"\"\"Convert data to JSON\n\n        Parameters\n        ----------\n        data_path : string\n            If not None, then data is written to a separate file at the\n            specified path. Note that the ``url`` attribute if the data must\n            be set independently for the data to load correctly.\n\n        Returns\n        -------\n        string\n            Valid Vega JSON.\n        \"\"\"\n        # TODO: support writing to separate file\n        return super(self.__class__, self).to_json(validate=validate,\n                                                   pretty_print=pretty_print)",
    "reference": "Convert data to JSON\n\n        Parameters\n        ----------\n        data_path : string\n            If not None, then data is written to a separate file at the\n            specified path. Note that the ``url`` attribute if the data must\n            be set independently for the data to load correctly.\n\n        Returns\n        -------\n        string\n            Valid Vega JSON.",
    "generated": "Convert data to JSON\n\n        Parameters\n        ----------\n        data_path : string\n            If not None, then data is written to a separate file at the\n            specified path. Note that the ``url`` attribute if the data must\n            be set independently for the data to load correctly."
  },
  {
    "code": "def _assert_is_type(name, value, value_type):\n    \"\"\"Assert that a value must be a given type.\"\"\"\n    if not isinstance(value, value_type):\n        if type(value_type) is tuple:\n            types = ', '.join(t.__name__ for t in value_type)\n            raise ValueError('{0} must be one of ({1})'.format(name, types))\n        else:\n            raise ValueError('{0} must be {1}'\n                             .format(name, value_type.__name__))",
    "reference": "Assert that a value must be a given type.",
    "generated": "Assert that a value must be a given type."
  },
  {
    "code": "def grammar(grammar_type=None, grammar_name=None):\n    \"\"\"Decorator to define properties that map to the ``grammar``\n    dict. This dict is the canonical representation of the Vega grammar\n    within Vincent.\n\n    This decorator is intended for classes that map to some pre-defined JSON\n    structure, such as axes, data, marks, scales, etc. It is assumed that this\n    decorates functions with an instance of ``self.grammar``.\n\n    Parameters\n    ----------\n    grammar_type : type or tuple of types, default None\n        If the argument to the decorated function is not of the given types,\n        then a ValueError is raised. No type checking is done if the type is\n        None (default).\n    grammar_name : string, default None\n        An optional name to map to the internal ``grammar`` dict. If None\n        (default), then the key for the dict is the name of the function\n        being decorated. If not None, then it will be the name specified\n        here. This is useful if the expected JSON field name is a Python\n        keyword or has an un-Pythonic name.\n\n    This should decorate a \"validator\" function that should return no value\n    but raise an exception if the provided value is not valid Vega grammar. If\n    the validator throws no exception, then the value is assigned to the\n    ``grammar`` dict.\n\n    The validator function should take only one argument - the value to be\n    validated - so that no ``self`` argument is included; the validator\n    should not modify the class.\n\n    If no arguments are given, then no type-checking is done the property\n    will be mapped to a field with the name of the decorated function.\n\n    The doc string for the property is taken from the validator functions's\n    doc string.\n    \"\"\"\n    def grammar_creator(validator, name):\n        def setter(self, value):\n            if isinstance(grammar_type, (type, tuple)):\n                _assert_is_type(validator.__name__, value, grammar_type)\n            validator(value)\n            self.grammar[name] = value\n\n        def getter(self):\n            return self.grammar.get(name, None)\n\n        def deleter(self):\n            if name in self.grammar:\n                del self.grammar[name]\n\n        return property(getter, setter, deleter, validator.__doc__)\n\n    if isinstance(grammar_type, (type, tuple)):\n        # If grammar_type is a type, return another decorator.\n        def grammar_dec(validator):\n            # Make sure to use the grammar name if it's there.\n            if grammar_name:\n                return grammar_creator(validator, grammar_name)\n            else:\n                return grammar_creator(validator, validator.__name__)\n        return grammar_dec\n    elif isinstance(grammar_name, str_types):\n        # If grammar_name is a string, use that name and return another\n        # decorator.\n        def grammar_dec(validator):\n            return grammar_creator(validator, grammar_name)\n        return grammar_dec\n    else:\n        # Otherwise we assume that grammar_type is actually the function being\n        # decorated.\n        return grammar_creator(grammar_type, grammar_type.__name__)",
    "reference": "Decorator to define properties that map to the ``grammar``\n    dict. This dict is the canonical representation of the Vega grammar\n    within Vincent.\n\n    This decorator is intended for classes that map to some pre-defined JSON\n    structure, such as axes, data, marks, scales, etc. It is assumed that this\n    decorates functions with an instance of ``self.grammar``.\n\n    Parameters\n    ----------\n    grammar_type : type or tuple of types, default None\n        If the argument to the decorated function is not of the given types,\n        then a ValueError is raised. No type checking is done if the type is\n        None (default).\n    grammar_name : string, default None\n        An optional name to map to the internal ``grammar`` dict. If None\n        (default), then the key for the dict is the name of the function\n        being decorated. If not None, then it will be the name specified\n        here. This is useful if the expected JSON field name is a Python\n        keyword or has an un-Pythonic name.\n\n    This should decorate a \"validator\" function that should return no value\n    but raise an exception if the provided value is not valid Vega grammar. If\n    the validator throws no exception, then the value is assigned to the\n    ``grammar`` dict.\n\n    The validator function should take only one argument - the value to be\n    validated - so that no ``self`` argument is included; the validator\n    should not modify the class.\n\n    If no arguments are given, then no type-checking is done the property\n    will be mapped to a field with the name of the decorated function.\n\n    The doc string for the property is taken from the validator functions's\n    doc string.",
    "generated": "Decorator to define properties that map to the ``grammar``\n    dict. This dict is the canonical representation of the Vega grammar\n    within Vincent.\n\n    This decorator is intended for classes that map to some pre-defined JSON\n    structure, such as axes, data, marks, etc. It is assumed that this\n    decorates functions with an instance of ``self.grammar``.\n\n    Parameters\n    ----------\n    grammar_type : type or tuple of types,"
  },
  {
    "code": "def validate(self):\n        \"\"\"Validate the contents of the object.\n\n        This calls ``setattr`` for each of the class's grammar properties. It\n        will catch ``ValueError``s raised by the grammar property's setters\n        and re-raise them as :class:`ValidationError`.\n        \"\"\"\n        for key, val in self.grammar.items():\n            try:\n                setattr(self, key, val)\n            except ValueError as e:\n                raise ValidationError('invalid contents: ' + e.args[0])",
    "reference": "Validate the contents of the object.\n\n        This calls ``setattr`` for each of the class's grammar properties. It\n        will catch ``ValueError``s raised by the grammar property's setters\n        and re-raise them as :class:`ValidationError`.",
    "generated": "Validate the contents of the object.\n\n        This calls ``setattr`` for each of the class's grammar properties. It\n        will catch ``ValueError``s raised by the grammar property's setters\n        and re-raise them as :class:`ValidationError`."
  },
  {
    "code": "def to_json(self, path=None, html_out=False,\n                html_path='vega_template.html', validate=False,\n                pretty_print=True):\n        \"\"\"Convert object to JSON\n\n        Parameters\n        ----------\n        path: string, default None\n            Path to write JSON out. If there is no path provided, JSON\n            will be returned as a string to the console.\n        html_out: boolean, default False\n            If True, vincent will output an simple HTML scaffold to\n            visualize the vega json output.\n        html_path: string, default 'vega_template.html'\n            Path for the html file (if html_out=True)\n        validate : boolean\n            If True, call the object's `validate` method before\n            serializing. Default is False.\n        pretty_print : boolean\n            If True (default), JSON is printed in more-readable form with\n            indentation and spaces.\n\n        Returns\n        -------\n        string\n            JSON serialization of the class's grammar properties.\n        \"\"\"\n        if validate:\n            self.validate()\n\n        if pretty_print:\n            dumps_args = {'indent': 2, 'separators': (',', ': ')}\n        else:\n            dumps_args = {}\n\n        def encoder(obj):\n            if hasattr(obj, 'grammar'):\n                return obj.grammar\n\n        if html_out:\n            template = Template(\n                str(resource_string('vincent', 'vega_template.html')))\n            with open(html_path, 'w') as f:\n                f.write(template.substitute(path=path))\n\n        if path:\n            with open(path, 'w') as f:\n                json.dump(self.grammar, f, default=encoder, sort_keys=True,\n                          **dumps_args)\n        else:\n            return json.dumps(self.grammar, default=encoder, sort_keys=True,\n                              **dumps_args)",
    "reference": "Convert object to JSON\n\n        Parameters\n        ----------\n        path: string, default None\n            Path to write JSON out. If there is no path provided, JSON\n            will be returned as a string to the console.\n        html_out: boolean, default False\n            If True, vincent will output an simple HTML scaffold to\n            visualize the vega json output.\n        html_path: string, default 'vega_template.html'\n            Path for the html file (if html_out=True)\n        validate : boolean\n            If True, call the object's `validate` method before\n            serializing. Default is False.\n        pretty_print : boolean\n            If True (default), JSON is printed in more-readable form with\n            indentation and spaces.\n\n        Returns\n        -------\n        string\n            JSON serialization of the class's grammar properties.",
    "generated": "Convert object to JSON\n\n        Parameters\n        ----------\n        path: string, default None\n            Path to write JSON out. If there is no path provided, JSON\n            will be returned as a string to the console.\n        html_out: boolean, default False\n            If True, vincent will output an simple HTML scaffold to\n            visualize the vega json output.\n        html_path: string, default 'vega_template.html'\n            Path for the html file (if html_out=True)"
  },
  {
    "code": "def useful_mimetype(text):\n    \"\"\"Check to see if the given mime type is a MIME type\n    which is useful in terms of how to treat this file.\n    \"\"\"\n    if text is None:\n        return False\n    mimetype = normalize_mimetype(text)\n    return mimetype not in [DEFAULT, PLAIN, None]",
    "reference": "Check to see if the given mime type is a MIME type\n    which is useful in terms of how to treat this file.",
    "generated": "Check to see if the given mime type is a MIME type\n    which is useful in terms of how to treat this file."
  },
  {
    "code": "def normalize_extension(extension):\n    \"\"\"Normalise a file name extension.\"\"\"\n    extension = decode_path(extension)\n    if extension is None:\n        return\n    if extension.startswith('.'):\n        extension = extension[1:]\n    if '.' in extension:\n        _, extension = os.path.splitext(extension)\n    extension = slugify(extension, sep='')\n    if extension is None:\n        return\n    if len(extension):\n        return extension",
    "reference": "Normalise a file name extension.",
    "generated": "Normalise a file name extension."
  },
  {
    "code": "def fetch(url: str, **kwargs) -> Selector:\n    \"\"\"\n    Send HTTP request and parse it as a DOM tree.\n\n    Args:\n        url (str): The url of the site.\n\n    Returns:\n        Selector: allows you to select parts of HTML text using CSS or XPath expressions.\n    \"\"\"\n    kwargs.setdefault('headers', DEFAULT_HEADERS)\n    try:\n        res = requests.get(url, **kwargs)\n        res.raise_for_status()\n    except requests.RequestException as e:\n        print(e)\n    else:\n        html = res.text\n        tree = Selector(text=html)\n        return tree",
    "reference": "Send HTTP request and parse it as a DOM tree.\n\n    Args:\n        url (str): The url of the site.\n\n    Returns:\n        Selector: allows you to select parts of HTML text using CSS or XPath expressions.",
    "generated": "Send HTTP request and parse it as a DOM tree."
  },
  {
    "code": "async def async_fetch(url: str, **kwargs) -> Selector:\n    \"\"\"\n    Do the fetch in an async style.\n\n    Args:\n        url (str): The url of the site.\n\n    Returns:\n        Selector: allows you to select parts of HTML text using CSS or XPath expressions.\n    \"\"\"\n    kwargs.setdefault('headers', DEFAULT_HEADERS)\n    async with aiohttp.ClientSession(**kwargs) as ses:\n        async with ses.get(url, **kwargs) as res:\n            html = await res.text()\n            tree = Selector(text=html)\n            return tree",
    "reference": "Do the fetch in an async style.\n\n    Args:\n        url (str): The url of the site.\n\n    Returns:\n        Selector: allows you to select parts of HTML text using CSS or XPath expressions.",
    "generated": "Do the fetch in an async style.\n\n    Args:\n        url (str): The url of the site.\n\n    Returns:\n        Selector: allows you to select parts of HTML text using CSS or XPath expressions."
  },
  {
    "code": "def links(res: requests.models.Response,\n          search: str = None,\n          pattern: str = None) -> list:\n    \"\"\"Get the links of the page.\n\n    Args:\n        res (requests.models.Response): The response of the page.\n        search (str, optional): Defaults to None. Search the links you want.\n        pattern (str, optional): Defaults to None. Search the links use a regex pattern.\n\n    Returns:\n        list: All the links of the page.\n    \"\"\"\n    hrefs = [link.to_text() for link in find_all_links(res.text)]\n    if search:\n        hrefs = [href for href in hrefs if search in href]\n    if pattern:\n        hrefs = [href for href in hrefs if re.findall(pattern, href)]\n    return list(set(hrefs))",
    "reference": "Get the links of the page.\n\n    Args:\n        res (requests.models.Response): The response of the page.\n        search (str, optional): Defaults to None. Search the links you want.\n        pattern (str, optional): Defaults to None. Search the links use a regex pattern.\n\n    Returns:\n        list: All the links of the page.",
    "generated": "Get the links of the page.\n\n    Args:\n        res (requests.models.Response): The response of the page.\n        search (str, optional): Defaults to None. Search the links you want.\n        pattern (str, optional): Defaults to None. Search the links use a regex pattern.\n\n    Returns:\n        list: All the links of the page."
  },
  {
    "code": "def save_as_json(total: list,\n                 name='data.json',\n                 sort_by: str = None,\n                 no_duplicate=False,\n                 order='asc'):\n    \"\"\"Save what you crawled as a json file.\n\n    Args:\n        total (list): Total of data you crawled.\n        name (str, optional): Defaults to 'data.json'. The name of the file.\n        sort_by (str, optional): Defaults to None. Sort items by a specific key.\n        no_duplicate (bool, optional): Defaults to False. If True, it will remove duplicated data.\n        order (str, optional): Defaults to 'asc'. The opposite option is 'desc'.\n    \"\"\"\n    if sort_by:\n        reverse = order == 'desc'\n        total = sorted(total, key=itemgetter(sort_by), reverse=reverse)\n    if no_duplicate:\n        total = [key for key, _ in groupby(total)]\n    data = json.dumps(total, ensure_ascii=False)\n    Path(name).write_text(data, encoding='utf-8')",
    "reference": "Save what you crawled as a json file.\n\n    Args:\n        total (list): Total of data you crawled.\n        name (str, optional): Defaults to 'data.json'. The name of the file.\n        sort_by (str, optional): Defaults to None. Sort items by a specific key.\n        no_duplicate (bool, optional): Defaults to False. If True, it will remove duplicated data.\n        order (str, optional): Defaults to 'asc'. The opposite option is 'desc'.",
    "generated": "Save what you crawled as a json file.\n\n    Args:\n        total (list): Total of data you crawled.\n        name (str, optional): Defaults to 'data.json'. The name of the file.\n        sort_by (str, optional): Defaults to None. Sort items by a specific key.\n        no_duplicate (bool, optional): Defaults to False. If True, it will remove duplicated data.\n        order (str, optional): Defaults to 'asc'. The opposite option is 'desc'."
  },
  {
    "code": "def set_observer(self, observer):\n        \"\"\"\n        Validates and sets the color's observer angle.\n\n        .. note:: This only changes the observer angle value. It does no conversion\n            of the color's coordinates.\n\n        :param str observer: One of '2' or '10'.\n        \"\"\"\n        observer = str(observer)\n        if observer not in color_constants.OBSERVERS:\n            raise InvalidObserverError(self)\n        self.observer = observer",
    "reference": "Validates and sets the color's observer angle.\n\n        .. note:: This only changes the observer angle value. It does no conversion\n            of the color's coordinates.\n\n        :param str observer: One of '2' or '10'.",
    "generated": "Sets the color's observer angle.\n\n        .. note:: This only changes the observer angle value."
  },
  {
    "code": "def set_illuminant(self, illuminant):\n        \"\"\"\n        Validates and sets the color's illuminant.\n\n        .. note:: This only changes the illuminant. It does no conversion\n            of the color's coordinates. For this, you'll want to refer to\n            :py:meth:`XYZColor.apply_adaptation <colormath.color_objects.XYZColor.apply_adaptation>`.\n\n        .. tip:: Call this after setting your observer.\n\n        :param str illuminant: One of the various illuminants.\n        \"\"\"\n        illuminant = illuminant.lower()\n        if illuminant not in color_constants.ILLUMINANTS[self.observer]:\n            raise InvalidIlluminantError(illuminant)\n        self.illuminant = illuminant",
    "reference": "Validates and sets the color's illuminant.\n\n        .. note:: This only changes the illuminant. It does no conversion\n            of the color's coordinates. For this, you'll want to refer to\n            :py:meth:`XYZColor.apply_adaptation <colormath.color_objects.XYZColor.apply_adaptation>`.\n\n        .. tip:: Call this after setting your observer.\n\n        :param str illuminant: One of the various illuminants.",
    "generated": "Sets the color's illuminant.\n\n        .. note:: This only changes the illuminant. It does no conversion\n            of the color's coordinates. For this, you'll want to refer to\n            :py:meth:`XYZColor.apply_adaptation <colormath.color_objects.XYZColor.apply_adaptation>`.\n\n        .. tip:: Call this after setting your observer.\n\n        :param str illuminant: One of the various illuminants."
  },
  {
    "code": "def get_numpy_array(self):\n        \"\"\"\n        Dump this color into NumPy array.\n        \"\"\"\n        # This holds the obect's spectral data, and will be passed to\n        # numpy.array() to create a numpy array (matrix) for the matrix math\n        # that will be done during the conversion to XYZ.\n        values = []\n\n        # Use the required value list to build this dynamically. Default to\n        # 0.0, since that ultimately won't affect the outcome due to the math\n        # involved.\n        for val in self.VALUES:\n            values.append(getattr(self, val, 0.0))\n\n        # Create and the actual numpy array/matrix from the spectral list.\n        color_array = numpy.array([values])\n        return color_array",
    "reference": "Dump this color into NumPy array.",
    "generated": "Dump this color into NumPy array."
  },
  {
    "code": "def apply_adaptation(self, target_illuminant, adaptation='bradford'):\n        \"\"\"\n        This applies an adaptation matrix to change the XYZ color's illuminant.\n        You'll most likely only need this during RGB conversions.\n        \"\"\"\n        logger.debug(\"  \\- Original illuminant: %s\", self.illuminant)\n        logger.debug(\"  \\- Target illuminant: %s\", target_illuminant)\n\n        # If the XYZ values were taken with a different reference white than the\n        # native reference white of the target RGB space, a transformation matrix\n        # must be applied.\n        if self.illuminant != target_illuminant:\n            logger.debug(\"  \\* Applying transformation from %s to %s \",\n                         self.illuminant, target_illuminant)\n            # Sets the adjusted XYZ values, and the new illuminant.\n            apply_chromatic_adaptation_on_color(\n                color=self,\n                targ_illum=target_illuminant,\n                adaptation=adaptation)",
    "reference": "This applies an adaptation matrix to change the XYZ color's illuminant.\n        You'll most likely only need this during RGB conversions.",
    "generated": "This applies an adaptation matrix to change the XYZ color's illuminant.\n        You'll most likely only need this during RGB conversions."
  },
  {
    "code": "def _clamp_rgb_coordinate(self, coord):\n        \"\"\"\n        Clamps an RGB coordinate, taking into account whether or not the\n        color is upscaled or not.\n\n        :param float coord: The coordinate value.\n        :rtype: float\n        :returns: The clamped value.\n        \"\"\"\n        if not self.is_upscaled:\n            return min(max(coord, 0.0), 1.0)\n        else:\n            return min(max(coord, 1), 255)",
    "reference": "Clamps an RGB coordinate, taking into account whether or not the\n        color is upscaled or not.\n\n        :param float coord: The coordinate value.\n        :rtype: float\n        :returns: The clamped value.",
    "generated": "Clamps an RGB coordinate, taking into account whether or not the\n        color is upscaled or not."
  },
  {
    "code": "def get_upscaled_value_tuple(self):\n        \"\"\"\n        Scales an RGB color object from decimal 0.0-1.0 to int 0-255.\n        \"\"\"\n        # Scale up to 0-255 values.\n        rgb_r = int(math.floor(0.5 + self.rgb_r * 255))\n        rgb_g = int(math.floor(0.5 + self.rgb_g * 255))\n        rgb_b = int(math.floor(0.5 + self.rgb_b * 255))\n\n        return rgb_r, rgb_g, rgb_b",
    "reference": "Scales an RGB color object from decimal 0.0-1.0 to int 0-255.",
    "generated": "Scale an RGB color object from decimal 0.0-1.0 to int 0-255."
  },
  {
    "code": "def auto_density(color):\n    \"\"\"\n    Given a SpectralColor, automatically choose the correct ANSI T filter.\n    Returns a tuple with a string representation of the filter the\n    calculated density.\n\n    :param SpectralColor color: The SpectralColor object to calculate\n        density for.\n    :rtype: float\n    :returns: The density value, with the filter selected automatically.\n    \"\"\"\n    blue_density = ansi_density(color, ANSI_STATUS_T_BLUE)\n    green_density = ansi_density(color, ANSI_STATUS_T_GREEN)\n    red_density = ansi_density(color, ANSI_STATUS_T_RED)\n\n    densities = [blue_density, green_density, red_density]\n    min_density = min(densities)\n    max_density = max(densities)\n    density_range = max_density - min_density\n\n    # See comments in density_standards.py for VISUAL_DENSITY_THRESH to\n    # understand what this is doing.\n    if density_range <= VISUAL_DENSITY_THRESH:\n        return ansi_density(color, ISO_VISUAL)\n    elif blue_density > green_density and blue_density > red_density:\n        return blue_density\n    elif green_density > blue_density and green_density > red_density:\n        return green_density\n    else:\n        return red_density",
    "reference": "Given a SpectralColor, automatically choose the correct ANSI T filter.\n    Returns a tuple with a string representation of the filter the\n    calculated density.\n\n    :param SpectralColor color: The SpectralColor object to calculate\n        density for.\n    :rtype: float\n    :returns: The density value, with the filter selected automatically.",
    "generated": "Given a SpectralColor, automatically choose the correct ANSI T filter.\n    Returns a tuple with a string representation of the filter the\n    calculated density."
  },
  {
    "code": "def _get_lab_color1_vector(color):\n    \"\"\"\n    Converts an LabColor into a NumPy vector.\n\n    :param LabColor color:\n    :rtype: numpy.ndarray\n    \"\"\"\n    if not color.__class__.__name__ == 'LabColor':\n        raise ValueError(\n            \"Delta E functions can only be used with two LabColor objects.\")\n    return numpy.array([color.lab_l, color.lab_a, color.lab_b])",
    "reference": "Converts an LabColor into a NumPy vector.\n\n    :param LabColor color:\n    :rtype: numpy.ndarray",
    "generated": "Converts an LabColor into a NumPy vector."
  },
  {
    "code": "def _get_adaptation_matrix(wp_src, wp_dst, observer, adaptation):\n    \"\"\"\n    Calculate the correct transformation matrix based on origin and target\n    illuminants. The observer angle must be the same between illuminants.\n\n    See colormath.color_constants.ADAPTATION_MATRICES for a list of possible\n    adaptations.\n\n    Detailed conversion documentation is available at:\n    http://brucelindbloom.com/Eqn_ChromAdapt.html\n    \"\"\"\n    # Get the appropriate transformation matrix, [MsubA].\n    m_sharp = color_constants.ADAPTATION_MATRICES[adaptation]\n\n    # In case the white-points are still input as strings\n    # Get white-points for illuminant\n    if isinstance(wp_src, str):\n        orig_illum = wp_src.lower()\n        wp_src = color_constants.ILLUMINANTS[observer][orig_illum]\n    elif hasattr(wp_src, '__iter__'):\n        wp_src = wp_src\n\n    if isinstance(wp_dst, str):\n        targ_illum = wp_dst.lower()\n        wp_dst = color_constants.ILLUMINANTS[observer][targ_illum]\n    elif hasattr(wp_dst, '__iter__'):\n        wp_dst = wp_dst\n\n    # Sharpened cone responses ~ rho gamma beta ~ sharpened r g b\n    rgb_src = numpy.dot(m_sharp, wp_src)\n    rgb_dst = numpy.dot(m_sharp, wp_dst)\n\n    # Ratio of whitepoint sharpened responses\n    m_rat = numpy.diag(rgb_dst / rgb_src)\n\n    # Final transformation matrix\n    m_xfm = numpy.dot(numpy.dot(pinv(m_sharp), m_rat), m_sharp)\n\n    return m_xfm",
    "reference": "Calculate the correct transformation matrix based on origin and target\n    illuminants. The observer angle must be the same between illuminants.\n\n    See colormath.color_constants.ADAPTATION_MATRICES for a list of possible\n    adaptations.\n\n    Detailed conversion documentation is available at:\n    http://brucelindbloom.com/Eqn_ChromAdapt.html",
    "generated": "Calculate the correct transformation matrix based on origin and target\n    illuminants. The observer angle must be the same between illuminants.\n\n    See colormath.color_constants.ADAPTATION_MATRICES for a list of possible\n    adaptations.\n\n    Detailed conversion documentation is available at:\n    http://brucelindbloom.com/Eqn_ChromAdapt.html"
  },
  {
    "code": "def apply_chromatic_adaptation(val_x, val_y, val_z, orig_illum, targ_illum,\n                               observer='2', adaptation='bradford'):\n    \"\"\"\n    Applies a chromatic adaptation matrix to convert XYZ values between\n    illuminants. It is important to recognize that color transformation results\n    in color errors, determined by how far the original illuminant is from the\n    target illuminant. For example, D65 to A could result in very high maximum\n    deviance.\n\n    An informative article with estimate average Delta E values for each\n    illuminant conversion may be found at:\n\n    http://brucelindbloom.com/ChromAdaptEval.html\n    \"\"\"\n    # It's silly to have to do this, but some people may want to call this\n    # function directly, so we'll protect them from messing up upper/lower case.\n    adaptation = adaptation.lower()\n\n    # Get white-points for illuminant\n    if isinstance(orig_illum, str):\n        orig_illum = orig_illum.lower()\n        wp_src = color_constants.ILLUMINANTS[observer][orig_illum]\n    elif hasattr(orig_illum, '__iter__'):\n        wp_src = orig_illum\n\n    if isinstance(targ_illum, str):\n        targ_illum = targ_illum.lower()\n        wp_dst = color_constants.ILLUMINANTS[observer][targ_illum]\n    elif hasattr(targ_illum, '__iter__'):\n        wp_dst = targ_illum\n\n    logger.debug(\"  \\* Applying adaptation matrix: %s\", adaptation)\n    # Retrieve the appropriate transformation matrix from the constants.\n    transform_matrix = _get_adaptation_matrix(wp_src, wp_dst,\n                                              observer, adaptation)\n\n    # Stuff the XYZ values into a NumPy matrix for conversion.\n    XYZ_matrix = numpy.array((val_x, val_y, val_z))\n    # Perform the adaptation via matrix multiplication.\n    result_matrix = numpy.dot(transform_matrix, XYZ_matrix)\n\n    # Return individual X, Y, and Z coordinates.\n    return result_matrix[0], result_matrix[1], result_matrix[2]",
    "reference": "Applies a chromatic adaptation matrix to convert XYZ values between\n    illuminants. It is important to recognize that color transformation results\n    in color errors, determined by how far the original illuminant is from the\n    target illuminant. For example, D65 to A could result in very high maximum\n    deviance.\n\n    An informative article with estimate average Delta E values for each\n    illuminant conversion may be found at:\n\n    http://brucelindbloom.com/ChromAdaptEval.html",
    "generated": "Applies a chromatic adaptation matrix to convert XYZ values between\n    illuminants. It is important to recognize that color transformation results\n    in color errors, determined by how far the original illuminant is from the\n    target illuminant. For example, D65 to A could result in very high maximum\n    deviance.\n\n    An informative article with estimate average Delta E values for each\n    illuminant conversion may be found at:\n\n    http://brucelindbloom.com/ChromAdaptEval.html"
  },
  {
    "code": "def apply_chromatic_adaptation_on_color(color, targ_illum, adaptation='bradford'):\n    \"\"\"\n    Convenience function to apply an adaptation directly to a Color object.\n    \"\"\"\n    xyz_x = color.xyz_x\n    xyz_y = color.xyz_y\n    xyz_z = color.xyz_z\n    orig_illum = color.illuminant\n    targ_illum = targ_illum.lower()\n    observer = color.observer\n    adaptation = adaptation.lower()\n\n    # Return individual X, Y, and Z coordinates.\n    color.xyz_x, color.xyz_y, color.xyz_z = apply_chromatic_adaptation(\n        xyz_x, xyz_y, xyz_z, orig_illum, targ_illum,\n        observer=observer, adaptation=adaptation)\n    color.set_illuminant(targ_illum)\n\n    return color",
    "reference": "Convenience function to apply an adaptation directly to a Color object.",
    "generated": "Apply an adaptation directly to a Color object."
  },
  {
    "code": "def example_lab_to_xyz():\n    \"\"\"\n    This function shows a simple conversion of an Lab color to an XYZ color.\n    \"\"\"\n\n    print(\"=== Simple Example: Lab->XYZ ===\")\n    # Instantiate an Lab color object with the given values.\n    lab = LabColor(0.903, 16.296, -2.22)\n    # Show a string representation.\n    print(lab)\n    # Convert to XYZ.\n    xyz = convert_color(lab, XYZColor)\n    print(xyz)\n    print(\"=== End Example ===\\n\")",
    "reference": "This function shows a simple conversion of an Lab color to an XYZ color.",
    "generated": "This function shows a simple conversion of an Lab color to an XYZ color."
  },
  {
    "code": "def example_lchab_to_lchuv():\n    \"\"\"\n    This function shows very complex chain of conversions in action.\n\n    LCHab to LCHuv involves four different calculations, making this the\n    conversion requiring the most steps.\n    \"\"\"\n\n    print(\"=== Complex Example: LCHab->LCHuv ===\")\n    # Instantiate an LCHab color object with the given values.\n    lchab = LCHabColor(0.903, 16.447, 352.252)\n    # Show a string representation.\n    print(lchab)\n    # Convert to LCHuv.\n    lchuv = convert_color(lchab, LCHuvColor)\n    print(lchuv)\n    print(\"=== End Example ===\\n\")",
    "reference": "This function shows very complex chain of conversions in action.\n\n    LCHab to LCHuv involves four different calculations, making this the\n    conversion requiring the most steps.",
    "generated": "This function shows very complex chain of conversions in action.\n\n    LCHab to LCHuv involves four different calculations, making this the\n    conversion requiring the most steps."
  },
  {
    "code": "def example_lab_to_rgb():\n    \"\"\"\n    Conversions to RGB are a little more complex mathematically. There are also\n    several kinds of RGB color spaces. When converting from a device-independent\n    color space to RGB, sRGB is assumed unless otherwise specified with the\n    target_rgb keyword arg.\n    \"\"\"\n\n    print(\"=== RGB Example: Lab->RGB ===\")\n    # Instantiate an Lab color object with the given values.\n    lab = LabColor(0.903, 16.296, -2.217)\n    # Show a string representation.\n    print(lab)\n    # Convert to XYZ.\n    rgb = convert_color(lab, sRGBColor)\n    print(rgb)\n    print(\"=== End Example ===\\n\")",
    "reference": "Conversions to RGB are a little more complex mathematically. There are also\n    several kinds of RGB color spaces. When converting from a device-independent\n    color space to RGB, sRGB is assumed unless otherwise specified with the\n    target_rgb keyword arg.",
    "generated": "Converts from a device-independent\n    color space to RGB, sRGB is assumed unless otherwise specified with the\n    target_rgb keyword arg."
  },
  {
    "code": "def example_rgb_to_xyz():\n    \"\"\"\n    The reverse is similar.\n    \"\"\"\n\n    print(\"=== RGB Example: RGB->XYZ ===\")\n    # Instantiate an Lab color object with the given values.\n    rgb = sRGBColor(120, 130, 140)\n    # Show a string representation.\n    print(rgb)\n    # Convert RGB to XYZ using a D50 illuminant.\n    xyz = convert_color(rgb, XYZColor, target_illuminant='D50')\n    print(xyz)\n    print(\"=== End Example ===\\n\")",
    "reference": "The reverse is similar.",
    "generated": "The reverse is similar."
  },
  {
    "code": "def example_spectral_to_xyz():\n    \"\"\"\n    Instantiate an Lab color object with the given values. Note that the\n    spectral range can run from 340nm to 830nm. Any omitted values assume a\n    value of 0.0, which is more or less ignored. For the distribution below,\n    we are providing an example reading from an X-Rite i1 Pro, which only\n    measures between 380nm and 730nm.\n    \"\"\"\n\n    print(\"=== Example: Spectral->XYZ ===\")\n    spc = SpectralColor(\n        observer='2', illuminant='d50',\n        spec_380nm=0.0600, spec_390nm=0.0600, spec_400nm=0.0641,\n        spec_410nm=0.0654, spec_420nm=0.0645, spec_430nm=0.0605,\n        spec_440nm=0.0562, spec_450nm=0.0543, spec_460nm=0.0537,\n        spec_470nm=0.0541, spec_480nm=0.0559, spec_490nm=0.0603,\n        spec_500nm=0.0651, spec_510nm=0.0680, spec_520nm=0.0705,\n        spec_530nm=0.0736, spec_540nm=0.0772, spec_550nm=0.0809,\n        spec_560nm=0.0870, spec_570nm=0.0990, spec_580nm=0.1128,\n        spec_590nm=0.1251, spec_600nm=0.1360, spec_610nm=0.1439,\n        spec_620nm=0.1511, spec_630nm=0.1590, spec_640nm=0.1688,\n        spec_650nm=0.1828, spec_660nm=0.1996, spec_670nm=0.2187,\n        spec_680nm=0.2397, spec_690nm=0.2618, spec_700nm=0.2852,\n        spec_710nm=0.2500, spec_720nm=0.2400, spec_730nm=0.2300)\n    xyz = convert_color(spc, XYZColor)\n    print(xyz)\n    print(\"=== End Example ===\\n\")",
    "reference": "Instantiate an Lab color object with the given values. Note that the\n    spectral range can run from 340nm to 830nm. Any omitted values assume a\n    value of 0.0, which is more or less ignored. For the distribution below,\n    we are providing an example reading from an X-Rite i1 Pro, which only\n    measures between 380nm and 730nm.",
    "generated": "Instantiate an Lab color object with the given values. Note that the\n    spectral range can run from 340nm to 830nm. Any omitted values assume a\n    value of 0.0, which is more or less ignored. For the distribution below,\n    we are providing an example reading from an X-Rite i1 Pro, which only\n    measures between 380nm and 730nm."
  },
  {
    "code": "def example_lab_to_ipt():\n    \"\"\"\n    This function shows a simple conversion of an XYZ color to an IPT color.\n    \"\"\"\n\n    print(\"=== Simple Example: XYZ->IPT ===\")\n    # Instantiate an XYZ color object with the given values.\n    xyz = XYZColor(0.5, 0.5, 0.5, illuminant='d65')\n    # Show a string representation.\n    print(xyz)\n    # Convert to IPT.\n    ipt = convert_color(xyz, IPTColor)\n    print(ipt)\n    print(\"=== End Example ===\\n\")",
    "reference": "This function shows a simple conversion of an XYZ color to an IPT color.",
    "generated": "This function shows a simple conversion of an XYZ color to an IPT color."
  },
  {
    "code": "def apply_RGB_matrix(var1, var2, var3, rgb_type, convtype=\"xyz_to_rgb\"):\n    \"\"\"\n    Applies an RGB working matrix to convert from XYZ to RGB.\n    The arguments are tersely named var1, var2, and var3 to allow for the\n    passing of XYZ _or_ RGB values. var1 is X for XYZ, and R for RGB. var2 and\n    var3 follow suite.\n    \"\"\"\n    convtype = convtype.lower()\n    # Retrieve the appropriate transformation matrix from the constants.\n    rgb_matrix = rgb_type.conversion_matrices[convtype]\n\n    logger.debug(\"  \\* Applying RGB conversion matrix: %s->%s\",\n                 rgb_type.__class__.__name__, convtype)\n    # Stuff the RGB/XYZ values into a NumPy matrix for conversion.\n    var_matrix = numpy.array((\n        var1, var2, var3\n    ))\n    # Perform the adaptation via matrix multiplication.\n    result_matrix = numpy.dot(rgb_matrix, var_matrix)\n    rgb_r, rgb_g, rgb_b = result_matrix\n    # Clamp these values to a valid range.\n    rgb_r = max(rgb_r, 0.0)\n    rgb_g = max(rgb_g, 0.0)\n    rgb_b = max(rgb_b, 0.0)\n    return rgb_r, rgb_g, rgb_b",
    "reference": "Applies an RGB working matrix to convert from XYZ to RGB.\n    The arguments are tersely named var1, var2, and var3 to allow for the\n    passing of XYZ _or_ RGB values. var1 is X for XYZ, and R for RGB. var2 and\n    var3 follow suite.",
    "generated": "Applies an RGB working matrix to convert from XYZ to RGB.\n    The arguments are tersely named var1, var2, and var3 to allow for the\n    passing of XYZ _or_ RGB values. var1 is X for XYZ, and R for RGB. var2 and\n    var3 follow suite."
  },
  {
    "code": "def color_conversion_function(start_type, target_type):\n    \"\"\"\n    Decorator to indicate a function that performs a conversion from one color\n    space to another.\n\n    This decorator will return the original function unmodified, however it will\n    be registered in the _conversion_manager so it can be used to perform color\n    space transformations between color spaces that do not have direct\n    conversion functions (e.g., Luv to CMYK).\n\n    Note: For a conversion to/from RGB supply the BaseRGBColor class.\n\n    :param start_type: Starting color space type\n    :param target_type: Target color space type\n    \"\"\"\n    def decorator(f):\n        f.start_type = start_type\n        f.target_type = target_type\n        _conversion_manager.add_type_conversion(start_type, target_type, f)\n        return f\n\n    return decorator",
    "reference": "Decorator to indicate a function that performs a conversion from one color\n    space to another.\n\n    This decorator will return the original function unmodified, however it will\n    be registered in the _conversion_manager so it can be used to perform color\n    space transformations between color spaces that do not have direct\n    conversion functions (e.g., Luv to CMYK).\n\n    Note: For a conversion to/from RGB supply the BaseRGBColor class.\n\n    :param start_type: Starting color space type\n    :param target_type: Target color space type",
    "generated": "Decororator to indicate a function that performs a conversion from one color\n    space to another.\n\n    This decorator will return the original function unmodified, however it will\n    be registered in the _conversion_manager so it can be used to perform color\n    space transformations between color spaces that do not have direct\n    conversion functions (e.g., Luv to CMYK)."
  },
  {
    "code": "def Spectral_to_XYZ(cobj, illuminant_override=None, *args, **kwargs):\n    \"\"\"\n    Converts spectral readings to XYZ.\n    \"\"\"\n    # If the user provides an illuminant_override numpy array, use it.\n    if illuminant_override:\n        reference_illum = illuminant_override\n    else:\n        # Otherwise, look up the illuminant from known standards based\n        # on the value of 'illuminant' pulled from the SpectralColor object.\n        try:\n            reference_illum = spectral_constants.REF_ILLUM_TABLE[cobj.illuminant]\n        except KeyError:\n            raise InvalidIlluminantError(cobj.illuminant)\n\n    # Get the spectral distribution of the selected standard observer.\n    if cobj.observer == '10':\n        std_obs_x = spectral_constants.STDOBSERV_X10\n        std_obs_y = spectral_constants.STDOBSERV_Y10\n        std_obs_z = spectral_constants.STDOBSERV_Z10\n    else:\n        # Assume 2 degree, since it is theoretically the only other possibility.\n        std_obs_x = spectral_constants.STDOBSERV_X2\n        std_obs_y = spectral_constants.STDOBSERV_Y2\n        std_obs_z = spectral_constants.STDOBSERV_Z2\n\n    # This is a NumPy array containing the spectral distribution of the color.\n    sample = cobj.get_numpy_array()\n\n    # The denominator is constant throughout the entire calculation for X,\n    # Y, and Z coordinates. Calculate it once and re-use.\n    denom = std_obs_y * reference_illum\n\n    # This is also a common element in the calculation whereby the sample\n    # NumPy array is multiplied by the reference illuminant's power distribution\n    # (which is also a NumPy array).\n    sample_by_ref_illum = sample * reference_illum\n\n    # Calculate the numerator of the equation to find X.\n    x_numerator = sample_by_ref_illum * std_obs_x\n    y_numerator = sample_by_ref_illum * std_obs_y\n    z_numerator = sample_by_ref_illum * std_obs_z\n\n    xyz_x = x_numerator.sum() / denom.sum()\n    xyz_y = y_numerator.sum() / denom.sum()\n    xyz_z = z_numerator.sum() / denom.sum()\n\n    return XYZColor(\n        xyz_x, xyz_y, xyz_z, observer=cobj.observer, illuminant=cobj.illuminant)",
    "reference": "Converts spectral readings to XYZ.",
    "generated": "Converts spectral readings to XYZ."
  },
  {
    "code": "def Lab_to_XYZ(cobj, *args, **kwargs):\n    \"\"\"\n    Convert from Lab to XYZ\n    \"\"\"\n    illum = cobj.get_illuminant_xyz()\n    xyz_y = (cobj.lab_l + 16.0) / 116.0\n    xyz_x = cobj.lab_a / 500.0 + xyz_y\n    xyz_z = xyz_y - cobj.lab_b / 200.0\n\n    if math.pow(xyz_y, 3) > color_constants.CIE_E:\n        xyz_y = math.pow(xyz_y, 3)\n    else:\n        xyz_y = (xyz_y - 16.0 / 116.0) / 7.787\n\n    if math.pow(xyz_x, 3) > color_constants.CIE_E:\n        xyz_x = math.pow(xyz_x, 3)\n    else:\n        xyz_x = (xyz_x - 16.0 / 116.0) / 7.787\n\n    if math.pow(xyz_z, 3) > color_constants.CIE_E:\n        xyz_z = math.pow(xyz_z, 3)\n    else:\n        xyz_z = (xyz_z - 16.0 / 116.0) / 7.787\n\n    xyz_x = (illum[\"X\"] * xyz_x)\n    xyz_y = (illum[\"Y\"] * xyz_y)\n    xyz_z = (illum[\"Z\"] * xyz_z)\n\n    return XYZColor(\n        xyz_x, xyz_y, xyz_z, observer=cobj.observer, illuminant=cobj.illuminant)",
    "reference": "Convert from Lab to XYZ",
    "generated": "Convert from Lab to XYZ\n   "
  },
  {
    "code": "def Luv_to_XYZ(cobj, *args, **kwargs):\n    \"\"\"\n    Convert from Luv to XYZ.\n    \"\"\"\n    illum = cobj.get_illuminant_xyz()\n    # Without Light, there is no color. Short-circuit this and avoid some\n    # zero division errors in the var_a_frac calculation.\n    if cobj.luv_l <= 0.0:\n        xyz_x = 0.0\n        xyz_y = 0.0\n        xyz_z = 0.0\n        return XYZColor(\n            xyz_x, xyz_y, xyz_z,\n            observer=cobj.observer, illuminant=cobj.illuminant)\n\n    # Various variables used throughout the conversion.\n    cie_k_times_e = color_constants.CIE_K * color_constants.CIE_E\n    u_sub_0 = (4.0 * illum[\"X\"]) / (illum[\"X\"] + 15.0 * illum[\"Y\"] + 3.0 * illum[\"Z\"])\n    v_sub_0 = (9.0 * illum[\"Y\"]) / (illum[\"X\"] + 15.0 * illum[\"Y\"] + 3.0 * illum[\"Z\"])\n    var_u = cobj.luv_u / (13.0 * cobj.luv_l) + u_sub_0\n    var_v = cobj.luv_v / (13.0 * cobj.luv_l) + v_sub_0\n\n    # Y-coordinate calculations.\n    if cobj.luv_l > cie_k_times_e:\n        xyz_y = math.pow((cobj.luv_l + 16.0) / 116.0, 3.0)\n    else:\n        xyz_y = cobj.luv_l / color_constants.CIE_K\n\n    # X-coordinate calculation.\n    xyz_x = xyz_y * 9.0 * var_u / (4.0 * var_v)\n    # Z-coordinate calculation.\n    xyz_z = xyz_y * (12.0 - 3.0 * var_u - 20.0 * var_v) / (4.0 * var_v)\n\n    return XYZColor(\n        xyz_x, xyz_y, xyz_z, illuminant=cobj.illuminant, observer=cobj.observer)",
    "reference": "Convert from Luv to XYZ.",
    "generated": "Convert from Luv to XYZ."
  },
  {
    "code": "def xyY_to_XYZ(cobj, *args, **kwargs):\n    \"\"\"\n    Convert from xyY to XYZ.\n    \"\"\"\n    # avoid division by zero\n    if cobj.xyy_y == 0.0:\n        xyz_x = 0.0\n        xyz_y = 0.0\n        xyz_z = 0.0\n    else:\n        xyz_x = (cobj.xyy_x * cobj.xyy_Y) / cobj.xyy_y\n        xyz_y = cobj.xyy_Y\n        xyz_z = ((1.0 - cobj.xyy_x - cobj.xyy_y) * xyz_y) / cobj.xyy_y\n\n    return XYZColor(\n        xyz_x, xyz_y, xyz_z, illuminant=cobj.illuminant, observer=cobj.observer)",
    "reference": "Convert from xyY to XYZ.",
    "generated": "Convert from xyY to XYZ."
  },
  {
    "code": "def XYZ_to_xyY(cobj, *args, **kwargs):\n    \"\"\"\n    Convert from XYZ to xyY.\n    \"\"\"\n    xyz_sum = cobj.xyz_x + cobj.xyz_y + cobj.xyz_z\n    # avoid division by zero\n    if xyz_sum == 0.0:\n        xyy_x = 0.0\n        xyy_y = 0.0\n    else:\n        xyy_x = cobj.xyz_x / xyz_sum\n        xyy_y = cobj.xyz_y / xyz_sum\n    xyy_Y = cobj.xyz_y\n\n    return xyYColor(\n        xyy_x, xyy_y, xyy_Y, observer=cobj.observer, illuminant=cobj.illuminant)",
    "reference": "Convert from XYZ to xyY.",
    "generated": "Convert from XYZ to xyY."
  },
  {
    "code": "def XYZ_to_Luv(cobj, *args, **kwargs):\n    \"\"\"\n    Convert from XYZ to Luv\n    \"\"\"\n    temp_x = cobj.xyz_x\n    temp_y = cobj.xyz_y\n    temp_z = cobj.xyz_z\n    denom = temp_x + (15.0 * temp_y) + (3.0 * temp_z)\n    # avoid division by zero\n    if denom == 0.0:\n        luv_u = 0.0\n        luv_v = 0.0\n    else:\n        luv_u = (4.0 * temp_x) / denom\n        luv_v = (9.0 * temp_y) / denom\n\n    illum = cobj.get_illuminant_xyz()\n    temp_y = temp_y / illum[\"Y\"]\n    if temp_y > color_constants.CIE_E:\n        temp_y = math.pow(temp_y, (1.0 / 3.0))\n    else:\n        temp_y = (7.787 * temp_y) + (16.0 / 116.0)\n\n    ref_U = (4.0 * illum[\"X\"]) / (illum[\"X\"] + (15.0 * illum[\"Y\"]) + (3.0 * illum[\"Z\"]))\n    ref_V = (9.0 * illum[\"Y\"]) / (illum[\"X\"] + (15.0 * illum[\"Y\"]) + (3.0 * illum[\"Z\"]))\n\n    luv_l = (116.0 * temp_y) - 16.0\n    luv_u = 13.0 * luv_l * (luv_u - ref_U)\n    luv_v = 13.0 * luv_l * (luv_v - ref_V)\n\n    return LuvColor(\n        luv_l, luv_u, luv_v, observer=cobj.observer, illuminant=cobj.illuminant)",
    "reference": "Convert from XYZ to Luv",
    "generated": "Convert from XYZ to Luv"
  },
  {
    "code": "def XYZ_to_Lab(cobj, *args, **kwargs):\n    \"\"\"\n    Converts XYZ to Lab.\n    \"\"\"\n    illum = cobj.get_illuminant_xyz()\n    temp_x = cobj.xyz_x / illum[\"X\"]\n    temp_y = cobj.xyz_y / illum[\"Y\"]\n    temp_z = cobj.xyz_z / illum[\"Z\"]\n\n    if temp_x > color_constants.CIE_E:\n        temp_x = math.pow(temp_x, (1.0 / 3.0))\n    else:\n        temp_x = (7.787 * temp_x) + (16.0 / 116.0)\n\n    if temp_y > color_constants.CIE_E:\n        temp_y = math.pow(temp_y, (1.0 / 3.0))\n    else:\n        temp_y = (7.787 * temp_y) + (16.0 / 116.0)\n\n    if temp_z > color_constants.CIE_E:\n        temp_z = math.pow(temp_z, (1.0 / 3.0))\n    else:\n        temp_z = (7.787 * temp_z) + (16.0 / 116.0)\n\n    lab_l = (116.0 * temp_y) - 16.0\n    lab_a = 500.0 * (temp_x - temp_y)\n    lab_b = 200.0 * (temp_y - temp_z)\n    return LabColor(\n        lab_l, lab_a, lab_b, observer=cobj.observer, illuminant=cobj.illuminant)",
    "reference": "Converts XYZ to Lab.",
    "generated": "Converts XYZ to Lab."
  },
  {
    "code": "def XYZ_to_RGB(cobj, target_rgb, *args, **kwargs):\n    \"\"\"\n    XYZ to RGB conversion.\n    \"\"\"\n    temp_X = cobj.xyz_x\n    temp_Y = cobj.xyz_y\n    temp_Z = cobj.xyz_z\n\n    logger.debug(\"  \\- Target RGB space: %s\", target_rgb)\n    target_illum = target_rgb.native_illuminant\n    logger.debug(\"  \\- Target native illuminant: %s\", target_illum)\n    logger.debug(\"  \\- XYZ color's illuminant: %s\", cobj.illuminant)\n\n    # If the XYZ values were taken with a different reference white than the\n    # native reference white of the target RGB space, a transformation matrix\n    # must be applied.\n    if cobj.illuminant != target_illum:\n        logger.debug(\"  \\* Applying transformation from %s to %s \",\n                     cobj.illuminant, target_illum)\n        # Get the adjusted XYZ values, adapted for the target illuminant.\n        temp_X, temp_Y, temp_Z = apply_chromatic_adaptation(\n            temp_X, temp_Y, temp_Z,\n            orig_illum=cobj.illuminant, targ_illum=target_illum)\n        logger.debug(\"  \\*   New values: %.3f, %.3f, %.3f\",\n                     temp_X, temp_Y, temp_Z)\n\n    # Apply an RGB working space matrix to the XYZ values (matrix mul).\n    rgb_r, rgb_g, rgb_b = apply_RGB_matrix(\n        temp_X, temp_Y, temp_Z,\n        rgb_type=target_rgb, convtype=\"xyz_to_rgb\")\n\n    # v\n    linear_channels = dict(r=rgb_r, g=rgb_g, b=rgb_b)\n    # V\n    nonlinear_channels = {}\n    if target_rgb == sRGBColor:\n        for channel in ['r', 'g', 'b']:\n            v = linear_channels[channel]\n            if v <= 0.0031308:\n                nonlinear_channels[channel] = v * 12.92\n            else:\n                nonlinear_channels[channel] = 1.055 * math.pow(v, 1 / 2.4) - 0.055\n    elif target_rgb == BT2020Color:\n        if kwargs.get('is_12_bits_system'):\n            a, b = 1.0993, 0.0181\n        else:\n            a, b = 1.099, 0.018\n        for channel in ['r', 'g', 'b']:\n            v = linear_channels[channel]\n            if v < b:\n                nonlinear_channels[channel] = v * 4.5\n            else:\n                nonlinear_channels[channel] = a * math.pow(v, 0.45) - (a - 1)\n    else:\n        # If it's not sRGB...\n        for channel in ['r', 'g', 'b']:\n            v = linear_channels[channel]\n            nonlinear_channels[channel] = math.pow(v, 1 / target_rgb.rgb_gamma)\n\n    return target_rgb(\n        nonlinear_channels['r'], nonlinear_channels['g'], nonlinear_channels['b'])",
    "reference": "XYZ to RGB conversion.",
    "generated": "Convert XYZ to RGB conversion."
  },
  {
    "code": "def RGB_to_XYZ(cobj, target_illuminant=None, *args, **kwargs):\n    \"\"\"\n    RGB to XYZ conversion. Expects 0-255 RGB values.\n\n    Based off of: http://www.brucelindbloom.com/index.html?Eqn_RGB_to_XYZ.html\n    \"\"\"\n    # Will contain linearized RGB channels (removed the gamma func).\n    linear_channels = {}\n\n    if isinstance(cobj, sRGBColor):\n        for channel in ['r', 'g', 'b']:\n            V = getattr(cobj, 'rgb_' + channel)\n            if V <= 0.04045:\n                linear_channels[channel] = V / 12.92\n            else:\n                linear_channels[channel] = math.pow((V + 0.055) / 1.055, 2.4)\n    elif isinstance(cobj, BT2020Color):\n        if kwargs.get('is_12_bits_system'):\n            a, b, c = 1.0993, 0.0181, 0.081697877417347\n        else:\n            a, b, c = 1.099, 0.018, 0.08124794403514049\n        for channel in ['r', 'g', 'b']:\n            V = getattr(cobj, 'rgb_' + channel)\n            if V <= c:\n                linear_channels[channel] = V / 4.5\n            else:\n                linear_channels[channel] = math.pow((V + (a - 1)) / a, 1 / 0.45)\n    else:\n        # If it's not sRGB...\n        gamma = cobj.rgb_gamma\n\n        for channel in ['r', 'g', 'b']:\n            V = getattr(cobj, 'rgb_' + channel)\n            linear_channels[channel] = math.pow(V, gamma)\n\n    # Apply an RGB working space matrix to the XYZ values (matrix mul).\n    xyz_x, xyz_y, xyz_z = apply_RGB_matrix(\n        linear_channels['r'], linear_channels['g'], linear_channels['b'],\n        rgb_type=cobj, convtype=\"rgb_to_xyz\")\n\n    if target_illuminant is None:\n        target_illuminant = cobj.native_illuminant\n\n    # The illuminant of the original RGB object. This will always match\n    # the RGB colorspace's native illuminant.\n    illuminant = cobj.native_illuminant\n    xyzcolor = XYZColor(xyz_x, xyz_y, xyz_z, illuminant=illuminant)\n    # This will take care of any illuminant changes for us (if source\n    # illuminant != target illuminant).\n    xyzcolor.apply_adaptation(target_illuminant)\n\n    return xyzcolor",
    "reference": "RGB to XYZ conversion. Expects 0-255 RGB values.\n\n    Based off of: http://www.brucelindbloom.com/index.html?Eqn_RGB_to_XYZ.html",
    "generated": "Convert RGB to XYZ conversion. Expects 0-255 RGB values.\n\n    Based off of: http://www.brucelindbloom.com/index.html?Eqn_RGB_to_XYZ.html"
  },
  {
    "code": "def RGB_to_HSV(cobj, *args, **kwargs):\n    \"\"\"\n    Converts from RGB to HSV.\n\n    H values are in degrees and are 0 to 360.\n    S values are a percentage, 0.0 to 1.0.\n    V values are a percentage, 0.0 to 1.0.\n    \"\"\"\n    var_R = cobj.rgb_r\n    var_G = cobj.rgb_g\n    var_B = cobj.rgb_b\n\n    var_max = max(var_R, var_G, var_B)\n    var_min = min(var_R, var_G, var_B)\n\n    var_H = __RGB_to_Hue(var_R, var_G, var_B, var_min, var_max)\n\n    if var_max == 0:\n        var_S = 0\n    else:\n        var_S = 1.0 - (var_min / var_max)\n\n    var_V = var_max\n\n    return HSVColor(\n        var_H, var_S, var_V)",
    "reference": "Converts from RGB to HSV.\n\n    H values are in degrees and are 0 to 360.\n    S values are a percentage, 0.0 to 1.0.\n    V values are a percentage, 0.0 to 1.0.",
    "generated": "Converts from RGB to HSV.\n\n    H values are in degrees and are 0 to 360.\n    S values are a percentage, 0.0 to 1.0."
  },
  {
    "code": "def RGB_to_HSL(cobj, *args, **kwargs):\n    \"\"\"\n    Converts from RGB to HSL.\n\n    H values are in degrees and are 0 to 360.\n    S values are a percentage, 0.0 to 1.0.\n    L values are a percentage, 0.0 to 1.0.\n    \"\"\"\n    var_R = cobj.rgb_r\n    var_G = cobj.rgb_g\n    var_B = cobj.rgb_b\n\n    var_max = max(var_R, var_G, var_B)\n    var_min = min(var_R, var_G, var_B)\n\n    var_H = __RGB_to_Hue(var_R, var_G, var_B, var_min, var_max)\n    var_L = 0.5 * (var_max + var_min)\n\n    if var_max == var_min:\n        var_S = 0\n    elif var_L <= 0.5:\n        var_S = (var_max - var_min) / (2.0 * var_L)\n    else:\n        var_S = (var_max - var_min) / (2.0 - (2.0 * var_L))\n\n    return HSLColor(\n        var_H, var_S, var_L)",
    "reference": "Converts from RGB to HSL.\n\n    H values are in degrees and are 0 to 360.\n    S values are a percentage, 0.0 to 1.0.\n    L values are a percentage, 0.0 to 1.0.",
    "generated": "Converts from RGB to HSL.\n\n    H values are in degrees and are 0 to 360.\n    S values are a percentage, 0.0 to 1.0."
  },
  {
    "code": "def __Calc_HSL_to_RGB_Components(var_q, var_p, C):\n    \"\"\"\n    This is used in HSL_to_RGB conversions on R, G, and B.\n    \"\"\"\n    if C < 0:\n        C += 1.0\n    if C > 1:\n        C -= 1.0\n\n    # Computing C of vector (Color R, Color G, Color B)\n    if C < (1.0 / 6.0):\n        return var_p + ((var_q - var_p) * 6.0 * C)\n    elif (1.0 / 6.0) <= C < 0.5:\n        return var_q\n    elif 0.5 <= C < (2.0 / 3.0):\n        return var_p + ((var_q - var_p) * 6.0 * ((2.0 / 3.0) - C))\n    else:\n        return var_p",
    "reference": "This is used in HSL_to_RGB conversions on R, G, and B.",
    "generated": "This is used in HSL_to_RGB conversions on R, G, and B."
  },
  {
    "code": "def HSV_to_RGB(cobj, target_rgb, *args, **kwargs):\n    \"\"\"\n    HSV to RGB conversion.\n\n    H values are in degrees and are 0 to 360.\n    S values are a percentage, 0.0 to 1.0.\n    V values are a percentage, 0.0 to 1.0.\n    \"\"\"\n    H = cobj.hsv_h\n    S = cobj.hsv_s\n    V = cobj.hsv_v\n\n    h_floored = int(math.floor(H))\n    h_sub_i = int(h_floored / 60) % 6\n    var_f = (H / 60.0) - (h_floored // 60)\n    var_p = V * (1.0 - S)\n    var_q = V * (1.0 - var_f * S)\n    var_t = V * (1.0 - (1.0 - var_f) * S)\n\n    if h_sub_i == 0:\n        rgb_r = V\n        rgb_g = var_t\n        rgb_b = var_p\n    elif h_sub_i == 1:\n        rgb_r = var_q\n        rgb_g = V\n        rgb_b = var_p\n    elif h_sub_i == 2:\n        rgb_r = var_p\n        rgb_g = V\n        rgb_b = var_t\n    elif h_sub_i == 3:\n        rgb_r = var_p\n        rgb_g = var_q\n        rgb_b = V\n    elif h_sub_i == 4:\n        rgb_r = var_t\n        rgb_g = var_p\n        rgb_b = V\n    elif h_sub_i == 5:\n        rgb_r = V\n        rgb_g = var_p\n        rgb_b = var_q\n    else:\n        raise ValueError(\"Unable to convert HSL->RGB due to value error.\")\n\n    # TODO: Investigate intent of following code block.\n    # In the event that they define an HSV color and want to convert it to\n    # a particular RGB space, let them override it here.\n    # if target_rgb is not None:\n    #     rgb_type = target_rgb\n    # else:\n    #     rgb_type = cobj.rgb_type\n\n    return target_rgb(rgb_r, rgb_g, rgb_b)",
    "reference": "HSV to RGB conversion.\n\n    H values are in degrees and are 0 to 360.\n    S values are a percentage, 0.0 to 1.0.\n    V values are a percentage, 0.0 to 1.0.",
    "generated": "Convert HSV to RGB conversion.\n\n    H values are in degrees and are 0 to 360.\n    S values are a percentage, 0.0 to 1.0.\n    V values are a percentage, 0.0 to 1.0."
  },
  {
    "code": "def HSL_to_RGB(cobj, target_rgb, *args, **kwargs):\n    \"\"\"\n    HSL to RGB conversion.\n    \"\"\"\n    H = cobj.hsl_h\n    S = cobj.hsl_s\n    L = cobj.hsl_l\n\n    if L < 0.5:\n        var_q = L * (1.0 + S)\n    else:\n        var_q = L + S - (L * S)\n\n    var_p = 2.0 * L - var_q\n\n    # H normalized to range [0,1]\n    h_sub_k = (H / 360.0)\n\n    t_sub_R = h_sub_k + (1.0 / 3.0)\n    t_sub_G = h_sub_k\n    t_sub_B = h_sub_k - (1.0 / 3.0)\n\n    rgb_r = __Calc_HSL_to_RGB_Components(var_q, var_p, t_sub_R)\n    rgb_g = __Calc_HSL_to_RGB_Components(var_q, var_p, t_sub_G)\n    rgb_b = __Calc_HSL_to_RGB_Components(var_q, var_p, t_sub_B)\n\n    # TODO: Investigate intent of following code block.\n    # In the event that they define an HSV color and want to convert it to\n    # a particular RGB space, let them override it here.\n    # if target_rgb is not None:\n    #     rgb_type = target_rgb\n    # else:\n    #     rgb_type = cobj.rgb_type\n\n    return target_rgb(rgb_r, rgb_g, rgb_b)",
    "reference": "HSL to RGB conversion.",
    "generated": "Convert HSL to RGB conversion."
  },
  {
    "code": "def RGB_to_CMY(cobj, *args, **kwargs):\n    \"\"\"\n    RGB to CMY conversion.\n\n    NOTE: CMYK and CMY values range from 0.0 to 1.0\n    \"\"\"\n    cmy_c = 1.0 - cobj.rgb_r\n    cmy_m = 1.0 - cobj.rgb_g\n    cmy_y = 1.0 - cobj.rgb_b\n\n    return CMYColor(cmy_c, cmy_m, cmy_y)",
    "reference": "RGB to CMY conversion.\n\n    NOTE: CMYK and CMY values range from 0.0 to 1.0",
    "generated": "Convert RGB to CMY conversion.\n\n    NOTE: CMYK and CMY values range from 0.0 to 1.0"
  },
  {
    "code": "def CMY_to_RGB(cobj, target_rgb, *args, **kwargs):\n    \"\"\"\n    Converts CMY to RGB via simple subtraction.\n\n    NOTE: Returned values are in the range of 0-255.\n    \"\"\"\n    rgb_r = 1.0 - cobj.cmy_c\n    rgb_g = 1.0 - cobj.cmy_m\n    rgb_b = 1.0 - cobj.cmy_y\n\n    return target_rgb(rgb_r, rgb_g, rgb_b)",
    "reference": "Converts CMY to RGB via simple subtraction.\n\n    NOTE: Returned values are in the range of 0-255.",
    "generated": "Converts CMY to RGB via simple subtraction.\n\n    NOTE: Returned values are in the range of 0-255."
  },
  {
    "code": "def CMY_to_CMYK(cobj, *args, **kwargs):\n    \"\"\"\n    Converts from CMY to CMYK.\n\n    NOTE: CMYK and CMY values range from 0.0 to 1.0\n    \"\"\"\n    var_k = 1.0\n    if cobj.cmy_c < var_k:\n        var_k = cobj.cmy_c\n    if cobj.cmy_m < var_k:\n        var_k = cobj.cmy_m\n    if cobj.cmy_y < var_k:\n        var_k = cobj.cmy_y\n\n    if var_k == 1:\n        cmyk_c = 0.0\n        cmyk_m = 0.0\n        cmyk_y = 0.0\n    else:\n        cmyk_c = (cobj.cmy_c - var_k) / (1.0 - var_k)\n        cmyk_m = (cobj.cmy_m - var_k) / (1.0 - var_k)\n        cmyk_y = (cobj.cmy_y - var_k) / (1.0 - var_k)\n    cmyk_k = var_k\n\n    return CMYKColor(cmyk_c, cmyk_m, cmyk_y, cmyk_k)",
    "reference": "Converts from CMY to CMYK.\n\n    NOTE: CMYK and CMY values range from 0.0 to 1.0",
    "generated": "Converts from CMY to CMYK.\n\n    NOTE: CMYK and CMY values range from 0.0 to 1.0"
  },
  {
    "code": "def CMYK_to_CMY(cobj, *args, **kwargs):\n    \"\"\"\n    Converts CMYK to CMY.\n\n    NOTE: CMYK and CMY values range from 0.0 to 1.0\n    \"\"\"\n    cmy_c = cobj.cmyk_c * (1.0 - cobj.cmyk_k) + cobj.cmyk_k\n    cmy_m = cobj.cmyk_m * (1.0 - cobj.cmyk_k) + cobj.cmyk_k\n    cmy_y = cobj.cmyk_y * (1.0 - cobj.cmyk_k) + cobj.cmyk_k\n\n    return CMYColor(cmy_c, cmy_m, cmy_y)",
    "reference": "Converts CMYK to CMY.\n\n    NOTE: CMYK and CMY values range from 0.0 to 1.0",
    "generated": "Converts CMYK to CMY.\n\n    NOTE: CMYK and CMY values range from 0.0 to 1.0"
  },
  {
    "code": "def XYZ_to_IPT(cobj, *args, **kwargs):\n    \"\"\"\n    Converts XYZ to IPT.\n\n    NOTE: XYZ values need to be adapted to 2 degree D65\n\n    Reference:\n    Fairchild, M. D. (2013). Color appearance models, 3rd Ed. (pp. 271-272). John Wiley & Sons.\n    \"\"\"\n    if cobj.illuminant != 'd65' or cobj.observer != '2':\n        raise ValueError('XYZColor for XYZ->IPT conversion needs to be D65 adapted.')\n    xyz_values = numpy.array(cobj.get_value_tuple())\n    lms_values = numpy.dot(\n        IPTColor.conversion_matrices['xyz_to_lms'],\n        xyz_values)\n\n    lms_prime = numpy.sign(lms_values) * numpy.abs(lms_values) ** 0.43\n\n    ipt_values = numpy.dot(\n        IPTColor.conversion_matrices['lms_to_ipt'],\n        lms_prime)\n    return IPTColor(*ipt_values)",
    "reference": "Converts XYZ to IPT.\n\n    NOTE: XYZ values need to be adapted to 2 degree D65\n\n    Reference:\n    Fairchild, M. D. (2013). Color appearance models, 3rd Ed. (pp. 271-272). John Wiley & Sons.",
    "generated": "Converts XYZ to IPT.\n\n    NOTE: XYZ values need to be adapted to 2 degree D65\n\n    Reference:\n    Fairchild, M. D (2013). Color appearance models, 3rd Ed. (pp. 271-272). John Wiley & Sons."
  },
  {
    "code": "def IPT_to_XYZ(cobj, *args, **kwargs):\n    \"\"\"\n    Converts IPT to XYZ.\n    \"\"\"\n    ipt_values = numpy.array(cobj.get_value_tuple())\n    lms_values = numpy.dot(\n        numpy.linalg.inv(IPTColor.conversion_matrices['lms_to_ipt']),\n        ipt_values)\n\n    lms_prime = numpy.sign(lms_values) * numpy.abs(lms_values) ** (1 / 0.43)\n\n    xyz_values = numpy.dot(\n        numpy.linalg.inv(IPTColor.conversion_matrices['xyz_to_lms']),\n        lms_prime)\n    return XYZColor(*xyz_values, observer='2', illuminant='d65')",
    "reference": "Converts IPT to XYZ.",
    "generated": "Converts IPT to XYZ."
  },
  {
    "code": "def convert_color(color, target_cs, through_rgb_type=sRGBColor,\n                  target_illuminant=None, *args, **kwargs):\n    \"\"\"\n    Converts the color to the designated color space.\n\n    :param color: A Color instance to convert.\n    :param target_cs: The Color class to convert to. Note that this is not\n        an instance, but a class.\n    :keyword BaseRGBColor through_rgb_type: If during your conversion between\n        your original and target color spaces you have to pass through RGB,\n        this determines which kind of RGB to use. For example, XYZ->HSL.\n        You probably don't need to specify this unless you have a special\n        usage case.\n    :type target_illuminant: None or str\n    :keyword target_illuminant: If during conversion from RGB to a reflective\n        color space you want to explicitly end up with a certain illuminant,\n        pass this here. Otherwise the RGB space's native illuminant\n        will be used.\n    :returns: An instance of the type passed in as ``target_cs``.\n    :raises: :py:exc:`colormath.color_exceptions.UndefinedConversionError`\n        if conversion between the two color spaces isn't possible.\n    \"\"\"\n    if isinstance(target_cs, str):\n        raise ValueError(\"target_cs parameter must be a Color object.\")\n    if not issubclass(target_cs, ColorBase):\n        raise ValueError(\"target_cs parameter must be a Color object.\")\n\n    conversions = _conversion_manager.get_conversion_path(color.__class__, target_cs)\n\n    logger.debug('Converting %s to %s', color, target_cs)\n    logger.debug(' @ Conversion path: %s', conversions)\n\n    # Start with original color in case we convert to the same color space.\n    new_color = color\n\n    if issubclass(target_cs, BaseRGBColor):\n        # If the target_cs is an RGB color space of some sort, then we\n        # have to set our through_rgb_type to make sure the conversion returns\n        # the expected RGB colorspace (instead of defaulting to sRGBColor).\n        through_rgb_type = target_cs\n\n    # We have to be careful to use the same RGB color space that created\n    # an object (if it was created by a conversion) in order to get correct\n    # results. For example, XYZ->HSL via Adobe RGB should default to Adobe\n    # RGB when taking that generated HSL object back to XYZ.\n    # noinspection PyProtectedMember\n    if through_rgb_type != sRGBColor:\n        # User overrides take priority over everything.\n        # noinspection PyProtectedMember\n        target_rgb = through_rgb_type\n    elif color._through_rgb_type:\n        # Otherwise, a value on the color object is the next best thing,\n        # when available.\n        # noinspection PyProtectedMember\n        target_rgb = color._through_rgb_type\n    else:\n        # We could collapse this into a single if statement above,\n        # but I think this reads better.\n        target_rgb = through_rgb_type\n\n    # Iterate through the list of functions for the conversion path, storing\n    # the results in a dictionary via update(). This way the user has access\n    # to all of the variables involved in the conversion.\n    for func in conversions:\n        # Execute the function in this conversion step and store the resulting\n        # Color object.\n        logger.debug(' * Conversion: %s passed to %s()',\n                     new_color.__class__.__name__, func)\n        logger.debug(' |->  in %s', new_color)\n\n        if func:\n            # This can be None if you try to convert a color to the color\n            # space that is already in. IE: XYZ->XYZ.\n            new_color = func(\n                new_color,\n                target_rgb=target_rgb,\n                target_illuminant=target_illuminant,\n                *args, **kwargs)\n\n        logger.debug(' |-< out %s', new_color)\n\n    # If this conversion had something other than the default sRGB color space\n    # requested,\n    if through_rgb_type != sRGBColor:\n        new_color._through_rgb_type = through_rgb_type\n\n    return new_color",
    "reference": "Converts the color to the designated color space.\n\n    :param color: A Color instance to convert.\n    :param target_cs: The Color class to convert to. Note that this is not\n        an instance, but a class.\n    :keyword BaseRGBColor through_rgb_type: If during your conversion between\n        your original and target color spaces you have to pass through RGB,\n        this determines which kind of RGB to use. For example, XYZ->HSL.\n        You probably don't need to specify this unless you have a special\n        usage case.\n    :type target_illuminant: None or str\n    :keyword target_illuminant: If during conversion from RGB to a reflective\n        color space you want to explicitly end up with a certain illuminant,\n        pass this here. Otherwise the RGB space's native illuminant\n        will be used.\n    :returns: An instance of the type passed in as ``target_cs``.\n    :raises: :py:exc:`colormath.color_exceptions.UndefinedConversionError`\n        if conversion between the two color spaces isn't possible.",
    "generated": "Converts the color to the designated color space.\n\n    :param color: A Color instance to convert.\n    :param target_cs: The Color class to convert. Note that this is not\n        an instance but a class.\n    :keyword BaseRGBColor through_rgb_type: If during your conversion between\n        your original and target color spaces you have to pass through RGB,\n        this determines which kind of RGB to use. For example, XYZ->HSL.\n        You probably don't need to specify this"
  },
  {
    "code": "def adjust_white_for_scc(cls, rgb_p, rgb_b, rgb_w, p):\n        \"\"\"\n        Adjust the white point for simultaneous chromatic contrast.\n\n        :param rgb_p: Cone signals of proxima field.\n        :param rgb_b: Cone signals of background.\n        :param rgb_w: Cone signals of reference white.\n        :param p: Simultaneous contrast/assimilation parameter.\n        :return: Adjusted cone signals for reference white.\n        \"\"\"\n        p_rgb = rgb_p / rgb_b\n        rgb_w = rgb_w * (((1 - p) * p_rgb + (1 + p) / p_rgb) ** 0.5) / (((1 + p) * p_rgb + (1 - p) / p_rgb) ** 0.5)\n        return rgb_w",
    "reference": "Adjust the white point for simultaneous chromatic contrast.\n\n        :param rgb_p: Cone signals of proxima field.\n        :param rgb_b: Cone signals of background.\n        :param rgb_w: Cone signals of reference white.\n        :param p: Simultaneous contrast/assimilation parameter.\n        :return: Adjusted cone signals for reference white.",
    "generated": "Adjust the white point for simultaneous chromatic contrast.\n\n        :param rgb_p: Cone signals of proxima field.\n        :param rgb_b: Cone signals of background."
  },
  {
    "code": "def _compute_adaptation(self, xyz, xyz_w, f_l, d):\n        \"\"\"\n        Modified adaptation procedure incorporating simultaneous chromatic contrast from Hunt model.\n\n        :param xyz: Stimulus XYZ.\n        :param xyz_w: Reference white XYZ.\n        :param f_l: Luminance adaptation factor\n        :param d: Degree of adaptation.\n        :return: Tuple of adapted rgb and rgb_w arrays.\n        \"\"\"\n        # Transform input colors to cone responses\n        rgb = self._xyz_to_rgb(xyz)\n        logger.debug(\"RGB: {}\".format(rgb))\n\n        rgb_b = self._xyz_to_rgb(self._xyz_b)\n        rgb_w = self._xyz_to_rgb(xyz_w)\n        rgb_w = Hunt.adjust_white_for_scc(rgb, rgb_b, rgb_w, self._p)\n        logger.debug(\"RGB_W: {}\".format(rgb_w))\n\n        # Compute adapted tristimulus-responses\n        rgb_c = self._white_adaption(rgb, rgb_w, d)\n        logger.debug(\"RGB_C: {}\".format(rgb_c))\n        rgb_cw = self._white_adaption(rgb_w, rgb_w, d)\n        logger.debug(\"RGB_CW: {}\".format(rgb_cw))\n\n        # Convert adapted tristimulus-responses to Hunt-Pointer-Estevez fundamentals\n        rgb_p = self._compute_hunt_pointer_estevez_fundamentals(rgb_c)\n        logger.debug(\"RGB': {}\".format(rgb_p))\n        rgb_wp = self._compute_hunt_pointer_estevez_fundamentals(rgb_cw)\n        logger.debug(\"RGB'_W: {}\".format(rgb_wp))\n\n        # Compute post-adaptation non-linearities\n        rgb_ap = self._compute_nonlinearities(f_l, rgb_p)\n        rgb_awp = self._compute_nonlinearities(f_l, rgb_wp)\n\n        return rgb_ap, rgb_awp",
    "reference": "Modified adaptation procedure incorporating simultaneous chromatic contrast from Hunt model.\n\n        :param xyz: Stimulus XYZ.\n        :param xyz_w: Reference white XYZ.\n        :param f_l: Luminance adaptation factor\n        :param d: Degree of adaptation.\n        :return: Tuple of adapted rgb and rgb_w arrays.",
    "generated": "Modified adaptation procedure incorporating simultaneous chromatic contrast from Hunt model.\n\n        :param xyz: Stimulus XYZ.\n        :param xyz_w: Reference white XYZ.\n        :param f_l: Luminance adaptation factor\n        :param d: Degree of adaptation."
  },
  {
    "code": "def validate_one_touch_signature(self, signature, nonce, method, url, params):\n        \"\"\"\n        Function to validate signature in X-Authy-Signature key of headers.\n\n        :param string signature: X-Authy-Signature key of headers.\n        :param string nonce: X-Authy-Signature-Nonce key of headers.\n        :param string method: GET or POST - configured in app settings for OneTouch.\n        :param string url: base callback url.\n        :param dict params: params sent by Authy.\n        :return bool: True if calculated signature and X-Authy-Signature are identical else False.\n        \"\"\"\n        if not signature or not isinstance(signature, str):\n            raise AuthyFormatException(\n                \"Invalid signature - should not be empty. It is required\")\n\n        if not nonce:\n            raise AuthyFormatException(\n                \"Invalid nonce - should not be empty. It is required\")\n\n        if not method or not ('get' == method.lower() or 'post' == method.lower()):\n            raise AuthyFormatException(\n                \"Invalid method - should not be empty. It is required\")\n\n        if not params or not isinstance(params, dict):\n            raise AuthyFormatException(\n                \"Invalid params - should not be empty. It is required\")\n\n        query_params = self.__make_http_query(params)\n        # Sort and replace encoded  params in case-sensitive order\n        sorted_params = '&'.join(sorted(query_params.replace(\n            '/', '%2F').replace('%20', '+').split('&')))\n        sorted_params = re.sub(\"\\\\%5B([0-9])*\\\\%5D\", \"%5B%5D\", sorted_params)\n        sorted_params = re.sub(\"\\\\=None\", \"=\", sorted_params)\n        data = nonce + \"|\" + method + \"|\" + url + \"|\" + sorted_params\n        try:\n            calculated_signature = base64.b64encode(\n                hmac.new(self.api_key.encode(), data.encode(), hashlib.sha256).digest())\n            return calculated_signature.decode() == signature\n        except:\n            calculated_signature = base64.b64encode(\n                hmac.new(self.api_key, data, hashlib.sha256).digest())\n            return calculated_signature == signature",
    "reference": "Function to validate signature in X-Authy-Signature key of headers.\n\n        :param string signature: X-Authy-Signature key of headers.\n        :param string nonce: X-Authy-Signature-Nonce key of headers.\n        :param string method: GET or POST - configured in app settings for OneTouch.\n        :param string url: base callback url.\n        :param dict params: params sent by Authy.\n        :return bool: True if calculated signature and X-Authy-Signature are identical else False.",
    "generated": "Validate signature in X-Authy-Signature key of headers.\n\n        :param string signature: X-Authy-Signature key of headers.\n        :param string nonce: X-Authy-Signature-Nonce key of headers.\n        :param string method: GET or POST - configured in app settings for OneTouch.\n        :param string nonce: X-Authy-Signature-Nonce key of headers.\n        :param string nonce: X-Authy-Signature-Nonce key of headers.\n        :param string nonce: X-Authy-Signature-Nonce key of headers."
  },
  {
    "code": "def compile(script, vars={}, library_paths=[]):\n    \"\"\"\n    Compile a jq script, retuning a script object.\n\n    library_paths is a list of strings that defines the module search path.\n    \"\"\"\n\n    return _pyjq.Script(script.encode('utf-8'), vars=vars,\n                        library_paths=library_paths)",
    "reference": "Compile a jq script, retuning a script object.\n\n    library_paths is a list of strings that defines the module search path.",
    "generated": "Compile a jq script, retuning a script object.\n\n    library_paths is a list of strings that defines the module search path."
  },
  {
    "code": "def apply(script, value=None, vars={}, url=None, opener=default_opener, library_paths=[]):\n    \"\"\"\n    Transform value by script, returning all results as list.\n    \"\"\"\n    return all(script, value, vars, url, opener, library_paths)",
    "reference": "Transform value by script, returning all results as list.",
    "generated": "Transform value by script, returning all results as list."
  },
  {
    "code": "def first(script, value=None, default=None, vars={}, url=None, opener=default_opener, library_paths=[]):\n    \"\"\"\n    Transform object by jq script, returning the first result.\n    Return default if result is empty.\n    \"\"\"\n    return compile(script, vars, library_paths).first(_get_value(value, url, opener), default)",
    "reference": "Transform object by jq script, returning the first result.\n    Return default if result is empty.",
    "generated": "Transform object by jq script, returning the first result."
  },
  {
    "code": "def one(script, value=None, vars={}, url=None, opener=default_opener, library_paths=[]):\n    \"\"\"\n    Transform object by jq script, returning the first result.\n    Raise ValueError unless results does not include exactly one element.\n    \"\"\"\n    return compile(script, vars, library_paths).one(_get_value(value, url, opener))",
    "reference": "Transform object by jq script, returning the first result.\n    Raise ValueError unless results does not include exactly one element.",
    "generated": "Transform object by jq script, returning the first result.\n    Raise ValueError unless results does not include exactly one element."
  },
  {
    "code": "def calculate_mypypath() -> List[str]:\n    \"\"\"Return MYPYPATH so that stubs have precedence over local sources.\"\"\"\n\n    typeshed_root = None\n    count = 0\n    started = time.time()\n    for parent in itertools.chain(\n        # Look in current script's parents, useful for zipapps.\n        Path(__file__).parents,\n        # Look around site-packages, useful for virtualenvs.\n        Path(mypy.api.__file__).parents,\n        # Look in global paths, useful for globally installed.\n        Path(os.__file__).parents,\n    ):\n        count += 1\n        candidate = parent / 'lib' / 'mypy' / 'typeshed'\n        if candidate.is_dir():\n            typeshed_root = candidate\n            break\n\n        # Also check the non-installed path, useful for `setup.py develop`.\n        candidate = parent / 'typeshed'\n        if candidate.is_dir():\n            typeshed_root = candidate\n            break\n\n    LOG.debug(\n        'Checked %d paths in %.2fs looking for typeshed. Found %s',\n        count,\n        time.time() - started,\n        typeshed_root,\n    )\n\n    if not typeshed_root:\n        return []\n\n    stdlib_dirs = ('3.7', '3.6', '3.5', '3.4', '3.3', '3.2', '3', '2and3')\n    stdlib_stubs = [\n        typeshed_root / 'stdlib' / stdlib_dir\n        for stdlib_dir in stdlib_dirs\n    ]\n    third_party_dirs = ('3.7', '3.6', '3', '2and3')\n    third_party_stubs = [\n        typeshed_root / 'third_party' / tp_dir\n        for tp_dir in third_party_dirs\n    ]\n    return [\n        str(p) for p in stdlib_stubs + third_party_stubs\n    ]",
    "reference": "Return MYPYPATH so that stubs have precedence over local sources.",
    "generated": "Return MYPYPATH so that stubs have precedence over local sources."
  },
  {
    "code": "def email_list_to_email_dict(email_list):\n    \"\"\"Convert a list of email to a dict of email.\"\"\"\n    if email_list is None:\n        return {}\n    result = {}\n    for value in email_list:\n        realname, address = email.utils.parseaddr(value)\n        result[address] = realname if realname and address else address\n    return result",
    "reference": "Convert a list of email to a dict of email.",
    "generated": "Convert a list of email to a dict of email."
  },
  {
    "code": "def email_address_to_list(email_address):\n    \"\"\"Convert an email address to a list.\"\"\"\n    realname, address = email.utils.parseaddr(email_address)\n    return (\n        [address, realname] if realname and address else\n        [email_address, email_address]\n    )",
    "reference": "Convert an email address to a list.",
    "generated": "Convert an email address to a list."
  },
  {
    "code": "def send(sender_instance):\n    \"\"\"Send a transactional email using SendInBlue API.\n\n    Site: https://www.sendinblue.com\n    API: https://apidocs.sendinblue.com/\n    \"\"\"\n    m = Mailin(\n        \"https://api.sendinblue.com/v2.0\",\n        sender_instance._kwargs.get(\"api_key\")\n    )\n    data = {\n        \"to\": email_list_to_email_dict(sender_instance._recipient_list),\n        \"cc\": email_list_to_email_dict(sender_instance._cc),\n        \"bcc\": email_list_to_email_dict(sender_instance._bcc),\n        \"from\": email_address_to_list(sender_instance._from_email),\n        \"subject\": sender_instance._subject,\n    }\n    if sender_instance._template.is_html:\n        data.update({\n            \"html\": sender_instance._message,\n            \"headers\": {\"Content-Type\": \"text/html; charset=utf-8\"}\n        })\n    else:\n        data.update({\"text\": sender_instance._message})\n    if \"attachments\" in sender_instance._kwargs:\n        data[\"attachment\"] = {}\n        for attachment in sender_instance._kwargs[\"attachments\"]:\n            data[\"attachment\"][attachment[0]] = base64.b64encode(attachment[1])\n    result = m.send_email(data)\n    if result[\"code\"] != \"success\":\n        raise SendInBlueError(result[\"message\"])",
    "reference": "Send a transactional email using SendInBlue API.\n\n    Site: https://www.sendinblue.com\n    API: https://apidocs.sendinblue.com/",
    "generated": "Send a transactional email using SendInBlue API.\n\n    Site: https://www.sendinblue.com\n    API: https://apidocs.sendinblue.com/"
  },
  {
    "code": "def _mpv_coax_proptype(value, proptype=str):\n    \"\"\"Intelligently coax the given python value into something that can be understood as a proptype property.\"\"\"\n    if type(value) is bytes:\n        return value;\n    elif type(value) is bool:\n        return b'yes' if value else b'no'\n    elif proptype in (str, int, float):\n        return str(proptype(value)).encode('utf-8')\n    else:\n        raise TypeError('Cannot coax value of type {} into property type {}'.format(type(value), proptype))",
    "reference": "Intelligently coax the given python value into something that can be understood as a proptype property.",
    "generated": "Intelligently coax the given python value into something that can understood as a proptype property."
  },
  {
    "code": "def _make_node_str_list(l):\n    \"\"\"Take a list of python objects and make a MPV string node array from it.\n\n    As an example, the python list ``l = [ \"foo\", 23, false ]`` will result in the following MPV node object::\n\n        struct mpv_node {\n            .format = MPV_NODE_ARRAY,\n            .u.list = *(struct mpv_node_array){\n                .num = len(l),\n                .keys = NULL,\n                .values = struct mpv_node[len(l)] {\n                    { .format = MPV_NODE_STRING, .u.string = l[0] },\n                    { .format = MPV_NODE_STRING, .u.string = l[1] },\n                    ...\n                }\n            }\n        }\n    \"\"\"\n    char_ps = [ c_char_p(_mpv_coax_proptype(e, str)) for e in l ]\n    node_list = MpvNodeList(\n        num=len(l),\n        keys=None,\n        values=( MpvNode * len(l))( *[ MpvNode(\n                format=MpvFormat.STRING,\n                val=MpvNodeUnion(string=p))\n            for p in char_ps ]))\n    node = MpvNode(\n        format=MpvFormat.NODE_ARRAY,\n        val=MpvNodeUnion(list=pointer(node_list)))\n    return char_ps, node_list, node, cast(pointer(node), c_void_p)",
    "reference": "Take a list of python objects and make a MPV string node array from it.\n\n    As an example, the python list ``l = [ \"foo\", 23, false ]`` will result in the following MPV node object::\n\n        struct mpv_node {\n            .format = MPV_NODE_ARRAY,\n            .u.list = *(struct mpv_node_array){\n                .num = len(l),\n                .keys = NULL,\n                .values = struct mpv_node[len(l)] {\n                    { .format = MPV_NODE_STRING, .u.string = l[0] },\n                    { .format = MPV_NODE_STRING, .u.string = l[1] },\n                    ...\n                }\n            }\n        }",
    "generated": "Take a list of python objects and make a MPV string node array from it.\n\n    As an example, the python list ``l = [ \"foo\", 23, false]`` will result in the following MPV node object::\n\n        struct mpv_node {\n            .format = MPV_NODE_ARRAY,\n            .u.list = *(struct mpv_node_array){\n            .num = len(l),\n            .u.list = *(struct mpv_node_array)"
  },
  {
    "code": "def wait_for_property(self, name, cond=lambda val: val, level_sensitive=True):\n        \"\"\"Waits until ``cond`` evaluates to a truthy value on the named property. This can be used to wait for\n        properties such as ``idle_active`` indicating the player is done with regular playback and just idling around\n        \"\"\"\n        sema = threading.Semaphore(value=0)\n        def observer(name, val):\n            if cond(val):\n                sema.release()\n        self.observe_property(name, observer)\n        if not level_sensitive or not cond(getattr(self, name.replace('-', '_'))):\n            sema.acquire()\n        self.unobserve_property(name, observer)",
    "reference": "Waits until ``cond`` evaluates to a truthy value on the named property. This can be used to wait for\n        properties such as ``idle_active`` indicating the player is done with regular playback and just idling around",
    "generated": "Waits until ``cond`` evaluates to a truthy value on the named property. This can be used to wait for\n        properties such as ``idle_active`` indicating the player is done with regular playback and just idling around\n        \"\"\"Waits until ``cond`` evaluates to a truthy value on the named property. This can be used to wait for\n        properties such as ``idle_active`` indicating the player is done with regular playback and just idling around"
  },
  {
    "code": "def terminate(self):\n        \"\"\"Properly terminates this player instance. Preferably use this instead of relying on python's garbage\n        collector to cause this to be called from the object's destructor.\n        \"\"\"\n        self.handle, handle = None, self.handle\n        if threading.current_thread() is self._event_thread:\n            # Handle special case to allow event handle to be detached.\n            # This is necessary since otherwise the event thread would deadlock itself.\n            grim_reaper = threading.Thread(target=lambda: _mpv_terminate_destroy(handle))\n            grim_reaper.start()\n        else:\n            _mpv_terminate_destroy(handle)\n            if self._event_thread:\n                self._event_thread.join()",
    "reference": "Properly terminates this player instance. Preferably use this instead of relying on python's garbage\n        collector to cause this to be called from the object's destructor.",
    "generated": "Properly terminates this player instance. Preferably use this instead of relying on python's garbage\n        collector to cause this to be called from the object's destructor."
  },
  {
    "code": "def command(self, name, *args):\n        \"\"\"Execute a raw command.\"\"\"\n        args = [name.encode('utf-8')] + [ (arg if type(arg) is bytes else str(arg).encode('utf-8'))\n                for arg in args if arg is not None ] + [None]\n        _mpv_command(self.handle, (c_char_p*len(args))(*args))",
    "reference": "Execute a raw command.",
    "generated": "Execute a raw command."
  },
  {
    "code": "def property_observer(self, name):\n        \"\"\"Function decorator to register a property observer. See ``MPV.observe_property`` for details.\"\"\"\n        def wrapper(fun):\n            self.observe_property(name, fun)\n            fun.unobserve_mpv_properties = lambda: self.unobserve_property(name, fun)\n            return fun\n        return wrapper",
    "reference": "Function decorator to register a property observer. See ``MPV.observe_property`` for details.",
    "generated": "Function decorator to register a property observer. See ``MPV.observe_property`` for details."
  },
  {
    "code": "def unregister_message_handler(self, target_or_handler):\n        \"\"\"Unregister a mpv script message handler for the given script message target name.\n\n        You can also call the ``unregister_mpv_messages`` function attribute set on the handler function when it is\n        registered.\n        \"\"\"\n        if isinstance(target_or_handler, str):\n            del self._message_handlers[target_or_handler]\n        else:\n            for key, val in self._message_handlers.items():\n                if val == target_or_handler:\n                    del self._message_handlers[key]",
    "reference": "Unregister a mpv script message handler for the given script message target name.\n\n        You can also call the ``unregister_mpv_messages`` function attribute set on the handler function when it is\n        registered.",
    "generated": "Unregister a mpv script message handler for the given script message target name.\n\n        You can also call the ``unregister_mpv_messages`` function attribute set on the handler function when it is\n        registered."
  },
  {
    "code": "def message_handler(self, target):\n        \"\"\"Decorator to register a mpv script message handler.\n\n        WARNING: Only one handler can be registered at a time for any given target.\n\n        To unregister the message handler, call its ``unregister_mpv_messages`` function::\n\n            player = mpv.MPV()\n            @player.message_handler('foo')\n            def my_handler(some, args):\n                print(args)\n\n            my_handler.unregister_mpv_messages()\n        \"\"\"\n        def register(handler):\n            self._register_message_handler_internal(target, handler)\n            handler.unregister_mpv_messages = lambda: self.unregister_message_handler(handler)\n            return handler\n        return register",
    "reference": "Decorator to register a mpv script message handler.\n\n        WARNING: Only one handler can be registered at a time for any given target.\n\n        To unregister the message handler, call its ``unregister_mpv_messages`` function::\n\n            player = mpv.MPV()\n            @player.message_handler('foo')\n            def my_handler(some, args):\n                print(args)\n\n            my_handler.unregister_mpv_messages()",
    "generated": "Decorator to register a mpv script message handler.\n\n        WARNING: Only one handler can register at a time for any given target.\n\n        To unregister the message handler, call its ``unregister_mpv_messages`` function::\n\n            player = mpv.MPV()\n            @player.message_handler('foo')\n            def my_handler(some, args):\n                print(args)"
  },
  {
    "code": "def key_binding(self, keydef, mode='force'):\n        \"\"\"Function decorator to register a low-level key binding.\n\n        The callback function signature is ``fun(key_state, key_name)`` where ``key_state`` is either ``'U'`` for \"key\n        up\" or ``'D'`` for \"key down\".\n\n        The keydef format is: ``[Shift+][Ctrl+][Alt+][Meta+]<key>`` where ``<key>`` is either the literal character the\n        key produces (ASCII or Unicode character), or a symbolic name (as printed by ``mpv --input-keylist``).\n\n        To unregister the callback function, you can call its ``unregister_mpv_key_bindings`` attribute::\n\n            player = mpv.MPV()\n            @player.key_binding('Q')\n            def binding(state, name):\n                print('blep')\n\n            binding.unregister_mpv_key_bindings()\n\n        WARNING: For a single keydef only a single callback/command can be registered at the same time. If you register\n        a binding multiple times older bindings will be overwritten and there is a possibility of references leaking. So\n        don't do that.\n\n        BIG FAT WARNING: mpv's key binding mechanism is pretty powerful.  This means, you essentially get arbitrary code\n        exectution through key bindings. This interface makes some limited effort to sanitize the keydef given in the\n        first parameter, but YOU SHOULD NOT RELY ON THIS IN FOR SECURITY. If your input comes from config files, this is\n        completely fine--but, if you are about to pass untrusted input into this parameter, better double-check whether\n        this is secure in your case.\n        \"\"\"\n        def register(fun):\n            fun.mpv_key_bindings = getattr(fun, 'mpv_key_bindings', []) + [keydef]\n            def unregister_all():\n                for keydef in fun.mpv_key_bindings:\n                    self.unregister_key_binding(keydef)\n            fun.unregister_mpv_key_bindings = unregister_all\n\n            self.register_key_binding(keydef, fun, mode)\n            return fun\n        return register",
    "reference": "Function decorator to register a low-level key binding.\n\n        The callback function signature is ``fun(key_state, key_name)`` where ``key_state`` is either ``'U'`` for \"key\n        up\" or ``'D'`` for \"key down\".\n\n        The keydef format is: ``[Shift+][Ctrl+][Alt+][Meta+]<key>`` where ``<key>`` is either the literal character the\n        key produces (ASCII or Unicode character), or a symbolic name (as printed by ``mpv --input-keylist``).\n\n        To unregister the callback function, you can call its ``unregister_mpv_key_bindings`` attribute::\n\n            player = mpv.MPV()\n            @player.key_binding('Q')\n            def binding(state, name):\n                print('blep')\n\n            binding.unregister_mpv_key_bindings()\n\n        WARNING: For a single keydef only a single callback/command can be registered at the same time. If you register\n        a binding multiple times older bindings will be overwritten and there is a possibility of references leaking. So\n        don't do that.\n\n        BIG FAT WARNING: mpv's key binding mechanism is pretty powerful.  This means, you essentially get arbitrary code\n        exectution through key bindings. This interface makes some limited effort to sanitize the keydef given in the\n        first parameter, but YOU SHOULD NOT RELY ON THIS IN FOR SECURITY. If your input comes from config files, this is\n        completely fine--but, if you are about to pass untrusted input into this parameter, better double-check whether\n        this is secure in your case.",
    "generated": "Function decorator to register a low-level key binding.\n\n        The callback function signature is ``fun(key_state, key_name)`` where ``key_state`` is either ``'U'`` for \"key\n        up\" or ``'D'`` for \"key down\".\n\n        The keydef format is: ``[Shift+][Ctrl+][Alt+][Meta+]<key>`` where ``<key>`` is either the literal character the\n        key produces (ASCII or Unicode character), or a symbolic name (as printed by ``mpv --input-key"
  },
  {
    "code": "def register_key_binding(self, keydef, callback_or_cmd, mode='force'):\n        \"\"\"Register a key binding. This takes an mpv keydef and either a string containing a mpv command or a python\n        callback function.  See ``MPV.key_binding`` for details.\n        \"\"\"\n        if not re.match(r'(Shift+)?(Ctrl+)?(Alt+)?(Meta+)?(.|\\w+)', keydef):\n            raise ValueError('Invalid keydef. Expected format: [Shift+][Ctrl+][Alt+][Meta+]<key>\\n'\n                    '<key> is either the literal character the key produces (ASCII or Unicode character), or a '\n                    'symbolic name (as printed by --input-keylist')\n        binding_name = MPV._binding_name(keydef)\n        if callable(callback_or_cmd):\n            self._key_binding_handlers[binding_name] = callback_or_cmd\n            self.register_message_handler('key-binding', self._handle_key_binding_message)\n            self.command('define-section',\n                    binding_name, '{} script-binding py_event_handler/{}'.format(keydef, binding_name), mode)\n        elif isinstance(callback_or_cmd, str):\n            self.command('define-section', binding_name, '{} {}'.format(keydef, callback_or_cmd), mode)\n        else:\n            raise TypeError('register_key_binding expects either an str with an mpv command or a python callable.')\n        self.command('enable-section', binding_name, 'allow-hide-cursor+allow-vo-dragging')",
    "reference": "Register a key binding. This takes an mpv keydef and either a string containing a mpv command or a python\n        callback function.  See ``MPV.key_binding`` for details.",
    "generated": "Register a key binding. This takes an mpv keydef and either a string containing a mpv command or a python\n        callback function. See ``MPV.key_binding`` for details."
  },
  {
    "code": "def unregister_key_binding(self, keydef):\n        \"\"\"Unregister a key binding by keydef.\"\"\"\n        binding_name = MPV._binding_name(keydef)\n        self.command('disable-section', binding_name)\n        self.command('define-section', binding_name, '')\n        if binding_name in self._key_binding_handlers:\n            del self._key_binding_handlers[binding_name]\n            if not self._key_binding_handlers:\n                self.unregister_message_handler('key-binding')",
    "reference": "Unregister a key binding by keydef.",
    "generated": "Unregister a key binding by keydef."
  },
  {
    "code": "def _process_origin(self, req, resp, origin):\n        \"\"\"Inspects the request and adds the Access-Control-Allow-Origin\n        header if the requested origin is allowed.\n\n        Returns:\n            ``True`` if the header was added and the requested origin\n            is allowed, ``False`` if the origin is not allowed and the\n            header has not been added.\n        \"\"\"\n        if self._cors_config['allow_all_origins']:\n            if self.supports_credentials:\n                self._set_allow_origin(resp, origin)\n            else:\n                self._set_allow_origin(resp, '*')\n            return True\n\n        if origin in self._cors_config['allow_origins_list']:\n            self._set_allow_origin(resp, origin)\n            return True\n\n        regex = self._cors_config['allow_origins_regex']\n        if regex is not None:\n            if regex.match(origin):\n                self._set_allow_origin(resp, origin)\n                return True\n\n        return False",
    "reference": "Inspects the request and adds the Access-Control-Allow-Origin\n        header if the requested origin is allowed.\n\n        Returns:\n            ``True`` if the header was added and the requested origin\n            is allowed, ``False`` if the origin is not allowed and the\n            header has not been added.",
    "generated": "Inspects the request and adds the Access-Control-Allow-Origin\n        header if the requested origin is allowed.\n\n        Returns:\n            ``True`` if the header was added and the requested origin\n            is allowed, ``False`` if the origin is allowed and the\n            header has not been added."
  },
  {
    "code": "def _process_allow_headers(self, req, resp, requested_headers):\n        \"\"\"Adds the Access-Control-Allow-Headers header to the response,\n        using the cors settings to determine which headers are allowed.\n\n        Returns:\n            True if all the headers the client requested are allowed.\n            False if some or none of the headers the client requested are allowed.\n        \"\"\"\n        if not requested_headers:\n            return True\n        elif self._cors_config['allow_all_headers']:\n            self._set_allowed_headers(resp, requested_headers)\n            return True\n\n        approved_headers = []\n        for header in requested_headers:\n            if header.lower() in self._cors_config['allow_headers_list']:\n                approved_headers.append(header)\n            elif self._cors_config.get('allow_headers_regex'):\n                if self._cors_config['allow_headers_regex'].match(header):\n                    approved_headers.append(header)\n\n        if len(approved_headers) == len(requested_headers):\n            self._set_allowed_headers(resp, approved_headers)\n            return True\n\n        return False",
    "reference": "Adds the Access-Control-Allow-Headers header to the response,\n        using the cors settings to determine which headers are allowed.\n\n        Returns:\n            True if all the headers the client requested are allowed.\n            False if some or none of the headers the client requested are allowed.",
    "generated": "Adds the Access-Control-Allow-Headers header to the response,\n        using the cors settings to determine which headers are allowed.\n\n        Returns:\n            True if all the headers the client requested are allowed."
  },
  {
    "code": "def _process_methods(self, req, resp, resource):\n        \"\"\"Adds the Access-Control-Allow-Methods header to the response,\n        using the cors settings to determine which methods are allowed.\n        \"\"\"\n        requested_method = self._get_requested_method(req)\n        if not requested_method:\n            return False\n\n        if self._cors_config['allow_all_methods']:\n            allowed_methods = self._get_resource_methods(resource)\n            self._set_allowed_methods(resp, allowed_methods)\n            if requested_method in allowed_methods:\n                return True\n        elif requested_method in self._cors_config['allow_methods_list']:\n            resource_methods = self._get_resource_methods(resource)\n            # Only list methods as allowed if they exist\n            # on the resource AND are in the allowed_methods_list\n            allowed_methods = [\n                method for method in resource_methods\n                if method in self._cors_config['allow_methods_list']\n            ]\n            self._set_allowed_methods(resp, allowed_methods)\n            if requested_method in allowed_methods:\n                return True\n\n        return False",
    "reference": "Adds the Access-Control-Allow-Methods header to the response,\n        using the cors settings to determine which methods are allowed.",
    "generated": "Adds the Access-Control-Allow-Methods header to the response,\n        using the cors settings to determine which methods are allowed."
  },
  {
    "code": "def _process_credentials(self, req, resp, origin):\n        \"\"\"Adds the Access-Control-Allow-Credentials to the response\n        if the cors settings indicates it should be set.\n        \"\"\"\n        if self._cors_config['allow_credentials_all_origins']:\n            self._set_allow_credentials(resp)\n            return True\n\n        if origin in self._cors_config['allow_credentials_origins_list']:\n            self._set_allow_credentials(resp)\n            return True\n\n        credentials_regex = self._cors_config['allow_credentials_origins_regex']\n        if credentials_regex:\n            if credentials_regex.match(origin):\n                self._set_allow_credentials(resp)\n                return True\n\n        return False",
    "reference": "Adds the Access-Control-Allow-Credentials to the response\n        if the cors settings indicates it should be set.",
    "generated": "Adds the Access-Control-Allow-Credentials to the response\n        if the cors settings indicates it should be set."
  },
  {
    "code": "def _send(self, email_message):\n        \"\"\"Sends an individual message via the Amazon SES HTTP API.\n\n        Args:\n            email_message: A single Django EmailMessage object.\n        Returns:\n            True if the EmailMessage was sent successfully, otherwise False.\n        Raises:\n            ClientError: An interaction with the Amazon SES HTTP API\n                failed.\n        \"\"\"\n        pre_send.send(self.__class__, message=email_message)\n\n        if not email_message.recipients():\n            return False\n\n        from_email = sanitize_address(email_message.from_email,\n                                      email_message.encoding)\n        recipients = [sanitize_address(addr, email_message.encoding)\n                      for addr in email_message.recipients()]\n        message = email_message.message().as_bytes(linesep='\\r\\n')\n\n        try:\n            result = self.conn.send_raw_email(\n                Source=from_email,\n                Destinations=recipients,\n                RawMessage={\n                    'Data': message\n                }\n            )\n            message_id = result['MessageId']\n            post_send.send(\n                self.__class__,\n                message=email_message,\n                message_id=message_id\n            )\n        except ClientError:\n            if not self.fail_silently:\n                raise\n            return False\n        return True",
    "reference": "Sends an individual message via the Amazon SES HTTP API.\n\n        Args:\n            email_message: A single Django EmailMessage object.\n        Returns:\n            True if the EmailMessage was sent successfully, otherwise False.\n        Raises:\n            ClientError: An interaction with the Amazon SES HTTP API\n                failed.",
    "generated": "Sends an individual message via the Amazon SES HTTP API.\n\n        Args:\n            email_message: A single Django EmailMessage object.\n        Returns:\n            True if the EmailMessage was sent successfully, otherwise False.\n        Raises:\n            ClientError: An interaction with the Amazon SES HTTP API\n                failed."
  },
  {
    "code": "def was_modified_since(header=None, mtime=0, size=0):\n    \"\"\"\n    Was something modified since the user last downloaded it?\n    header\n      This is the value of the If-Modified-Since header.  If this is None,\n      I'll just return True.\n    mtime\n      This is the modification time of the item we're talking about.\n    size\n      This is the size of the item we're talking about.\n    \"\"\"\n    try:\n        if header is None:\n            raise ValueError\n        matches = re.match(r\"^([^;]+)(; length=([0-9]+))?$\", header,\n                           re.IGNORECASE)\n        header_mtime = parse_http_date(matches.group(1))\n        header_len = matches.group(3)\n        if header_len and int(header_len) != size:\n            raise ValueError\n        if int(mtime) > header_mtime:\n            raise ValueError\n    except (AttributeError, ValueError, OverflowError):\n        return True\n    return False",
    "reference": "Was something modified since the user last downloaded it?\n    header\n      This is the value of the If-Modified-Since header.  If this is None,\n      I'll just return True.\n    mtime\n      This is the modification time of the item we're talking about.\n    size\n      This is the size of the item we're talking about.",
    "generated": "Was something modified since the user last downloaded it?\n    header\n      This is the modification time of the item we're talking about.\n    size\n      This is the size of the item we're talking about."
  },
  {
    "code": "def unpublish_object(content_type_pk, obj_pk):\n    \"\"\"\n    Unbuild all views related to a object and then sync to S3.\n\n    Accepts primary keys to retrieve a model object that\n    inherits bakery's BuildableModel class.\n    \"\"\"\n    ct = ContentType.objects.get_for_id(content_type_pk)\n    obj = ct.get_object_for_this_type(pk=obj_pk)\n    try:\n        # Unbuild the object\n        logger.info(\"unpublish_object task has received %s\" % obj)\n        obj.unbuild()\n        # Run the `publish` management command unless the\n        # ALLOW_BAKERY_AUTO_PUBLISHING variable is explictly set to False.\n        if not getattr(settings, 'ALLOW_BAKERY_AUTO_PUBLISHING', True):\n            logger.info(\"Not running publish command because \\\nALLOW_BAKERY_AUTO_PUBLISHING is False\")\n        else:\n            management.call_command(\"publish\")\n    except Exception:\n        # Log the error if this crashes\n        logger.error(\"Task Error: unpublish_object\", exc_info=True)",
    "reference": "Unbuild all views related to a object and then sync to S3.\n\n    Accepts primary keys to retrieve a model object that\n    inherits bakery's BuildableModel class.",
    "generated": "Unbuild all views related to a object and then sync to S3.\n\n    Accepts primary keys to retrieve a model object that\n    inherits bakery's BuildableModel class."
  },
  {
    "code": "def prep_directory(self, target_dir):\n        \"\"\"\n        Prepares a new directory to store the file at the provided path, if needed.\n        \"\"\"\n        dirname = path.dirname(target_dir)\n        if dirname:\n            dirname = path.join(settings.BUILD_DIR, dirname)\n            if not self.fs.exists(dirname):\n                logger.debug(\"Creating directory at {}{}\".format(self.fs_name, dirname))\n                self.fs.makedirs(dirname)",
    "reference": "Prepares a new directory to store the file at the provided path, if needed.",
    "generated": "Prepares a new directory to store the file at the provided path, if needed."
  },
  {
    "code": "def write_file(self, target_path, html):\n        \"\"\"\n        Writes out the provided HTML to the provided path.\n        \"\"\"\n        logger.debug(\"Building to {}{}\".format(self.fs_name, target_path))\n        with self.fs.open(smart_text(target_path), 'wb') as outfile:\n            outfile.write(six.binary_type(html))\n            outfile.close()",
    "reference": "Writes out the provided HTML to the provided path.",
    "generated": "Write out the provided HTML to the provided path."
  },
  {
    "code": "def is_gzippable(self, path):\n        \"\"\"\n        Returns a boolean indicating if the provided file path is a candidate\n        for gzipping.\n        \"\"\"\n        # First check if gzipping is allowed by the global setting\n        if not getattr(settings, 'BAKERY_GZIP', False):\n            return False\n        # Then check if the content type of this particular file is gzippable\n        whitelist = getattr(\n            settings,\n            'GZIP_CONTENT_TYPES',\n            DEFAULT_GZIP_CONTENT_TYPES\n        )\n        return mimetypes.guess_type(path)[0] in whitelist",
    "reference": "Returns a boolean indicating if the provided file path is a candidate\n        for gzipping.",
    "generated": "Check if the provided file path is a candidate\n        for gzipping."
  },
  {
    "code": "def gzip_file(self, target_path, html):\n        \"\"\"\n        Zips up the provided HTML as a companion for the provided path.\n\n        Intended to take advantage of the peculiarities of\n        Amazon S3's GZIP service.\n\n        mtime, an option that writes a timestamp to the output file\n        is set to 0, to avoid having s3cmd do unnecessary uploads because\n        of differences in the timestamp\n        \"\"\"\n        logger.debug(\"Gzipping to {}{}\".format(self.fs_name, target_path))\n\n        # Write GZIP data to an in-memory buffer\n        data_buffer = six.BytesIO()\n        kwargs = dict(\n            filename=path.basename(target_path),\n            mode='wb',\n            fileobj=data_buffer\n        )\n        if float(sys.version[:3]) >= 2.7:\n            kwargs['mtime'] = 0\n        with gzip.GzipFile(**kwargs) as f:\n            f.write(six.binary_type(html))\n\n        # Write that buffer out to the filesystem\n        with self.fs.open(smart_text(target_path), 'wb') as outfile:\n            outfile.write(data_buffer.getvalue())\n            outfile.close()",
    "reference": "Zips up the provided HTML as a companion for the provided path.\n\n        Intended to take advantage of the peculiarities of\n        Amazon S3's GZIP service.\n\n        mtime, an option that writes a timestamp to the output file\n        is set to 0, to avoid having s3cmd do unnecessary uploads because\n        of differences in the timestamp",
    "generated": "Write the provided HTML as a companion for the provided path.\n\n        Intended to take advantage of the peculiarities of\n        Amazon S3's GZIP service.\n\n        mtime, an option that writes a timestamp to the output file\n        is set to 0, to avoid having s3cmd do unnecessary uploads because\n        of differences in the timestamp\n       "
  },
  {
    "code": "def save(self, *args, **kwargs):\n        \"\"\"\n        A custom save that publishes or unpublishes the object where\n        appropriate.\n\n        Save with keyword argument obj.save(publish=False) to skip the process.\n        \"\"\"\n        from bakery import tasks\n        from django.contrib.contenttypes.models import ContentType\n        # if obj.save(publish=False) has been passed, we skip everything.\n        if not kwargs.pop('publish', True):\n            super(AutoPublishingBuildableModel, self).save(*args, **kwargs)\n        # Otherwise, for the standard obj.save(), here we go...\n        else:\n            # First figure out if the record is an addition, or an edit of\n            # a preexisting record.\n            try:\n                preexisting = self.__class__.objects.get(pk=self.pk)\n            except self.__class__.DoesNotExist:\n                preexisting = None\n            # If this is an addition...\n            if not preexisting:\n                # We will publish if that's the boolean\n                if self.get_publication_status():\n                    action = 'publish'\n                # Otherwise we will do nothing do nothing\n                else:\n                    action = None\n            # If this is an edit...\n            else:\n                # If it's being unpublished...\n                if not self.get_publication_status() and \\\n                        preexisting.get_publication_status():\n                    action = 'unpublish'\n                # If it's being published...\n                elif self.get_publication_status():\n                    action = 'publish'\n                # If it's remaining unpublished...\n                else:\n                    action = None\n            # Now, no matter what, save it normally inside of a dedicated\n            # database transaction so that we are sure that the save will\n            # be complete before we trigger any task\n            with transaction.atomic():\n                super(AutoPublishingBuildableModel, self).save(*args, **kwargs)\n            # Finally, depending on the action, fire off a task\n            ct = ContentType.objects.get_for_model(self.__class__)\n            if action == 'publish':\n                tasks.publish_object.delay(ct.pk, self.pk)\n            elif action == 'unpublish':\n                tasks.unpublish_object.delay(ct.pk, self.pk)",
    "reference": "A custom save that publishes or unpublishes the object where\n        appropriate.\n\n        Save with keyword argument obj.save(publish=False) to skip the process.",
    "generated": "A custom save that publishes or unpublishes the object where\n        appropriate.\n\n        Save with keyword argument obj.save(publish=False) to skip the process.\n        \"\"\"\n        A custom save that publishes or unpublishes the object where\n        appropriate.\n\n        Save with keyword argument obj.save(publish=False) to skip the process.\n        \"\"\"\n        A custom save that publishes or unpublishes the object where\n        appropriate.\n\n        Save with keyword argument obj.save(publish=False) to skip the process."
  },
  {
    "code": "def delete(self, *args, **kwargs):\n        \"\"\"\n        Triggers a task that will unpublish the object after it is deleted.\n\n        Save with keyword argument obj.delete(unpublish=False) to skip it.\n        \"\"\"\n        from bakery import tasks\n        from django.contrib.contenttypes.models import ContentType\n        # if obj.save(unpublish=False) has been passed, we skip the task.\n        unpublish = kwargs.pop('unpublish', True)\n        # Delete it from the database\n        super(AutoPublishingBuildableModel, self).delete(*args, **kwargs)\n        if unpublish:\n            ct = ContentType.objects.get_for_model(self.__class__)\n            tasks.unpublish_object.delay(ct.pk, self.pk)",
    "reference": "Triggers a task that will unpublish the object after it is deleted.\n\n        Save with keyword argument obj.delete(unpublish=False) to skip it.",
    "generated": "Deletes the object after it is deleted.\n\n        Save with keyword argument obj.delete(unpublish=False) to skip it."
  },
  {
    "code": "def handle(self, *args, **options):\n        \"\"\"\n        Making it happen.\n        \"\"\"\n        logger.info(\"Build started\")\n\n        # Set options\n        self.set_options(*args, **options)\n\n        # Get the build directory ready\n        if not options.get(\"keep_build_dir\"):\n            self.init_build_dir()\n\n        # Build up static files\n        if not options.get(\"skip_static\"):\n            self.build_static()\n\n        # Build the media directory\n        if not options.get(\"skip_media\"):\n            self.build_media()\n\n        # Build views\n        self.build_views()\n\n        # Close out\n        logger.info(\"Build finished\")",
    "reference": "Making it happen.",
    "generated": "Making it happen."
  },
  {
    "code": "def set_options(self, *args, **options):\n        \"\"\"\n        Configure a few global options before things get going.\n        \"\"\"\n        self.verbosity = int(options.get('verbosity', 1))\n\n        # Figure out what build directory to use\n        if options.get(\"build_dir\"):\n            self.build_dir = options.get(\"build_dir\")\n            settings.BUILD_DIR = self.build_dir\n        else:\n            if not hasattr(settings, 'BUILD_DIR'):\n                raise CommandError(self.build_unconfig_msg)\n            self.build_dir = settings.BUILD_DIR\n\n        # Get the datatypes right so fs will be happy\n        self.build_dir = smart_text(self.build_dir)\n        self.static_root = smart_text(settings.STATIC_ROOT)\n        self.media_root = smart_text(settings.MEDIA_ROOT)\n\n        # Connect the BUILD_DIR with our filesystem backend\n        self.app = apps.get_app_config(\"bakery\")\n        self.fs = self.app.filesystem\n        self.fs_name = self.app.filesystem_name\n\n        # If the build dir doesn't exist make it\n        if not self.fs.exists(self.build_dir):\n            self.fs.makedirs(self.build_dir)\n\n        # Figure out what views we'll be using\n        if options.get('view_list'):\n            self.view_list = options['view_list']\n        else:\n            if not hasattr(settings, 'BAKERY_VIEWS'):\n                raise CommandError(self.views_unconfig_msg)\n            self.view_list = settings.BAKERY_VIEWS\n\n        # Are we pooling?\n        self.pooling = options.get('pooling')",
    "reference": "Configure a few global options before things get going.",
    "generated": "Configure a few global options before things get going."
  },
  {
    "code": "def init_build_dir(self):\n        \"\"\"\n        Clear out the build directory and create a new one.\n        \"\"\"\n        # Destroy the build directory, if it exists\n        logger.debug(\"Initializing %s\" % self.build_dir)\n        if self.verbosity > 1:\n            self.stdout.write(\"Initializing build directory\")\n        if self.fs.exists(self.build_dir):\n            self.fs.removetree(self.build_dir)\n        # Then recreate it from scratch\n        self.fs.makedirs(self.build_dir)",
    "reference": "Clear out the build directory and create a new one.",
    "generated": "Clear out the build directory and create a new one."
  },
  {
    "code": "def build_static(self, *args, **options):\n        \"\"\"\n        Builds the static files directory as well as robots.txt and favicon.ico\n        \"\"\"\n        logger.debug(\"Building static directory\")\n        if self.verbosity > 1:\n            self.stdout.write(\"Building static directory\")\n        management.call_command(\n            \"collectstatic\",\n            interactive=False,\n            verbosity=0\n        )\n\n        # Set the target directory inside the filesystem.\n        target_dir = path.join(\n            self.build_dir,\n            settings.STATIC_URL.lstrip('/')\n        )\n        target_dir = smart_text(target_dir)\n\n        if os.path.exists(self.static_root) and settings.STATIC_URL:\n            if getattr(settings, 'BAKERY_GZIP', False):\n                self.copytree_and_gzip(self.static_root, target_dir)\n            # if gzip isn't enabled, just copy the tree straight over\n            else:\n                logger.debug(\"Copying {}{} to {}{}\".format(\"osfs://\", self.static_root, self.fs_name, target_dir))\n                copy.copy_dir(\"osfs:///\", self.static_root, self.fs, target_dir)\n\n        # If they exist in the static directory, copy the robots.txt\n        # and favicon.ico files down to the root so they will work\n        # on the live website.\n        robots_src = path.join(target_dir, 'robots.txt')\n        if self.fs.exists(robots_src):\n            robots_target = path.join(self.build_dir, 'robots.txt')\n            logger.debug(\"Copying {}{} to {}{}\".format(self.fs_name, robots_src, self.fs_name, robots_target))\n            self.fs.copy(robots_src, robots_target)\n\n        favicon_src = path.join(target_dir, 'favicon.ico')\n        if self.fs.exists(favicon_src):\n            favicon_target = path.join(self.build_dir, 'favicon.ico')\n            logger.debug(\"Copying {}{} to {}{}\".format(self.fs_name, favicon_src, self.fs_name, favicon_target))\n            self.fs.copy(favicon_src, favicon_target)",
    "reference": "Builds the static files directory as well as robots.txt and favicon.ico",
    "generated": "Builds the static files directory as well as robots.txt and favicon.ico\n        logger.debug(\"Building static directory\")\n        management.call_command(\n            \"collectstatic\",\n            interactive=False,\n            verbosity=0\n        )"
  },
  {
    "code": "def build_media(self):\n        \"\"\"\n        Build the media files.\n        \"\"\"\n        logger.debug(\"Building media directory\")\n        if self.verbosity > 1:\n            self.stdout.write(\"Building media directory\")\n        if os.path.exists(self.media_root) and settings.MEDIA_URL:\n            target_dir = path.join(self.fs_name, self.build_dir, settings.MEDIA_URL.lstrip('/'))\n            logger.debug(\"Copying {}{} to {}{}\".format(\"osfs://\", self.media_root, self.fs_name, target_dir))\n            copy.copy_dir(\"osfs:///\", smart_text(self.media_root), self.fs, smart_text(target_dir))",
    "reference": "Build the media files.",
    "generated": "Build the media files."
  },
  {
    "code": "def build_views(self):\n        \"\"\"\n        Bake out specified buildable views.\n        \"\"\"\n        # Then loop through and run them all\n        for view_str in self.view_list:\n            logger.debug(\"Building %s\" % view_str)\n            if self.verbosity > 1:\n                self.stdout.write(\"Building %s\" % view_str)\n            view = get_callable(view_str)\n            self.get_view_instance(view).build_method()",
    "reference": "Bake out specified buildable views.",
    "generated": "Bake out specified buildable views."
  },
  {
    "code": "def copytree_and_gzip(self, source_dir, target_dir):\n        \"\"\"\n        Copies the provided source directory to the provided target directory.\n\n        Gzips JavaScript, CSS and HTML and other files along the way.\n        \"\"\"\n        # Figure out what we're building...\n        build_list = []\n        # Walk through the source directory...\n        for (dirpath, dirnames, filenames) in os.walk(source_dir):\n            for f in filenames:\n                # Figure out what is going where\n                source_path = os.path.join(dirpath, f)\n                rel_path = os.path.relpath(dirpath, source_dir)\n                target_path = os.path.join(target_dir, rel_path, f)\n                # Add it to our list to build\n                build_list.append((source_path, target_path))\n\n        logger.debug(\"Gzipping {} files\".format(len(build_list)))\n\n        # Build em all\n        if not getattr(self, 'pooling', False):\n            [self.copyfile_and_gzip(*u) for u in build_list]\n        else:\n            cpu_count = multiprocessing.cpu_count()\n            logger.debug(\"Pooling build on {} CPUs\".format(cpu_count))\n            pool = ThreadPool(processes=cpu_count)\n            pool.map(self.pooled_copyfile_and_gzip, build_list)",
    "reference": "Copies the provided source directory to the provided target directory.\n\n        Gzips JavaScript, CSS and HTML and other files along the way.",
    "generated": "Copyies the provided source directory to the provided target directory.\n\n        Gzips JavaScript, CSS and HTML and other files along the way."
  },
  {
    "code": "def copyfile_and_gzip(self, source_path, target_path):\n        \"\"\"\n        Copies the provided file to the provided target directory.\n\n        Gzips JavaScript, CSS and HTML and other files along the way.\n        \"\"\"\n        # And then where we want to copy it to.\n        target_dir = path.dirname(target_path)\n        if not self.fs.exists(target_dir):\n            try:\n                self.fs.makedirs(target_dir)\n            except OSError:\n                pass\n\n        # determine the mimetype of the file\n        guess = mimetypes.guess_type(source_path)\n        content_type = guess[0]\n        encoding = guess[1]\n\n        # If it isn't a file want to gzip...\n        if content_type not in self.gzip_file_match:\n            # just copy it to the target.\n            logger.debug(\"Copying {}{} to {}{} because its filetype isn't on the whitelist\".format(\n                \"osfs://\",\n                source_path,\n                self.fs_name,\n                target_path\n            ))\n            copy.copy_file(\"osfs:///\", smart_text(source_path), self.fs, smart_text(target_path))\n\n        # # if the file is already gzipped\n        elif encoding == 'gzip':\n            logger.debug(\"Copying {}{} to {}{} because it's already gzipped\".format(\n                \"osfs://\",\n                source_path,\n                self.fs_name,\n                target_path\n            ))\n            copy.copy_file(\"osfs:///\", smart_text(source_path), self.fs, smart_text(target_path))\n\n        # If it is one we want to gzip...\n        else:\n            # ... let the world know ...\n            logger.debug(\"Gzipping {}{} to {}{}\".format(\n                \"osfs://\",\n                source_path,\n                self.fs_name,\n                target_path\n            ))\n            # Open up the source file from the OS\n            with open(source_path, 'rb') as source_file:\n                # Write GZIP data to an in-memory buffer\n                data_buffer = six.BytesIO()\n                kwargs = dict(\n                    filename=path.basename(target_path),\n                    mode='wb',\n                    fileobj=data_buffer\n                )\n                if float(sys.version[:3]) >= 2.7:\n                    kwargs['mtime'] = 0\n                with gzip.GzipFile(**kwargs) as f:\n                    f.write(six.binary_type(source_file.read()))\n\n                # Write that buffer out to the filesystem\n                with self.fs.open(smart_text(target_path), 'wb') as outfile:\n                    outfile.write(data_buffer.getvalue())\n                    outfile.close()",
    "reference": "Copies the provided file to the provided target directory.\n\n        Gzips JavaScript, CSS and HTML and other files along the way.",
    "generated": "Copyies the provided file to the provided target directory.\n\n        Gzips JavaScript, CSS and HTML and other files along the way."
  },
  {
    "code": "def set_options(self, options):\n        \"\"\"\n        Configure all the many options we'll need to make this happen.\n        \"\"\"\n        self.verbosity = int(options.get('verbosity'))\n\n        # Will we be gzipping?\n        self.gzip = getattr(settings, 'BAKERY_GZIP', False)\n\n        # And if so what content types will we be gzipping?\n        self.gzip_content_types = getattr(\n            settings,\n            'GZIP_CONTENT_TYPES',\n            DEFAULT_GZIP_CONTENT_TYPES\n        )\n\n        # What ACL (i.e. security permissions) will be giving the files on S3?\n        self.acl = getattr(settings, 'DEFAULT_ACL', self.DEFAULT_ACL)\n\n        # Should we set cache-control headers?\n        self.cache_control = getattr(settings, 'BAKERY_CACHE_CONTROL', {})\n\n        # If the user specifies a build directory...\n        if options.get('build_dir'):\n            # ... validate that it is good.\n            if not os.path.exists(options.get('build_dir')):\n                raise CommandError(self.build_missing_msg)\n            # Go ahead and use it\n            self.build_dir = options.get(\"build_dir\")\n        # If the user does not specify a build dir...\n        else:\n            # Check if it is set in settings.py\n            if not hasattr(settings, 'BUILD_DIR'):\n                raise CommandError(self.build_unconfig_msg)\n            # Then make sure it actually exists\n            if not os.path.exists(settings.BUILD_DIR):\n                raise CommandError(self.build_missing_msg)\n            # Go ahead and use it\n            self.build_dir = settings.BUILD_DIR\n\n        # If the user provides a bucket name, use that.\n        if options.get(\"aws_bucket_name\"):\n            self.aws_bucket_name = options.get(\"aws_bucket_name\")\n        else:\n            # Otherwise try to find it the settings\n            if not hasattr(settings, 'AWS_BUCKET_NAME'):\n                raise CommandError(self.bucket_unconfig_msg)\n            self.aws_bucket_name = settings.AWS_BUCKET_NAME\n\n        # The bucket prefix, if it exists\n        self.aws_bucket_prefix = options.get(\"aws_bucket_prefix\")\n\n        # If the user sets the --force option\n        if options.get('force'):\n            self.force_publish = True\n        else:\n            self.force_publish = False\n\n        # set the --dry-run option\n        if options.get('dry_run'):\n            self.dry_run = True\n            if self.verbosity > 0:\n                logger.info(\"Executing with the --dry-run option set.\")\n        else:\n            self.dry_run = False\n\n        self.no_delete = options.get('no_delete')\n        self.no_pooling = options.get('no_pooling')",
    "reference": "Configure all the many options we'll need to make this happen.",
    "generated": "Configure all the many options we'll need to make this happen."
  },
  {
    "code": "def get_local_file_list(self):\n        \"\"\"\n        Walk the local build directory and create a list of relative and\n        absolute paths to files.\n        \"\"\"\n        file_list = []\n        for (dirpath, dirnames, filenames) in os.walk(self.build_dir):\n            for fname in filenames:\n                # relative path, to sync with the S3 key\n                local_key = os.path.join(\n                    os.path.relpath(dirpath, self.build_dir),\n                    fname\n                )\n                if local_key.startswith('./'):\n                    local_key = local_key[2:]\n                file_list.append(local_key)\n        return file_list",
    "reference": "Walk the local build directory and create a list of relative and\n        absolute paths to files.",
    "generated": "Walk the local build directory and create a list of relative and\n        absolute paths to files."
  },
  {
    "code": "def sync_with_s3(self):\n        \"\"\"\n        Walk through our self.local_files list, and match them with the list\n        of keys in the S3 bucket.\n        \"\"\"\n        # Create a list to put all the files we're going to update\n        self.update_list = []\n\n        # Figure out which files need to be updated and upload all these files\n        logger.debug(\"Comparing {} local files with {} bucket files\".format(\n            len(self.local_file_list),\n            len(self.s3_obj_dict.keys())\n        ))\n        if self.no_pooling:\n            [self.compare_local_file(f) for f in self.local_file_list]\n        else:\n            cpu_count = multiprocessing.cpu_count()\n            logger.debug(\"Pooling local file comparison on {} CPUs\".format(cpu_count))\n            pool = ThreadPool(processes=cpu_count)\n            pool.map(self.compare_local_file, self.local_file_list)\n\n        logger.debug(\"Uploading {} new or updated files to bucket\".format(len(self.update_list)))\n        if self.no_pooling:\n            [self.upload_to_s3(*u) for u in self.update_list]\n        else:\n            logger.debug(\"Pooling s3 uploads on {} CPUs\".format(cpu_count))\n            pool = ThreadPool(processes=cpu_count)\n            pool.map(self.pooled_upload_to_s3, self.update_list)",
    "reference": "Walk through our self.local_files list, and match them with the list\n        of keys in the S3 bucket.",
    "generated": "Walk through our self.local_files list, and match them with the list\n        of keys in the S3 bucket."
  },
  {
    "code": "def get_md5(self, filename):\n        \"\"\"\n        Returns the md5 checksum of the provided file name.\n        \"\"\"\n        with open(filename, 'rb') as f:\n            m = hashlib.md5(f.read())\n        return m.hexdigest()",
    "reference": "Returns the md5 checksum of the provided file name.",
    "generated": "Returns the md5 checksum of the provided file name."
  },
  {
    "code": "def get_multipart_md5(self, filename, chunk_size=8 * 1024 * 1024):\n        \"\"\"\n        Returns the md5 checksum of the provided file name after breaking it into chunks.\n\n        This is done to mirror the method used by Amazon S3 after a multipart upload.\n        \"\"\"\n        # Loop through the file contents ...\n        md5s = []\n        with open(filename, 'rb') as fp:\n            while True:\n                # Break it into chunks\n                data = fp.read(chunk_size)\n                # Finish when there are no more\n                if not data:\n                    break\n                # Generate a md5 hash for each chunk\n                md5s.append(hashlib.md5(data))\n\n        # Combine the chunks\n        digests = b\"\".join(m.digest() for m in md5s)\n\n        # Generate a new hash using them\n        new_md5 = hashlib.md5(digests)\n\n        # Create the ETag as Amazon will\n        new_etag = '\"%s-%s\"' % (new_md5.hexdigest(), len(md5s))\n\n        # Trim it down and pass it back for comparison\n        return new_etag.strip('\"').strip(\"'\")",
    "reference": "Returns the md5 checksum of the provided file name after breaking it into chunks.\n\n        This is done to mirror the method used by Amazon S3 after a multipart upload.",
    "generated": "Returns the md5 checksum of the provided file name after breaking it into chunks.\n\n        This is done to mirror the method used by Amazon S3 after a multipart upload."
  },
  {
    "code": "def compare_local_file(self, file_key):\n        \"\"\"\n        Compares a local version of a file with what's already published.\n\n        If an update is needed, the file's key is added self.update_list.\n        \"\"\"\n        # Where is the file?\n        file_path = os.path.join(self.build_dir, file_key)\n\n        # If we're in force_publish mode just add it\n        if self.force_publish:\n            self.update_list.append((file_key, file_path))\n            # And quit now\n            return\n\n        # Does it exist in our s3 object list?\n        if file_key in self.s3_obj_dict:\n\n            # Get the md5 stored in Amazon's header\n            s3_md5 = self.s3_obj_dict[file_key].get('ETag').strip('\"').strip(\"'\")\n\n            # If there is a multipart ETag on S3, compare that to our local file after its chunked up.\n            # We are presuming this file was uploaded in multiple parts.\n            if \"-\" in s3_md5:\n                local_md5 = self.get_multipart_md5(file_path)\n            # Other, do it straight for the whole file\n            else:\n                local_md5 = self.get_md5(file_path)\n\n            # If their md5 hexdigests match, do nothing\n            if s3_md5 == local_md5:\n                pass\n            # If they don't match, we want to add it\n            else:\n                logger.debug(\"{} has changed\".format(file_key))\n                self.update_list.append((file_key, file_path))\n\n            # Remove the file from the s3 dict, we don't need it anymore\n            del self.s3_obj_dict[file_key]\n\n        # If the file doesn't exist, queue it for creation\n        else:\n            logger.debug(\"{} has been added\".format(file_key))\n            self.update_list.append((file_key, file_path))",
    "reference": "Compares a local version of a file with what's already published.\n\n        If an update is needed, the file's key is added self.update_list.",
    "generated": "Compares a local version of a file with what's already published.\n\n        If an update is needed, the file's key is added self.update_list.\n        \"\"\"\n        Compares a local version of a file with what's already published.\n\n        If an update is needed, the file's key is added self.update_list."
  },
  {
    "code": "def upload_to_s3(self, key, filename):\n        \"\"\"\n        Set the content type and gzip headers if applicable\n        and upload the item to S3\n        \"\"\"\n        extra_args = {'ACL': self.acl}\n        # determine the mimetype of the file\n        guess = mimetypes.guess_type(filename)\n        content_type = guess[0]\n        encoding = guess[1]\n\n        if content_type:\n            extra_args['ContentType'] = content_type\n\n        # add the gzip headers, if necessary\n        if (self.gzip and content_type in self.gzip_content_types) or encoding == 'gzip':\n            extra_args['ContentEncoding'] = 'gzip'\n\n        # add the cache-control headers if necessary\n        if content_type in self.cache_control:\n            extra_args['CacheControl'] = ''.join((\n                'max-age=',\n                str(self.cache_control[content_type])\n            ))\n\n        # access and write the contents from the file\n        if not self.dry_run:\n            logger.debug(\"Uploading %s\" % filename)\n            if self.verbosity > 0:\n                self.stdout.write(\"Uploading %s\" % filename)\n            s3_obj = self.s3_resource.Object(self.aws_bucket_name, key)\n            s3_obj.upload_file(filename, ExtraArgs=extra_args)\n\n        # Update counts\n        self.uploaded_files += 1\n        self.uploaded_file_list.append(filename)",
    "reference": "Set the content type and gzip headers if applicable\n        and upload the item to S3",
    "generated": "Set the content type and gzip headers if applicable\n        and upload the item to S3\n       "
  },
  {
    "code": "def build_year(self, dt):\n        \"\"\"\n        Build the page for the provided year.\n        \"\"\"\n        self.year = str(dt.year)\n        logger.debug(\"Building %s\" % self.year)\n        self.request = self.create_request(self.get_url())\n        target_path = self.get_build_path()\n        self.build_file(target_path, self.get_content())",
    "reference": "Build the page for the provided year.",
    "generated": "Build the page for the provided year."
  },
  {
    "code": "def get_day(self):\n        \"\"\"\n        Return the day from the database in the format expected by the URL.\n        \"\"\"\n        year = super(BuildableDayArchiveView, self).get_year()\n        month = super(BuildableDayArchiveView, self).get_month()\n        day = super(BuildableDayArchiveView, self).get_day()\n        fmt = self.get_day_format()\n        dt = date(int(year), int(month), int(day))\n        return dt.strftime(fmt)",
    "reference": "Return the day from the database in the format expected by the URL.",
    "generated": "Return the day from the database in the format expected by the URL."
  },
  {
    "code": "def build_day(self, dt):\n        \"\"\"\n        Build the page for the provided day.\n        \"\"\"\n        self.month = str(dt.month)\n        self.year = str(dt.year)\n        self.day = str(dt.day)\n        logger.debug(\"Building %s-%s-%s\" % (self.year, self.month, self.day))\n        self.request = self.create_request(self.get_url())\n        path = self.get_build_path()\n        self.build_file(path, self.get_content())",
    "reference": "Build the page for the provided day.",
    "generated": "Build the page for the provided day."
  },
  {
    "code": "def get_bucket_page(page):\n    \"\"\"\n    Returns all the keys in a s3 bucket paginator page.\n    \"\"\"\n    key_list = page.get('Contents', [])\n    logger.debug(\"Retrieving page with {} keys\".format(\n        len(key_list),\n    ))\n    return dict((k.get('Key'), k) for k in key_list)",
    "reference": "Returns all the keys in a s3 bucket paginator page.",
    "generated": "Returns all the keys in a s3 bucket paginator page."
  },
  {
    "code": "def batch_delete_s3_objects(\n        keys,\n        aws_bucket_name,\n        chunk_size=100,\n        s3_client=None\n):\n    \"\"\"\n    Utility method that batch deletes objects in given bucket.\n    \"\"\"\n    if s3_client is None:\n        s3_client, s3_resource = get_s3_client()\n\n    key_chunks = []\n    for i in range(0, len(keys), chunk_size):\n        chunk = []\n        for key in (list(keys)[i:i+100]):\n            chunk.append({'Key': key})\n        key_chunks.append(chunk)\n    for chunk in key_chunks:\n        s3_client.delete_objects(\n            Bucket=aws_bucket_name,\n            Delete={'Objects': chunk}\n        )",
    "reference": "Utility method that batch deletes objects in given bucket.",
    "generated": "Batch deletes objects in given bucket."
  },
  {
    "code": "def is_present(p):\n    \"\"\"\n    Given a parser or string, make a parser that returns\n    True if the parser matches, False otherwise\n    \"\"\"\n    return lexeme(p).optional().map(lambda v: False if v is None else True)",
    "reference": "Given a parser or string, make a parser that returns\n    True if the parser matches, False otherwise",
    "generated": "Given a parser or string, make a parser that returns\n    True if the parser matches, False otherwise"
  },
  {
    "code": "def parse(self, stream):\n        \"\"\"Parse a string or list of tokens and return the result or raise a ParseError.\"\"\"\n        (result, _) = (self << eof).parse_partial(stream)\n        return result",
    "reference": "Parse a string or list of tokens and return the result or raise a ParseError.",
    "generated": "Parse a string or list of tokens and return the result"
  },
  {
    "code": "def parse_partial(self, stream):\n        \"\"\"\n        Parse the longest possible prefix of a given string.\n        Return a tuple of the result and the rest of the string,\n        or raise a ParseError.\n        \"\"\"\n        result = self(stream, 0)\n\n        if result.status:\n            return (result.value, stream[result.index:])\n        else:\n            raise ParseError(result.expected, stream, result.furthest)",
    "reference": "Parse the longest possible prefix of a given string.\n        Return a tuple of the result and the rest of the string,\n        or raise a ParseError.",
    "generated": "Parse the longest possible prefix of a given string.\n        Return a tuple of the result and the rest of the string,"
  },
  {
    "code": "def extract_key_values(array_value, separators=(';', ',', ':'), **kwargs):\n    \"\"\"Serialize array of objects with simple key-values\n    \"\"\"\n    items_sep, fields_sep, keys_sep = separators\n    return items_sep.join(fields_sep.join(keys_sep.join(x) for x in sorted(it.items()))\n                          for it in array_value)",
    "reference": "Serialize array of objects with simple key-values",
    "generated": "Serialize array of objects with simple key-values"
  },
  {
    "code": "def from_schemafile(cls, schemafile):\n        \"\"\"Create a Flatson instance from a schemafile\n        \"\"\"\n        with open(schemafile) as f:\n            return cls(json.load(f))",
    "reference": "Create a Flatson instance from a schemafile",
    "generated": "Create a Flatson instance from a schemafile\n        \"\"\"Create a Flatson instance from a schemafile"
  },
  {
    "code": "def register_serialization_method(self, name, serialize_func):\n        \"\"\"Register a custom serialization method that can be\n        used via schema configuration\n        \"\"\"\n        if name in self._default_serialization_methods:\n            raise ValueError(\"Can't replace original %s serialization method\")\n        self._serialization_methods[name] = serialize_func",
    "reference": "Register a custom serialization method that can be\n        used via schema configuration",
    "generated": "Register a custom serialization method that can be\n        used via schema configuration"
  },
  {
    "code": "def flatten(self, obj):\n        \"\"\"Return a list with the field values\n        \"\"\"\n        return [self._serialize(f, obj) for f in self.fields]",
    "reference": "Return a list with the field values",
    "generated": "Return a list with the field values"
  },
  {
    "code": "def flatten_dict(self, obj):\n        \"\"\"Return an OrderedDict dict preserving order of keys in fieldnames\n        \"\"\"\n        return OrderedDict(zip(self.fieldnames, self.flatten(obj)))",
    "reference": "Return an OrderedDict dict preserving order of keys in fieldnames",
    "generated": "Return an OrderedDict dict preserving order of keys in fieldnames\n       "
  },
  {
    "code": "def busy(self):\n        \"\"\"Return if the connection is currently executing a query or is locked\n        by a session that still exists.\n\n        :rtype: bool\n\n        \"\"\"\n        if self.handle.isexecuting():\n            return True\n        elif self.used_by is None:\n            return False\n        return not self.used_by() is None",
    "reference": "Return if the connection is currently executing a query or is locked\n        by a session that still exists.\n\n        :rtype: bool",
    "generated": "Return if the connection is currently executing a query or is locked\n        by a session that still exists."
  },
  {
    "code": "def free(self):\n        \"\"\"Remove the lock on the connection if the connection is not active\n\n        :raises: ConnectionBusyError\n\n        \"\"\"\n        LOGGER.debug('Connection %s freeing', self.id)\n        if self.handle.isexecuting():\n            raise ConnectionBusyError(self)\n        with self._lock:\n            self.used_by = None\n        LOGGER.debug('Connection %s freed', self.id)",
    "reference": "Remove the lock on the connection if the connection is not active\n\n        :raises: ConnectionBusyError",
    "generated": "Remove the lock on the connection if the connection is not active\n\n        :raises: ConnectionBusyError"
  },
  {
    "code": "def lock(self, session):\n        \"\"\"Lock the connection, ensuring that it is not busy and storing\n        a weakref for the session.\n\n        :param queries.Session session: The session to lock the connection with\n        :raises: ConnectionBusyError\n\n        \"\"\"\n        if self.busy:\n            raise ConnectionBusyError(self)\n        with self._lock:\n            self.used_by = weakref.ref(session)\n        LOGGER.debug('Connection %s locked', self.id)",
    "reference": "Lock the connection, ensuring that it is not busy and storing\n        a weakref for the session.\n\n        :param queries.Session session: The session to lock the connection with\n        :raises: ConnectionBusyError",
    "generated": "Lock the connection, ensuring that it is busy and storing\n        a weakref for the session.\n\n        :param queries.Session session: The session to lock the connection with\n        :raises: ConnectionBusyError"
  },
  {
    "code": "def add(self, connection):\n        \"\"\"Add a new connection to the pool\n\n        :param connection: The connection to add to the pool\n        :type connection: psycopg2.extensions.connection\n        :raises: PoolFullError\n\n        \"\"\"\n        if id(connection) in self.connections:\n            raise ValueError('Connection already exists in pool')\n\n        if len(self.connections) == self.max_size:\n            LOGGER.warning('Race condition found when adding new connection')\n            try:\n                connection.close()\n            except (psycopg2.Error, psycopg2.Warning) as error:\n                LOGGER.error('Error closing the conn that cant be used: %s',\n                             error)\n            raise PoolFullError(self)\n        with self._lock:\n            self.connections[id(connection)] = Connection(connection)\n        LOGGER.debug('Pool %s added connection %s', self.id, id(connection))",
    "reference": "Add a new connection to the pool\n\n        :param connection: The connection to add to the pool\n        :type connection: psycopg2.extensions.connection\n        :raises: PoolFullError",
    "generated": "Add a new connection to the pool\n        :type connection: psycopg2.extensions.connection\n        :raises: PoolFullError"
  },
  {
    "code": "def clean(self):\n        \"\"\"Clean the pool by removing any closed connections and if the pool's\n        idle has exceeded its idle TTL, remove all connections.\n\n        \"\"\"\n        LOGGER.debug('Cleaning the pool')\n        for connection in [self.connections[k] for k in self.connections if\n                           self.connections[k].closed]:\n            LOGGER.debug('Removing %s', connection.id)\n            self.remove(connection.handle)\n\n        if self.idle_duration > self.idle_ttl:\n            self.close()\n\n        LOGGER.debug('Pool %s cleaned', self.id)",
    "reference": "Clean the pool by removing any closed connections and if the pool's\n        idle has exceeded its idle TTL, remove all connections.",
    "generated": "Clean the pool by removing any closed connections and if the pool's\n        idle has exceeded its idle TTL, remove all connections."
  },
  {
    "code": "def close(self):\n        \"\"\"Close the pool by closing and removing all of the connections\"\"\"\n        for cid in list(self.connections.keys()):\n            self.remove(self.connections[cid].handle)\n        LOGGER.debug('Pool %s closed', self.id)",
    "reference": "Close the pool by closing and removing all of the connections",
    "generated": "Close the pool by closing and removing all of the connections"
  },
  {
    "code": "def free(self, connection):\n        \"\"\"Free the connection from use by the session that was using it.\n\n        :param connection: The connection to free\n        :type connection: psycopg2.extensions.connection\n        :raises: ConnectionNotFoundError\n\n        \"\"\"\n        LOGGER.debug('Pool %s freeing connection %s', self.id, id(connection))\n        try:\n            self.connection_handle(connection).free()\n        except KeyError:\n            raise ConnectionNotFoundError(self.id, id(connection))\n\n        if self.idle_connections == list(self.connections.values()):\n            with self._lock:\n                self.idle_start = self.time_method()\n        LOGGER.debug('Pool %s freed connection %s', self.id, id(connection))",
    "reference": "Free the connection from use by the session that was using it.\n\n        :param connection: The connection to free\n        :type connection: psycopg2.extensions.connection\n        :raises: ConnectionNotFoundError",
    "generated": "Free the connection from use by the session that was using it.\n\n        :param connection: The connection to free\n        :type connection: psycopg2.extensions.connection\n        :raises: ConnectionNotFoundError"
  },
  {
    "code": "def get(self, session):\n        \"\"\"Return an idle connection and assign the session to the connection\n\n        :param queries.Session session: The session to assign\n        :rtype: psycopg2.extensions.connection\n        :raises: NoIdleConnectionsError\n\n        \"\"\"\n        idle = self.idle_connections\n        if idle:\n            connection = idle.pop(0)\n            connection.lock(session)\n            if self.idle_start:\n                with self._lock:\n                    self.idle_start = None\n            return connection.handle\n        raise NoIdleConnectionsError(self.id)",
    "reference": "Return an idle connection and assign the session to the connection\n\n        :param queries.Session session: The session to assign\n        :rtype: psycopg2.extensions.connection\n        :raises: NoIdleConnectionsError",
    "generated": "Return an idle connection and assign the session to the connection\n\n        :raises: NoIdleConnectionsError"
  },
  {
    "code": "def idle_connections(self):\n        \"\"\"Return a list of idle connections\n\n        :rtype: list\n\n        \"\"\"\n        return [c for c in self.connections.values()\n                if not c.busy and not c.closed]",
    "reference": "Return a list of idle connections\n\n        :rtype: list",
    "generated": "Return a list of idle connections\n\n        :rtype: list"
  },
  {
    "code": "def lock(self, connection, session):\n        \"\"\"Explicitly lock the specified connection\n\n        :type connection: psycopg2.extensions.connection\n        :param connection: The connection to lock\n        :param queries.Session session: The session to hold the lock\n\n        \"\"\"\n        cid = id(connection)\n        try:\n            self.connection_handle(connection).lock(session)\n        except KeyError:\n            raise ConnectionNotFoundError(self.id, cid)\n        else:\n            if self.idle_start:\n                with self._lock:\n                    self.idle_start = None\n        LOGGER.debug('Pool %s locked connection %s', self.id, cid)",
    "reference": "Explicitly lock the specified connection\n\n        :type connection: psycopg2.extensions.connection\n        :param connection: The connection to lock\n        :param queries.Session session: The session to hold the lock",
    "generated": "Explicitly lock the specified connection\n\n        :type connection: psycopg2.extensions.connection\n        :param connection: The connection to lock\n        :param queries.Session session: The session to hold the lock\n\n        \"\"\""
  },
  {
    "code": "def remove(self, connection):\n        \"\"\"Remove the connection from the pool\n\n        :param connection: The connection to remove\n        :type connection: psycopg2.extensions.connection\n        :raises: ConnectionNotFoundError\n        :raises: ConnectionBusyError\n\n        \"\"\"\n        cid = id(connection)\n        if cid not in self.connections:\n            raise ConnectionNotFoundError(self.id, cid)\n        self.connection_handle(connection).close()\n        with self._lock:\n            del self.connections[cid]\n        LOGGER.debug('Pool %s removed connection %s', self.id, cid)",
    "reference": "Remove the connection from the pool\n\n        :param connection: The connection to remove\n        :type connection: psycopg2.extensions.connection\n        :raises: ConnectionNotFoundError\n        :raises: ConnectionBusyError",
    "generated": "Remove the connection from the pool\n\n        :param connection: The connection to remove\n        :raises: ConnectionNotFoundError\n        :raises: ConnectionBusyError\n\n       "
  },
  {
    "code": "def report(self):\n        \"\"\"Return a report about the pool state and configuration.\n\n        :rtype: dict\n\n        \"\"\"\n        return {\n            'connections': {\n                'busy': len(self.busy_connections),\n                'closed': len(self.closed_connections),\n                'executing': len(self.executing_connections),\n                'idle': len(self.idle_connections),\n                'locked': len(self.busy_connections)\n            },\n            'exceptions': sum([c.exceptions\n                               for c in self.connections.values()]),\n            'executions': sum([c.executions\n                               for c in self.connections.values()]),\n            'full': self.is_full,\n            'idle': {\n                'duration': self.idle_duration,\n                'ttl': self.idle_ttl\n            },\n            'max_size': self.max_size\n        }",
    "reference": "Return a report about the pool state and configuration.\n\n        :rtype: dict",
    "generated": "Return a report about the pool state and configuration.\n\n        :rtype: dict"
  },
  {
    "code": "def shutdown(self):\n        \"\"\"Forcefully shutdown the entire pool, closing all non-executing\n        connections.\n\n        :raises: ConnectionBusyError\n\n        \"\"\"\n        with self._lock:\n            for cid in list(self.connections.keys()):\n                if self.connections[cid].executing:\n                    raise ConnectionBusyError(cid)\n                if self.connections[cid].locked:\n                    self.connections[cid].free()\n                self.connections[cid].close()\n                del self.connections[cid]",
    "reference": "Forcefully shutdown the entire pool, closing all non-executing\n        connections.\n\n        :raises: ConnectionBusyError",
    "generated": "Forcefully shutdown the entire pool, closing all non-executing\n        connections.\n\n        :raises: ConnectionBusyError"
  },
  {
    "code": "def add(cls, pid, connection):\n        \"\"\"Add a new connection and session to a pool.\n\n        :param str pid: The pool id\n        :type connection: psycopg2.extensions.connection\n        :param connection: The connection to add to the pool\n\n        \"\"\"\n        with cls._lock:\n            cls._ensure_pool_exists(pid)\n            cls._pools[pid].add(connection)",
    "reference": "Add a new connection and session to a pool.\n\n        :param str pid: The pool id\n        :type connection: psycopg2.extensions.connection\n        :param connection: The connection to add to the pool",
    "generated": "Add a new connection and session to a pool."
  },
  {
    "code": "def clean(cls, pid):\n        \"\"\"Clean the specified pool, removing any closed connections or\n        stale locks.\n\n        :param str pid: The pool id to clean\n\n        \"\"\"\n        with cls._lock:\n            try:\n                cls._ensure_pool_exists(pid)\n            except KeyError:\n                LOGGER.debug('Pool clean invoked against missing pool %s', pid)\n                return\n            cls._pools[pid].clean()\n            cls._maybe_remove_pool(pid)",
    "reference": "Clean the specified pool, removing any closed connections or\n        stale locks.\n\n        :param str pid: The pool id to clean",
    "generated": "Clean the specified pool, removing any closed connections or\n        stale locks."
  },
  {
    "code": "def create(cls, pid, idle_ttl=DEFAULT_IDLE_TTL, max_size=DEFAULT_MAX_SIZE,\n               time_method=None):\n        \"\"\"Create a new pool, with the ability to pass in values to override\n        the default idle TTL and the default maximum size.\n\n        A pool's idle TTL defines the amount of time that a pool can be open\n        without any sessions before it is removed.\n\n        A pool's max size defines the maximum number of connections that can\n        be added to the pool to prevent unbounded open connections.\n\n        :param str pid: The pool ID\n        :param int idle_ttl: Time in seconds for the idle TTL\n        :param int max_size: The maximum pool size\n        :param callable time_method: Override the use of :py:meth:`time.time`\n            method for time values.\n        :raises: KeyError\n\n        \"\"\"\n        if pid in cls._pools:\n            raise KeyError('Pool %s already exists' % pid)\n        with cls._lock:\n            LOGGER.debug(\"Creating Pool: %s (%i/%i)\", pid, idle_ttl, max_size)\n            cls._pools[pid] = Pool(pid, idle_ttl, max_size, time_method)",
    "reference": "Create a new pool, with the ability to pass in values to override\n        the default idle TTL and the default maximum size.\n\n        A pool's idle TTL defines the amount of time that a pool can be open\n        without any sessions before it is removed.\n\n        A pool's max size defines the maximum number of connections that can\n        be added to the pool to prevent unbounded open connections.\n\n        :param str pid: The pool ID\n        :param int idle_ttl: Time in seconds for the idle TTL\n        :param int max_size: The maximum pool size\n        :param callable time_method: Override the use of :py:meth:`time.time`\n            method for time values.\n        :raises: KeyError",
    "generated": "Create a new pool, with the ability to pass in values to override\n        the default idle TTL and the default maximum size.\n\n        A pool's idle TTL defines the amount of time that a pool can be open\n        without any sessions before it is removed.\n\n        A pool's max size defines the maximum number of connections that can\n        be added to the pool to prevent unbounded open connections."
  },
  {
    "code": "def free(cls, pid, connection):\n        \"\"\"Free a connection that was locked by a session\n\n        :param str pid: The pool ID\n        :param connection: The connection to remove\n        :type connection: psycopg2.extensions.connection\n\n        \"\"\"\n        with cls._lock:\n            LOGGER.debug('Freeing %s from pool %s', id(connection), pid)\n            cls._ensure_pool_exists(pid)\n            cls._pools[pid].free(connection)",
    "reference": "Free a connection that was locked by a session\n\n        :param str pid: The pool ID\n        :param connection: The connection to remove\n        :type connection: psycopg2.extensions.connection",
    "generated": "Free a connection that was locked by a session\n\n        :param str pid: The pool ID\n        :param connection: The connection to remove\n        :type connection: psycopg2.extensions.connection\n\n       "
  },
  {
    "code": "def get(cls, pid, session):\n        \"\"\"Get an idle, unused connection from the pool. Once a connection has\n        been retrieved, it will be marked as in-use until it is freed.\n\n        :param str pid: The pool ID\n        :param queries.Session session: The session to assign to the connection\n        :rtype: psycopg2.extensions.connection\n\n        \"\"\"\n        with cls._lock:\n            cls._ensure_pool_exists(pid)\n            return cls._pools[pid].get(session)",
    "reference": "Get an idle, unused connection from the pool. Once a connection has\n        been retrieved, it will be marked as in-use until it is freed.\n\n        :param str pid: The pool ID\n        :param queries.Session session: The session to assign to the connection\n        :rtype: psycopg2.extensions.connection",
    "generated": "Get an idle, unused connection from the pool. Once a connection has\n        been retrieved, it will be marked as in-use until it is freed."
  },
  {
    "code": "def has_connection(cls, pid, connection):\n        \"\"\"Check to see if a pool has the specified connection\n\n        :param str pid: The pool ID\n        :param connection: The connection to check for\n        :type connection: psycopg2.extensions.connection\n        :rtype: bool\n\n        \"\"\"\n        with cls._lock:\n            cls._ensure_pool_exists(pid)\n            return connection in cls._pools[pid]",
    "reference": "Check to see if a pool has the specified connection\n\n        :param str pid: The pool ID\n        :param connection: The connection to check for\n        :type connection: psycopg2.extensions.connection\n        :rtype: bool",
    "generated": "Check to see if a pool has the specified connection\n\n        :param str pid: The pool ID\n        :param connection: The connection to check for\n        :type connection: psycopg2.extensions.connection\n        :rtype: bool"
  },
  {
    "code": "def has_idle_connection(cls, pid):\n        \"\"\"Check to see if a pool has an idle connection\n\n        :param str pid: The pool ID\n        :rtype: bool\n\n        \"\"\"\n        with cls._lock:\n            cls._ensure_pool_exists(pid)\n            return bool(cls._pools[pid].idle_connections)",
    "reference": "Check to see if a pool has an idle connection\n\n        :param str pid: The pool ID\n        :rtype: bool",
    "generated": "Check to see if a pool has an idle connection\n\n        :param str pid: The pool ID\n        :rtype: bool"
  },
  {
    "code": "def is_full(cls, pid):\n        \"\"\"Return a bool indicating if the specified pool is full\n\n        :param str pid: The pool id\n        :rtype: bool\n\n        \"\"\"\n        with cls._lock:\n            cls._ensure_pool_exists(pid)\n            return cls._pools[pid].is_full",
    "reference": "Return a bool indicating if the specified pool is full\n\n        :param str pid: The pool id\n        :rtype: bool",
    "generated": "Return a bool indicating if the specified pool is full\n\n        :param str pid: The pool id\n        :rtype: bool"
  },
  {
    "code": "def lock(cls, pid, connection, session):\n        \"\"\"Explicitly lock the specified connection in the pool\n\n        :param str pid: The pool id\n        :type connection: psycopg2.extensions.connection\n        :param connection: The connection to add to the pool\n        :param queries.Session session: The session to hold the lock\n\n        \"\"\"\n        with cls._lock:\n            cls._ensure_pool_exists(pid)\n            cls._pools[pid].lock(connection, session)",
    "reference": "Explicitly lock the specified connection in the pool\n\n        :param str pid: The pool id\n        :type connection: psycopg2.extensions.connection\n        :param connection: The connection to add to the pool\n        :param queries.Session session: The session to hold the lock",
    "generated": "Explicitly lock the specified connection in the pool\n\n        :param str pid: The pool id\n        :type connection: psycopg2.extensions.connection\n        :param connection: The connection to add to the pool\n        :param queries.Session session: The session to hold the lock\n\n       "
  },
  {
    "code": "def remove(cls, pid):\n        \"\"\"Remove a pool, closing all connections\n\n        :param str pid: The pool ID\n\n        \"\"\"\n        with cls._lock:\n            cls._ensure_pool_exists(pid)\n            cls._pools[pid].close()\n            del cls._pools[pid]",
    "reference": "Remove a pool, closing all connections\n\n        :param str pid: The pool ID",
    "generated": "Remove a pool, closing all connections\n\n        :param str pid: The pool ID"
  },
  {
    "code": "def remove_connection(cls, pid, connection):\n        \"\"\"Remove a connection from the pool, closing it if is open.\n\n        :param str pid: The pool ID\n        :param connection: The connection to remove\n        :type connection: psycopg2.extensions.connection\n        :raises: ConnectionNotFoundError\n\n        \"\"\"\n        cls._ensure_pool_exists(pid)\n        cls._pools[pid].remove(connection)",
    "reference": "Remove a connection from the pool, closing it if is open.\n\n        :param str pid: The pool ID\n        :param connection: The connection to remove\n        :type connection: psycopg2.extensions.connection\n        :raises: ConnectionNotFoundError",
    "generated": "Remove a connection from the pool, closing it if is open.\n\n        :param str pid: The pool ID\n        :param connection: The connection to remove\n        :type connection: psycopg2.extensions.connection\n        :raises: ConnectionNotFoundError\n\n       "
  },
  {
    "code": "def set_idle_ttl(cls, pid, ttl):\n        \"\"\"Set the idle TTL for a pool, after which it will be destroyed.\n\n        :param str pid: The pool id\n        :param int ttl: The TTL for an idle pool\n\n        \"\"\"\n        with cls._lock:\n            cls._ensure_pool_exists(pid)\n            cls._pools[pid].set_idle_ttl(ttl)",
    "reference": "Set the idle TTL for a pool, after which it will be destroyed.\n\n        :param str pid: The pool id\n        :param int ttl: The TTL for an idle pool",
    "generated": "Set the idle TTL for a pool, after which it will be destroyed."
  },
  {
    "code": "def set_max_size(cls, pid, size):\n        \"\"\"Set the maximum number of connections for the specified pool\n\n        :param str pid: The pool to set the size for\n        :param int size: The maximum number of connections\n\n        \"\"\"\n        with cls._lock:\n            cls._ensure_pool_exists(pid)\n            cls._pools[pid].set_max_size(size)",
    "reference": "Set the maximum number of connections for the specified pool\n\n        :param str pid: The pool to set the size for\n        :param int size: The maximum number of connections",
    "generated": "Set the maximum number of connections for the specified pool\n\n        :param str pid: The pool to set the size for\n        :param int size: The maximum number of connections"
  },
  {
    "code": "def shutdown(cls):\n        \"\"\"Close all connections on in all pools\"\"\"\n        for pid in list(cls._pools.keys()):\n            cls._pools[pid].shutdown()\n        LOGGER.info('Shutdown complete, all pooled connections closed')",
    "reference": "Close all connections on in all pools",
    "generated": "Close all connections on in all pools"
  },
  {
    "code": "def size(cls, pid):\n        \"\"\"Return the number of connections in the pool\n\n        :param str pid: The pool id\n        :rtype int\n\n        \"\"\"\n        with cls._lock:\n            cls._ensure_pool_exists(pid)\n            return len(cls._pools[pid])",
    "reference": "Return the number of connections in the pool\n\n        :param str pid: The pool id\n        :rtype int",
    "generated": "Return the number of connections in the pool\n\n        :param str pid: The pool id\n        :rtype int"
  },
  {
    "code": "def report(cls):\n        \"\"\"Return the state of the all of the registered pools.\n\n        :rtype: dict\n\n        \"\"\"\n        return {\n            'timestamp': datetime.datetime.utcnow().isoformat(),\n            'process': os.getpid(),\n            'pools': dict([(i, p.report()) for i, p in cls._pools.items()])\n        }",
    "reference": "Return the state of the all of the registered pools.\n\n        :rtype: dict",
    "generated": "Return the state of the all of the registered pools.\n\n        :rtype: dict"
  },
  {
    "code": "def _maybe_remove_pool(cls, pid):\n        \"\"\"If the pool has no open connections, remove it\n\n        :param str pid: The pool id to clean\n\n        \"\"\"\n        if not len(cls._pools[pid]):\n            del cls._pools[pid]",
    "reference": "If the pool has no open connections, remove it\n\n        :param str pid: The pool id to clean",
    "generated": "Remove the pool from the pool."
  },
  {
    "code": "def close(self):\n        \"\"\"Explicitly close the connection and remove it from the connection\n        pool if pooling is enabled. If the connection is already closed\n\n        :raises: psycopg2.InterfaceError\n\n        \"\"\"\n        if not self._conn:\n            raise psycopg2.InterfaceError('Connection not open')\n        LOGGER.info('Closing connection %r in %s', self._conn, self.pid)\n        self._pool_manager.free(self.pid, self._conn)\n        self._pool_manager.remove_connection(self.pid, self._conn)\n\n        # Un-assign the connection and cursor\n        self._conn, self._cursor = None, None",
    "reference": "Explicitly close the connection and remove it from the connection\n        pool if pooling is enabled. If the connection is already closed\n\n        :raises: psycopg2.InterfaceError",
    "generated": "Explicitly close the connection and remove it from the connection\n        pool if pooling is enabled. If the connection is already closed\n\n        :raises: psycopg2.InterfaceError"
  },
  {
    "code": "def pid(self):\n        \"\"\"Return the pool ID used for connection pooling.\n\n        :rtype: str\n\n        \"\"\"\n        return hashlib.md5(':'.join([self.__class__.__name__,\n                                     self._uri]).encode('utf-8')).hexdigest()",
    "reference": "Return the pool ID used for connection pooling.\n\n        :rtype: str",
    "generated": "Return the pool ID used for connection pooling.\n\n        :rtype: str"
  },
  {
    "code": "def set_encoding(self, value=DEFAULT_ENCODING):\n        \"\"\"Set the client encoding for the session if the value specified\n        is different than the current client encoding.\n\n        :param str value: The encoding value to use\n\n        \"\"\"\n        if self._conn.encoding != value:\n            self._conn.set_client_encoding(value)",
    "reference": "Set the client encoding for the session if the value specified\n        is different than the current client encoding.\n\n        :param str value: The encoding value to use",
    "generated": "Set the client encoding for the session if the value specified\n        is different than the current client encoding."
  },
  {
    "code": "def _cleanup(self):\n        \"\"\"Remove the connection from the stack, closing out the cursor\"\"\"\n        if self._cursor:\n            LOGGER.debug('Closing the cursor on %s', self.pid)\n            self._cursor.close()\n            self._cursor = None\n\n        if self._conn:\n            LOGGER.debug('Freeing %s in the pool', self.pid)\n            try:\n                pool.PoolManager.instance().free(self.pid, self._conn)\n            except pool.ConnectionNotFoundError:\n                pass\n            self._conn = None",
    "reference": "Remove the connection from the stack, closing out the cursor",
    "generated": "Remove the connection from the stack, closing out the cursor"
  },
  {
    "code": "def _get_cursor(self, connection, name=None):\n        \"\"\"Return a cursor for the given cursor_factory. Specify a name to\n        use server-side cursors.\n\n        :param connection: The connection to create a cursor on\n        :type connection: psycopg2.extensions.connection\n        :param str name: A cursor name for a server side cursor\n        :rtype: psycopg2.extensions.cursor\n\n        \"\"\"\n        cursor = connection.cursor(name=name,\n                                   cursor_factory=self._cursor_factory)\n        if name is not None:\n            cursor.scrollable = True\n            cursor.withhold = True\n        return cursor",
    "reference": "Return a cursor for the given cursor_factory. Specify a name to\n        use server-side cursors.\n\n        :param connection: The connection to create a cursor on\n        :type connection: psycopg2.extensions.connection\n        :param str name: A cursor name for a server side cursor\n        :rtype: psycopg2.extensions.cursor",
    "generated": "Return a cursor for the given cursor_factory. Specify a name to\n        use server-side cursors.\n\n        :param connection: The connection to create a cursor on\n        :type connection: psycopg2.extensions.connection\n        :param str name: A cursor name for a server side cursor\n        :rtype: psycopg2.extensions.cursor\n\n        :param str name: A cursor name for a server side cursor\n        :rtype: psycopg2.extensions.cursor\n        :param str name: A cursor name for a server"
  },
  {
    "code": "def _register_unicode(connection):\n        \"\"\"Register the cursor to be able to receive Unicode string.\n\n        :type connection: psycopg2.extensions.connection\n        :param connection: Where to register things\n\n        \"\"\"\n        psycopg2.extensions.register_type(psycopg2.extensions.UNICODE,\n                                          connection)\n        psycopg2.extensions.register_type(psycopg2.extensions.UNICODEARRAY,\n                                          connection)",
    "reference": "Register the cursor to be able to receive Unicode string.\n\n        :type connection: psycopg2.extensions.connection\n        :param connection: Where to register things",
    "generated": "Register the cursor to be able to receive Unicode string.\n\n        :type connection: psycopg2.extensions.connection\n        :param connection: Where to register things\n\n       "
  },
  {
    "code": "def _status(self):\n        \"\"\"Return the current connection status as an integer value.\n\n        The status should match one of the following constants:\n\n        - queries.Session.INTRANS: Connection established, in transaction\n        - queries.Session.PREPARED: Prepared for second phase of transaction\n        - queries.Session.READY: Connected, no active transaction\n\n        :rtype: int\n\n        \"\"\"\n        if self._conn.status == psycopg2.extensions.STATUS_BEGIN:\n            return self.READY\n        return self._conn.status",
    "reference": "Return the current connection status as an integer value.\n\n        The status should match one of the following constants:\n\n        - queries.Session.INTRANS: Connection established, in transaction\n        - queries.Session.PREPARED: Prepared for second phase of transaction\n        - queries.Session.READY: Connected, no active transaction\n\n        :rtype: int",
    "generated": "Return the current connection status as an integer value.\n\n        The status should match one of the following constants:\n\n        - queries.Session.INTRANS: Connection established, in transaction\n        - queries.Session.PREPARED: Prepared for second phase of transaction\n        - queries.Session.READY: Ready, no active transaction\n\n        :rtype: int"
  },
  {
    "code": "def items(self):\n        \"\"\"Return all of the rows that are in the result set.\n\n        :rtype: list\n\n        \"\"\"\n        if not self.cursor.rowcount:\n            return []\n\n        self.cursor.scroll(0, 'absolute')\n        return self.cursor.fetchall()",
    "reference": "Return all of the rows that are in the result set.\n\n        :rtype: list",
    "generated": "Return the rows that are in the result set.\n\n        :rtype: list"
  },
  {
    "code": "def get_current_user():\n    \"\"\"Return the current username for the logged in user\n\n    :rtype: str\n\n    \"\"\"\n    if pwd is None:\n        return getpass.getuser()\n    else:\n        try:\n            return pwd.getpwuid(os.getuid())[0]\n        except KeyError as error:\n            LOGGER.error('Could not get logged-in user: %s', error)",
    "reference": "Return the current username for the logged in user\n\n    :rtype: str",
    "generated": "Return the current username for the logged in user\n\n    :rtype: str"
  },
  {
    "code": "def uri(host='localhost', port=5432, dbname='postgres', user='postgres',\n        password=None):\n    \"\"\"Return a PostgreSQL connection URI for the specified values.\n\n    :param str host: Host to connect to\n    :param int port: Port to connect on\n    :param str dbname: The database name\n    :param str user: User to connect as\n    :param str password: The password to use, None for no password\n    :return str: The PostgreSQL connection URI\n\n     \"\"\"\n    if port:\n        host = '%s:%s' % (host, port)\n    if password:\n        return 'postgresql://%s:%s@%s/%s' % (user, password, host, dbname)\n    return 'postgresql://%s@%s/%s' % (user, host, dbname)",
    "reference": "Return a PostgreSQL connection URI for the specified values.\n\n    :param str host: Host to connect to\n    :param int port: Port to connect on\n    :param str dbname: The database name\n    :param str user: User to connect as\n    :param str password: The password to use, None for no password\n    :return str: The PostgreSQL connection URI",
    "generated": "Return a PostgreSQL connection URI for the specified values.\n\n    :param str host: Host to connect to\n    :param int port: Port to connect on\n    :param str dbname: The database name\n    :param str user: User to use, None for no password"
  },
  {
    "code": "def uri_to_kwargs(uri):\n    \"\"\"Return a URI as kwargs for connecting to PostgreSQL with psycopg2,\n    applying default values for non-specified areas of the URI.\n\n    :param str uri: The connection URI\n    :rtype: dict\n\n    \"\"\"\n    parsed = urlparse(uri)\n    default_user = get_current_user()\n    password = unquote(parsed.password) if parsed.password else None\n    kwargs = {'host': parsed.hostname,\n              'port': parsed.port,\n              'dbname': parsed.path[1:] or default_user,\n              'user': parsed.username or default_user,\n              'password': password}\n    values = parse_qs(parsed.query)\n    if 'host' in values:\n        kwargs['host'] = values['host'][0]\n    for k in [k for k in values if k in KEYWORDS]:\n        kwargs[k] = values[k][0] if len(values[k]) == 1 else values[k]\n        try:\n            if kwargs[k].isdigit():\n                kwargs[k] = int(kwargs[k])\n        except AttributeError:\n            pass\n    return kwargs",
    "reference": "Return a URI as kwargs for connecting to PostgreSQL with psycopg2,\n    applying default values for non-specified areas of the URI.\n\n    :param str uri: The connection URI\n    :rtype: dict",
    "generated": "Return a URI as kwargs for connecting to PostgreSQL with psycopg2,\n    applying default values for non-specified areas of the URI.\n\n    :param str uri: The connection URI\n    :rtype: dict"
  },
  {
    "code": "def _ensure_pool_exists(self):\n        \"\"\"Create the pool in the pool manager if it does not exist.\"\"\"\n        if self.pid not in self._pool_manager:\n            self._pool_manager.create(self.pid, self._pool_idle_ttl,\n                                      self._pool_max_size, self._ioloop.time)",
    "reference": "Create the pool in the pool manager if it does not exist.",
    "generated": "Create the pool in the pool manager if it does not exist."
  },
  {
    "code": "def _create_connection(self, future):\n        \"\"\"Create a new PostgreSQL connection\n\n        :param tornado.concurrent.Future future: future for new conn result\n\n        \"\"\"\n        LOGGER.debug('Creating a new connection for %s', self.pid)\n\n        # Create a new PostgreSQL connection\n        kwargs = utils.uri_to_kwargs(self._uri)\n\n        try:\n            connection = self._psycopg2_connect(kwargs)\n        except (psycopg2.Error, OSError, socket.error) as error:\n            future.set_exception(error)\n            return\n\n        # Add the connection for use in _poll_connection\n        fd = connection.fileno()\n        self._connections[fd] = connection\n\n        def on_connected(cf):\n            \"\"\"Invoked by the IOLoop when the future is complete for the\n            connection\n\n            :param Future cf: The future for the initial connection\n\n            \"\"\"\n            if cf.exception():\n                self._cleanup_fd(fd, True)\n                future.set_exception(cf.exception())\n\n            else:\n\n                try:\n                    # Add the connection to the pool\n                    LOGGER.debug('Connection established for %s', self.pid)\n                    self._pool_manager.add(self.pid, connection)\n                except (ValueError, pool.PoolException) as err:\n                    LOGGER.exception('Failed to add %r to the pool', self.pid)\n                    self._cleanup_fd(fd)\n                    future.set_exception(err)\n                    return\n\n                self._pool_manager.lock(self.pid, connection, self)\n\n                # Added in because psycopg2cffi connects and leaves the\n                # connection in a weird state: consts.STATUS_DATESTYLE,\n                # returning from Connection._setup without setting the state\n                # as const.STATUS_OK\n                if utils.PYPY:\n                    connection.status = extensions.STATUS_READY\n\n                # Register the custom data types\n                self._register_unicode(connection)\n                self._register_uuid(connection)\n\n                # Set the future result\n                future.set_result(connection)\n\n        # Add a future that fires once connected\n        self._futures[fd] = concurrent.Future()\n        self._ioloop.add_future(self._futures[fd], on_connected)\n\n        # Add the connection to the IOLoop\n        self._ioloop.add_handler(connection.fileno(),\n                                 self._on_io_events,\n                                 ioloop.IOLoop.WRITE)",
    "reference": "Create a new PostgreSQL connection\n\n        :param tornado.concurrent.Future future: future for new conn result",
    "generated": "Create a new PostgreSQL connection\n\n        :param tornado.concurrent.Future future: future for new conn result\n\n       "
  },
  {
    "code": "def _exec_cleanup(self, cursor, fd):\n        \"\"\"Close the cursor, remove any references to the fd in internal state\n        and remove the fd from the ioloop.\n\n        :param psycopg2.extensions.cursor cursor: The cursor to close\n        :param int fd: The connection file descriptor\n\n        \"\"\"\n        LOGGER.debug('Closing cursor and cleaning %s', fd)\n        try:\n            cursor.close()\n        except (psycopg2.Error, psycopg2.Warning) as error:\n            LOGGER.debug('Error closing the cursor: %s', error)\n\n        self._cleanup_fd(fd)\n\n        # If the cleanup callback exists, remove it\n        if self._cleanup_callback:\n            self._ioloop.remove_timeout(self._cleanup_callback)\n\n        # Create a new cleanup callback to clean the pool of idle connections\n        self._cleanup_callback = self._ioloop.add_timeout(\n            self._ioloop.time() + self._pool_idle_ttl + 1,\n            self._pool_manager.clean, self.pid)",
    "reference": "Close the cursor, remove any references to the fd in internal state\n        and remove the fd from the ioloop.\n\n        :param psycopg2.extensions.cursor cursor: The cursor to close\n        :param int fd: The connection file descriptor",
    "generated": "Close the cursor, remove any references to the fd in internal state\n        and remove the fd from the ioloop.\n\n        :param psycopg2.extensions.cursor cursor: The cursor to close\n        :param int fd: The connection file descriptor"
  },
  {
    "code": "def _cleanup_fd(self, fd, close=False):\n        \"\"\"Ensure the socket socket is removed from the IOLoop, the\n        connection stack, and futures stack.\n\n        :param int fd: The fd # to cleanup\n\n        \"\"\"\n        self._ioloop.remove_handler(fd)\n        if fd in self._connections:\n            try:\n                self._pool_manager.free(self.pid, self._connections[fd])\n            except pool.ConnectionNotFoundError:\n                pass\n            if close:\n                self._connections[fd].close()\n            del self._connections[fd]\n        if fd in self._futures:\n            del self._futures[fd]",
    "reference": "Ensure the socket socket is removed from the IOLoop, the\n        connection stack, and futures stack.\n\n        :param int fd: The fd # to cleanup",
    "generated": "Ensure the socket socket is removed from the IOLoop, the\n        connection stack, and futures stack."
  },
  {
    "code": "def _on_io_events(self, fd=None, _events=None):\n        \"\"\"Invoked by Tornado's IOLoop when there are events for the fd\n\n        :param int fd: The file descriptor for the event\n        :param int _events: The events raised\n\n        \"\"\"\n        if fd not in self._connections:\n            LOGGER.warning('Received IO event for non-existing connection')\n            return\n        self._poll_connection(fd)",
    "reference": "Invoked by Tornado's IOLoop when there are events for the fd\n\n        :param int fd: The file descriptor for the event\n        :param int _events: The events raised",
    "generated": "Invoked by Tornado's IOLoop when there are events for the fd\n\n        :param int fd: The file descriptor for the event\n        :param int _events: The events raised"
  },
  {
    "code": "def _poll_connection(self, fd):\n        \"\"\"Check with psycopg2 to see what action to take. If the state is\n        POLL_OK, we should have a pending callback for that fd.\n\n        :param int fd: The socket fd for the postgresql connection\n\n        \"\"\"\n        try:\n            state = self._connections[fd].poll()\n        except (OSError, socket.error) as error:\n            self._ioloop.remove_handler(fd)\n            if fd in self._futures and not self._futures[fd].done():\n                self._futures[fd].set_exception(\n                    psycopg2.OperationalError('Connection error (%s)' % error)\n                )\n        except (psycopg2.Error, psycopg2.Warning) as error:\n            if fd in self._futures and not self._futures[fd].done():\n                self._futures[fd].set_exception(error)\n        else:\n            if state == extensions.POLL_OK:\n                if fd in self._futures and not self._futures[fd].done():\n                    self._futures[fd].set_result(True)\n            elif state == extensions.POLL_WRITE:\n                self._ioloop.update_handler(fd, ioloop.IOLoop.WRITE)\n            elif state == extensions.POLL_READ:\n                self._ioloop.update_handler(fd, ioloop.IOLoop.READ)\n            elif state == extensions.POLL_ERROR:\n                self._ioloop.remove_handler(fd)\n                if fd in self._futures and not self._futures[fd].done():\n                    self._futures[fd].set_exception(\n                        psycopg2.Error('Poll Error'))",
    "reference": "Check with psycopg2 to see what action to take. If the state is\n        POLL_OK, we should have a pending callback for that fd.\n\n        :param int fd: The socket fd for the postgresql connection",
    "generated": "Check with psycopg2 to see what action to take. If the state is\n        POLL_OK, we should have a pending callback for that fd.\n\n        :param int fd: The socket fd for the postgresql connection\n\n       "
  },
  {
    "code": "def main():\n    \"\"\"Setup.py entry point.\"\"\"\n    import codecs\n    setuptools.setup(\n        name='wcwidth',\n        version='0.1.7',\n        description=(\"Measures number of Terminal column cells \"\n                     \"of wide-character codes\"),\n        long_description=codecs.open(\n            os.path.join(HERE, 'README.rst'), 'r', 'utf8').read(),\n        author='Jeff Quast',\n        author_email='contact@jeffquast.com',\n        license='MIT',\n        packages=['wcwidth', 'wcwidth.tests'],\n        url='https://github.com/jquast/wcwidth',\n        include_package_data=True,\n        test_suite='wcwidth.tests',\n        zip_safe=True,\n        classifiers=[\n            'Intended Audience :: Developers',\n            'Natural Language :: English',\n            'Development Status :: 3 - Alpha',\n            'Environment :: Console',\n            'License :: OSI Approved :: MIT License',\n            'Operating System :: POSIX',\n            'Programming Language :: Python :: 2.7',\n            'Programming Language :: Python :: 3.4',\n            'Programming Language :: Python :: 3.5',\n            'Topic :: Software Development :: Libraries',\n            'Topic :: Software Development :: Localization',\n            'Topic :: Software Development :: Internationalization',\n            'Topic :: Terminals'\n            ],\n        keywords=['terminal', 'emulator', 'wcwidth', 'wcswidth', 'cjk',\n                  'combining', 'xterm', 'console', ],\n        cmdclass={'update': SetupUpdate},\n    )",
    "reference": "Setup.py entry point.",
    "generated": "Setup.py entry point."
  },
  {
    "code": "def _do_readme_update(self):\n        \"\"\"Patch README.rst to reflect the data files used in release.\"\"\"\n        import codecs\n        import glob\n\n        # read in,\n        data_in = codecs.open(\n            os.path.join(HERE, 'README.rst'), 'r', 'utf8').read()\n\n        # search for beginning and end positions,\n        pos_begin = data_in.find(self.README_PATCH_FROM)\n        assert pos_begin != -1, (pos_begin, self.README_PATCH_FROM)\n        pos_begin += len(self.README_PATCH_FROM)\n\n        pos_end = data_in.find(self.README_PATCH_TO)\n        assert pos_end != -1, (pos_end, self.README_PATCH_TO)\n\n        glob_pattern = os.path.join(HERE, 'data', '*.txt')\n        file_descriptions = [\n            self._describe_file_header(fpath)\n            for fpath in glob.glob(glob_pattern)]\n\n        # patch,\n        data_out = (\n            data_in[:pos_begin] +\n            '\\n\\n' +\n            '\\n'.join(file_descriptions) +\n            '\\n\\n' +\n            data_in[pos_end:]\n        )\n\n        # write.\n        print(\"patching {} ..\".format(self.README_RST))\n        codecs.open(\n            self.README_RST, 'w', 'utf8').write(data_out)",
    "reference": "Patch README.rst to reflect the data files used in release.",
    "generated": "Patch README.rst to reflect the data files used in release."
  },
  {
    "code": "def _do_east_asian(self):\n        \"\"\"Fetch and update east-asian tables.\"\"\"\n        self._do_retrieve(self.EAW_URL, self.EAW_IN)\n        (version, date, values) = self._parse_east_asian(\n            fname=self.EAW_IN,\n            properties=(u'W', u'F',)\n        )\n        table = self._make_table(values)\n        self._do_write(self.EAW_OUT, 'WIDE_EASTASIAN', version, date, table)",
    "reference": "Fetch and update east-asian tables.",
    "generated": "Fetch and update east-asian tables."
  },
  {
    "code": "def _do_zero_width(self):\n        \"\"\"Fetch and update zero width tables.\"\"\"\n        self._do_retrieve(self.UCD_URL, self.UCD_IN)\n        (version, date, values) = self._parse_category(\n            fname=self.UCD_IN,\n            categories=('Me', 'Mn',)\n        )\n        table = self._make_table(values)\n        self._do_write(self.ZERO_OUT, 'ZERO_WIDTH', version, date, table)",
    "reference": "Fetch and update zero width tables.",
    "generated": "Fetch and update zero width tables."
  },
  {
    "code": "def _make_table(values):\n        \"\"\"Return a tuple of lookup tables for given values.\"\"\"\n        import collections\n        table = collections.deque()\n        start, end = values[0], values[0]\n        for num, value in enumerate(values):\n            if num == 0:\n                table.append((value, value,))\n                continue\n            start, end = table.pop()\n            if end == value - 1:\n                table.append((start, value,))\n            else:\n                table.append((start, end,))\n                table.append((value, value,))\n        return tuple(table)",
    "reference": "Return a tuple of lookup tables for given values.",
    "generated": "Return a tuple of lookup tables for given values."
  },
  {
    "code": "def _do_retrieve(url, fname):\n        \"\"\"Retrieve given url to target filepath fname.\"\"\"\n        folder = os.path.dirname(fname)\n        if not os.path.exists(folder):\n            os.makedirs(folder)\n            print(\"{}/ created.\".format(folder))\n        if not os.path.exists(fname):\n            with open(fname, 'wb') as fout:\n                print(\"retrieving {}.\".format(url))\n                resp = urlopen(url)\n                fout.write(resp.read())\n            print(\"{} saved.\".format(fname))\n        else:\n            print(\"re-using artifact {}\".format(fname))\n        return fname",
    "reference": "Retrieve given url to target filepath fname.",
    "generated": "Retrieve given url to target filepath fname."
  },
  {
    "code": "def _parse_east_asian(fname, properties=(u'W', u'F',)):\n        \"\"\"Parse unicode east-asian width tables.\"\"\"\n        version, date, values = None, None, []\n        print(\"parsing {} ..\".format(fname))\n        for line in open(fname, 'rb'):\n            uline = line.decode('utf-8')\n            if version is None:\n                version = uline.split(None, 1)[1].rstrip()\n                continue\n            elif date is None:\n                date = uline.split(':', 1)[1].rstrip()\n                continue\n            if uline.startswith('#') or not uline.lstrip():\n                continue\n            addrs, details = uline.split(';', 1)\n            if any(details.startswith(property)\n                   for property in properties):\n                start, stop = addrs, addrs\n                if '..' in addrs:\n                    start, stop = addrs.split('..')\n                values.extend(range(int(start, 16), int(stop, 16) + 1))\n        return version, date, sorted(values)",
    "reference": "Parse unicode east-asian width tables.",
    "generated": "Parse unicode east-asian width tables."
  },
  {
    "code": "def _parse_category(fname, categories):\n        \"\"\"Parse unicode category tables.\"\"\"\n        version, date, values = None, None, []\n        print(\"parsing {} ..\".format(fname))\n        for line in open(fname, 'rb'):\n            uline = line.decode('utf-8')\n            if version is None:\n                version = uline.split(None, 1)[1].rstrip()\n                continue\n            elif date is None:\n                date = uline.split(':', 1)[1].rstrip()\n                continue\n            if uline.startswith('#') or not uline.lstrip():\n                continue\n            addrs, details = uline.split(';', 1)\n            addrs, details = addrs.rstrip(), details.lstrip()\n            if any(details.startswith('{} #'.format(value))\n                   for value in categories):\n                start, stop = addrs, addrs\n                if '..' in addrs:\n                    start, stop = addrs.split('..')\n                values.extend(range(int(start, 16), int(stop, 16) + 1))\n        return version, date, sorted(values)",
    "reference": "Parse unicode category tables.",
    "generated": "Parse unicode category tables."
  },
  {
    "code": "def _do_write(fname, variable, version, date, table):\n        \"\"\"Write combining tables to filesystem as python code.\"\"\"\n        # pylint: disable=R0914\n        #         Too many local variables (19/15) (col 4)\n        print(\"writing {} ..\".format(fname))\n        import unicodedata\n        import datetime\n        import string\n        utc_now = datetime.datetime.utcnow()\n        indent = 4\n        with open(fname, 'w') as fout:\n            fout.write(\n                '\"\"\"{variable_proper} table. Created by setup.py.\"\"\"\\n'\n                \"# Generated: {iso_utc}\\n\"\n                \"# Source: {version}\\n\"\n                \"# Date: {date}\\n\"\n                \"{variable} = (\".format(iso_utc=utc_now.isoformat(),\n                                        version=version,\n                                        date=date,\n                                        variable=variable,\n                                        variable_proper=variable.title()))\n            for start, end in table:\n                ucs_start, ucs_end = unichr(start), unichr(end)\n                hex_start, hex_end = ('0x{0:04x}'.format(start),\n                                      '0x{0:04x}'.format(end))\n                try:\n                    name_start = string.capwords(unicodedata.name(ucs_start))\n                except ValueError:\n                    name_start = u''\n                try:\n                    name_end = string.capwords(unicodedata.name(ucs_end))\n                except ValueError:\n                    name_end = u''\n                fout.write('\\n' + (' ' * indent))\n                fout.write('({0}, {1},),'.format(hex_start, hex_end))\n                fout.write('  # {0:24s}..{1}'.format(\n                    name_start[:24].rstrip() or '(nil)',\n                    name_end[:24].rstrip()))\n            fout.write('\\n)\\n')\n        print(\"complete.\")",
    "reference": "Write combining tables to filesystem as python code.",
    "generated": "Write combining tables to filesystem as python code."
  },
  {
    "code": "def report_ucs_msg(ucs, wcwidth_libc, wcwidth_local):\n    \"\"\"\n    Return string report of combining character differences.\n\n    :param ucs: unicode point.\n    :type ucs: unicode\n    :param wcwidth_libc: libc-wcwidth's reported character length.\n    :type comb_py: int\n    :param wcwidth_local: wcwidth's reported character length.\n    :type comb_wc: int\n    :rtype: unicode\n    \"\"\"\n    ucp = (ucs.encode('unicode_escape')[2:]\n           .decode('ascii')\n           .upper()\n           .lstrip('0'))\n    url = \"http://codepoints.net/U+{}\".format(ucp)\n    name = unicodedata.name(ucs)\n    return (u\"libc,ours={},{} [--o{}o--] name={} val={} {}\"\n            \" \".format(wcwidth_libc, wcwidth_local, ucs, name, ord(ucs), url))",
    "reference": "Return string report of combining character differences.\n\n    :param ucs: unicode point.\n    :type ucs: unicode\n    :param wcwidth_libc: libc-wcwidth's reported character length.\n    :type comb_py: int\n    :param wcwidth_local: wcwidth's reported character length.\n    :type comb_wc: int\n    :rtype: unicode",
    "generated": "Return string report of combining character differences.\n\n    :param ucs: unicode point.\n    :type ucs: unicode\n    :param wcwidth_libc: libc-wcwidth's reported character length"
  },
  {
    "code": "def validate_args(opts):\n    \"\"\"Validate and return options provided by docopt parsing.\"\"\"\n    if opts['--wide'] is None:\n        opts['--wide'] = 2\n    else:\n        assert opts['--wide'] in (\"1\", \"2\"), opts['--wide']\n    if opts['--alignment'] is None:\n        opts['--alignment'] = 'left'\n    else:\n        assert opts['--alignment'] in ('left', 'right'), opts['--alignment']\n    opts['--wide'] = int(opts['--wide'])\n    opts['character_factory'] = WcWideCharacterGenerator\n    if opts['--combining']:\n        opts['character_factory'] = WcCombinedCharacterGenerator\n    return opts",
    "reference": "Validate and return options provided by docopt parsing.",
    "generated": "Validate and return options provided by docopt parsing."
  },
  {
    "code": "def hint_width(self):\n        \"\"\"Width of a column segment.\"\"\"\n        return sum((len(self.style.delimiter),\n                    self.wide,\n                    len(self.style.delimiter),\n                    len(u' '),\n                    UCS_PRINTLEN + 2,\n                    len(u' '),\n                    self.style.name_len,))",
    "reference": "Width of a column segment.",
    "generated": "Width of a column segment."
  },
  {
    "code": "def head_item(self):\n        \"\"\"Text of a single column heading.\"\"\"\n        delimiter = self.style.attr_minor(self.style.delimiter)\n        hint = self.style.header_hint * self.wide\n        heading = (u'{delimiter}{hint}{delimiter}'\n                   .format(delimiter=delimiter, hint=hint))\n        alignment = lambda *args: (\n            self.term.rjust(*args) if self.style.alignment == 'right' else\n            self.term.ljust(*args))\n        txt = alignment(heading, self.hint_width, self.style.header_fill)\n        return self.style.attr_major(txt)",
    "reference": "Text of a single column heading.",
    "generated": "Text of a single column heading."
  },
  {
    "code": "def msg_intro(self):\n        \"\"\"Introductory message disabled above heading.\"\"\"\n        delim = self.style.attr_minor(self.style.delimiter)\n        txt = self.intro_msg_fmt.format(delim=delim).rstrip()\n        return self.term.center(txt)",
    "reference": "Introductory message disabled above heading.",
    "generated": "Introductory message disabled above heading."
  },
  {
    "code": "def num_columns(self):\n        \"\"\"Number of columns displayed.\"\"\"\n        if self.term.is_a_tty:\n            return self.term.width // self.hint_width\n        return 1",
    "reference": "Number of columns displayed.",
    "generated": "Number of columns displayed."
  },
  {
    "code": "def on_resize(self, *args):\n        \"\"\"Signal handler callback for SIGWINCH.\"\"\"\n        # pylint: disable=W0613\n        #         Unused argument 'args'\n        self.screen.style.name_len = min(self.screen.style.name_len,\n                                         self.term.width - 15)\n        assert self.term.width >= self.screen.hint_width, (\n            'Screen to small {}, must be at least {}'.format(\n                self.term.width, self.screen.hint_width))\n        self._set_lastpage()\n        self.dirty = self.STATE_REFRESH",
    "reference": "Signal handler callback for SIGWINCH.",
    "generated": "Signal handler callback for SIGWINCH."
  },
  {
    "code": "def _set_lastpage(self):\n        \"\"\"Calculate value of class attribute ``last_page``.\"\"\"\n        self.last_page = (len(self._page_data) - 1) // self.screen.page_size",
    "reference": "Calculate value of class attribute ``last_page``.",
    "generated": "Calculate value of class attribute ``last_page``"
  },
  {
    "code": "def display_initialize(self):\n        \"\"\"Display 'please wait' message, and narrow build warning.\"\"\"\n        echo(self.term.home + self.term.clear)\n        echo(self.term.move_y(self.term.height // 2))\n        echo(self.term.center('Initializing page data ...').rstrip())\n        flushout()\n\n        if LIMIT_UCS == 0x10000:\n            echo('\\n\\n')\n            echo(self.term.blink_red(self.term.center(\n                'narrow Python build: upperbound value is {n}.'\n                .format(n=LIMIT_UCS)).rstrip()))\n            echo('\\n\\n')\n            flushout()",
    "reference": "Display 'please wait' message, and narrow build warning.",
    "generated": "Display 'please wait' message, and narrow build warning."
  },
  {
    "code": "def initialize_page_data(self):\n        \"\"\"Initialize the page data for the given screen.\"\"\"\n        if self.term.is_a_tty:\n            self.display_initialize()\n        self.character_generator = self.character_factory(self.screen.wide)\n        page_data = list()\n        while True:\n            try:\n                page_data.append(next(self.character_generator))\n            except StopIteration:\n                break\n        if LIMIT_UCS == 0x10000:\n            echo(self.term.center('press any key.').rstrip())\n            flushout()\n            self.term.inkey(timeout=None)\n        return page_data",
    "reference": "Initialize the page data for the given screen.",
    "generated": "Initialize the page data for the given screen."
  },
  {
    "code": "def page_data(self, idx, offset):\n        \"\"\"\n        Return character data for page of given index and offset.\n\n        :param idx: page index.\n        :type idx: int\n        :param offset: scrolling region offset of current page.\n        :type offset: int\n        :returns: list of tuples in form of ``(ucs, name)``\n        :rtype: list[(unicode, unicode)]\n        \"\"\"\n        size = self.screen.page_size\n\n        while offset < 0 and idx:\n            offset += size\n            idx -= 1\n        offset = max(0, offset)\n\n        while offset >= size:\n            offset -= size\n            idx += 1\n\n        if idx == self.last_page:\n            offset = 0\n        idx = min(max(0, idx), self.last_page)\n\n        start = (idx * self.screen.page_size) + offset\n        end = start + self.screen.page_size\n        return (idx, offset), self._page_data[start:end]",
    "reference": "Return character data for page of given index and offset.\n\n        :param idx: page index.\n        :type idx: int\n        :param offset: scrolling region offset of current page.\n        :type offset: int\n        :returns: list of tuples in form of ``(ucs, name)``\n        :rtype: list[(unicode, unicode)]",
    "generated": "Return character data for page of given index and offset.\n\n        :type idx: int\n        :returns: list of tuples in form of ``(ucs, name)``\n        :rtype: list[(unicode, unicode)]"
  },
  {
    "code": "def _run_notty(self, writer):\n        \"\"\"Pager run method for terminals that are not a tty.\"\"\"\n        page_idx = page_offset = 0\n        while True:\n            npage_idx, _ = self.draw(writer, page_idx + 1, page_offset)\n            if npage_idx == self.last_page:\n                # page displayed was last page, quit.\n                break\n            page_idx = npage_idx\n            self.dirty = self.STATE_DIRTY\n        return",
    "reference": "Pager run method for terminals that are not a tty.",
    "generated": "Pager run method for terminals that are not a tty."
  },
  {
    "code": "def _run_tty(self, writer, reader):\n        \"\"\"Pager run method for terminals that are a tty.\"\"\"\n        # allow window-change signal to reflow screen\n        signal.signal(signal.SIGWINCH, self.on_resize)\n\n        page_idx = page_offset = 0\n        while True:\n            if self.dirty:\n                page_idx, page_offset = self.draw(writer,\n                                                  page_idx,\n                                                  page_offset)\n                self.dirty = self.STATE_CLEAN\n            inp = reader(timeout=0.25)\n            if inp is not None:\n                nxt, noff = self.process_keystroke(inp,\n                                                   page_idx,\n                                                   page_offset)\n            if not self.dirty:\n                self.dirty = nxt != page_idx or noff != page_offset\n            page_idx, page_offset = nxt, noff\n            if page_idx == -1:\n                return",
    "reference": "Pager run method for terminals that are a tty.",
    "generated": "Pager run method for terminals that are a tty."
  },
  {
    "code": "def run(self, writer, reader):\n        \"\"\"\n        Pager entry point.\n\n        In interactive mode (terminal is a tty), run until\n        ``process_keystroke()`` detects quit keystroke ('q').  In\n        non-interactive mode, exit after displaying all unicode points.\n\n        :param writer: callable writes to output stream, receiving unicode.\n        :type writer: callable\n        :param reader: callable reads keystrokes from input stream, sending\n                       instance of blessed.keyboard.Keystroke.\n        :type reader: callable\n        \"\"\"\n        self._page_data = self.initialize_page_data()\n        self._set_lastpage()\n        if not self.term.is_a_tty:\n            self._run_notty(writer)\n        else:\n            self._run_tty(writer, reader)",
    "reference": "Pager entry point.\n\n        In interactive mode (terminal is a tty), run until\n        ``process_keystroke()`` detects quit keystroke ('q').  In\n        non-interactive mode, exit after displaying all unicode points.\n\n        :param writer: callable writes to output stream, receiving unicode.\n        :type writer: callable\n        :param reader: callable reads keystrokes from input stream, sending\n                       instance of blessed.keyboard.Keystroke.\n        :type reader: callable",
    "generated": "Pager entry point.\n\n        In interactive mode (terminal is a tty), run until\n        ``process_keystroke()`` detects quit keystroke ('q'). In\n        non-interactive mode, exit after displaying all unicode points.\n\n        :param writer: callable writes to output stream, receiving unicode.\n        :type writer: callable\n        :param reader: callable reads keystrokes from input stream, sending\n                       instance of blessed.keyboard.Keystroke."
  },
  {
    "code": "def process_keystroke(self, inp, idx, offset):\n        \"\"\"\n        Process keystroke ``inp``, adjusting screen parameters.\n\n        :param inp: return value of Terminal.inkey().\n        :type inp: blessed.keyboard.Keystroke\n        :param idx: page index.\n        :type idx: int\n        :param offset: scrolling region offset of current page.\n        :type offset: int\n        :returns: tuple of next (idx, offset).\n        :rtype: (int, int)\n        \"\"\"\n        if inp.lower() in (u'q', u'Q'):\n            # exit\n            return (-1, -1)\n        self._process_keystroke_commands(inp)\n        idx, offset = self._process_keystroke_movement(inp, idx, offset)\n        return idx, offset",
    "reference": "Process keystroke ``inp``, adjusting screen parameters.\n\n        :param inp: return value of Terminal.inkey().\n        :type inp: blessed.keyboard.Keystroke\n        :param idx: page index.\n        :type idx: int\n        :param offset: scrolling region offset of current page.\n        :type offset: int\n        :returns: tuple of next (idx, offset).\n        :rtype: (int, int)",
    "generated": "Process keystroke ``inp`, adjusting screen parameters.\n\n        :param inp: return value of Terminal.inkey().\n        :type inp: blessed.keyboard.Keystroke\n        :param idx: page index.\n        :type idx: int\n        :param offset: scrolling region offset of current page."
  },
  {
    "code": "def _process_keystroke_movement(self, inp, idx, offset):\n        \"\"\"Process keystrokes that adjust index and offset.\"\"\"\n        term = self.term\n        if inp in (u'y', u'k') or inp.code in (term.KEY_UP,):\n            # scroll backward 1 line\n            idx, offset = (idx, offset - self.screen.num_columns)\n        elif inp in (u'e', u'j') or inp.code in (term.KEY_ENTER,\n                                                 term.KEY_DOWN,):\n            # scroll forward 1 line\n            idx, offset = (idx, offset + self.screen.num_columns)\n        elif inp in (u'f', u' ') or inp.code in (term.KEY_PGDOWN,):\n            # scroll forward 1 page\n            idx, offset = (idx + 1, offset)\n        elif inp == u'b' or inp.code in (term.KEY_PGUP,):\n            # scroll backward 1 page\n            idx, offset = (max(0, idx - 1), offset)\n        elif inp.code in (term.KEY_SDOWN,):\n            # scroll forward 10 pages\n            idx, offset = (max(0, idx + 10), offset)\n        elif inp.code in (term.KEY_SUP,):\n            # scroll forward 10 pages\n            idx, offset = (max(0, idx - 10), offset)\n        elif inp.code == term.KEY_HOME:\n            # top\n            idx, offset = (0, 0)\n        elif inp.code == term.KEY_END:\n            # bottom\n            idx, offset = (self.last_page, 0)\n        return idx, offset",
    "reference": "Process keystrokes that adjust index and offset.",
    "generated": "Process keystrokes that adjust index and offset."
  },
  {
    "code": "def draw(self, writer, idx, offset):\n        \"\"\"\n        Draw the current page view to ``writer``.\n\n        :param writer: callable writes to output stream, receiving unicode.\n        :type writer: callable\n        :param idx: current page index.\n        :type idx: int\n        :param offset: scrolling region offset of current page.\n        :type offset: int\n        :returns: tuple of next (idx, offset).\n        :rtype: (int, int)\n        \"\"\"\n        # as our screen can be resized while we're mid-calculation,\n        # our self.dirty flag can become re-toggled; because we are\n        # not re-flowing our pagination, we must begin over again.\n        while self.dirty:\n            self.draw_heading(writer)\n            self.dirty = self.STATE_CLEAN\n            (idx, offset), data = self.page_data(idx, offset)\n            for txt in self.page_view(data):\n                writer(txt)\n        self.draw_status(writer, idx)\n        flushout()\n        return idx, offset",
    "reference": "Draw the current page view to ``writer``.\n\n        :param writer: callable writes to output stream, receiving unicode.\n        :type writer: callable\n        :param idx: current page index.\n        :type idx: int\n        :param offset: scrolling region offset of current page.\n        :type offset: int\n        :returns: tuple of next (idx, offset).\n        :rtype: (int, int)",
    "generated": "Draw the current page view to ``writer``.\n\n        :param writer: callable writes to output stream, receiving unicode.\n        :type writer: callable\n        :param idx: current page index.\n        :type idx: int\n        :param offset: scrolling region offset of current page."
  },
  {
    "code": "def draw_heading(self, writer):\n        \"\"\"\n        Conditionally redraw screen when ``dirty`` attribute is valued REFRESH.\n\n        When Pager attribute ``dirty`` is ``STATE_REFRESH``, cursor is moved\n        to (0,0), screen is cleared, and heading is displayed.\n\n        :param writer: callable writes to output stream, receiving unicode.\n        :returns: True if class attribute ``dirty`` is ``STATE_REFRESH``.\n        \"\"\"\n        if self.dirty == self.STATE_REFRESH:\n            writer(u''.join(\n                (self.term.home, self.term.clear,\n                 self.screen.msg_intro, '\\n',\n                 self.screen.header, '\\n',)))\n            return True",
    "reference": "Conditionally redraw screen when ``dirty`` attribute is valued REFRESH.\n\n        When Pager attribute ``dirty`` is ``STATE_REFRESH``, cursor is moved\n        to (0,0), screen is cleared, and heading is displayed.\n\n        :param writer: callable writes to output stream, receiving unicode.\n        :returns: True if class attribute ``dirty`` is ``STATE_REFRESH``.",
    "generated": "Conditionally redraw screen when ``dirty`` attribute is valued REFRESH.\n\n        When Pager attribute ``dirty`` is ``STATE_REFRESH``, cursor is moved\n        to (0,0), screen is cleared, and heading is displayed."
  },
  {
    "code": "def draw_status(self, writer, idx):\n        \"\"\"\n        Conditionally draw status bar when output terminal is a tty.\n\n        :param writer: callable writes to output stream, receiving unicode.\n        :param idx: current page position index.\n        :type idx: int\n        \"\"\"\n        if self.term.is_a_tty:\n            writer(self.term.hide_cursor())\n            style = self.screen.style\n            writer(self.term.move(self.term.height - 1))\n            if idx == self.last_page:\n                last_end = u'(END)'\n            else:\n                last_end = u'/{0}'.format(self.last_page)\n            txt = (u'Page {idx}{last_end} - '\n                   u'{q} to quit, [keys: {keyset}]'\n                   .format(idx=style.attr_minor(u'{0}'.format(idx)),\n                           last_end=style.attr_major(last_end),\n                           keyset=style.attr_major('kjfb12-='),\n                           q=style.attr_minor(u'q')))\n            writer(self.term.center(txt).rstrip())",
    "reference": "Conditionally draw status bar when output terminal is a tty.\n\n        :param writer: callable writes to output stream, receiving unicode.\n        :param idx: current page position index.\n        :type idx: int",
    "generated": "Conditionally draw status bar when output terminal is a tty.\n\n        :param writer: callable writes to output stream, receiving unicode.\n        :type writer: int"
  },
  {
    "code": "def page_view(self, data):\n        \"\"\"\n        Generator yields text to be displayed for the current unicode pageview.\n\n        :param data: The current page's data as tuple of ``(ucs, name)``.\n        :rtype: generator\n        \"\"\"\n        if self.term.is_a_tty:\n            yield self.term.move(self.screen.row_begins, 0)\n        # sequence clears to end-of-line\n        clear_eol = self.term.clear_eol\n        # sequence clears to end-of-screen\n        clear_eos = self.term.clear_eos\n\n        # track our current column and row, where column is\n        # the whole segment of unicode value text, and draw\n        # only self.screen.num_columns before end-of-line.\n        #\n        # use clear_eol at end of each row to erase over any\n        # \"ghosted\" text, and clear_eos at end of screen to\n        # clear the same, especially for the final page which\n        # is often short.\n        col = 0\n        for ucs, name in data:\n            val = self.text_entry(ucs, name)\n            col += 1\n            if col == self.screen.num_columns:\n                col = 0\n                if self.term.is_a_tty:\n                    val = u''.join((val, clear_eol, u'\\n'))\n                else:\n                    val = u''.join((val.rstrip(), u'\\n'))\n            yield val\n\n        if self.term.is_a_tty:\n            yield u''.join((clear_eol, u'\\n', clear_eos))",
    "reference": "Generator yields text to be displayed for the current unicode pageview.\n\n        :param data: The current page's data as tuple of ``(ucs, name)``.\n        :rtype: generator",
    "generated": "Generator yields text to be displayed for the current unicode pageview.\n\n        :param data: The current page's data as tuple of ``(ucs, name)``.\n        :rtype: generator\n        \"\"\"\n        Generator yields text to be displayed for the current unicode pageview.\n\n        :param data: The current page's data as tuple of ``(ucs, name)``.\n        :rtype: generator\n       "
  },
  {
    "code": "def back_tick(cmd, ret_err=False, as_str=True, raise_err=None):\n    \"\"\" Run command `cmd`, return stdout, or stdout, stderr if `ret_err`\n\n    Roughly equivalent to ``check_output`` in Python 2.7\n\n    Parameters\n    ----------\n    cmd : sequence\n        command to execute\n    ret_err : bool, optional\n        If True, return stderr in addition to stdout.  If False, just return\n        stdout\n    as_str : bool, optional\n        Whether to decode outputs to unicode string on exit.\n    raise_err : None or bool, optional\n        If True, raise RuntimeError for non-zero return code. If None, set to\n        True when `ret_err` is False, False if `ret_err` is True\n\n    Returns\n    -------\n    out : str or tuple\n        If `ret_err` is False, return stripped string containing stdout from\n        `cmd`.  If `ret_err` is True, return tuple of (stdout, stderr) where\n        ``stdout`` is the stripped stdout, and ``stderr`` is the stripped\n        stderr.\n\n    Raises\n    ------\n    Raises RuntimeError if command returns non-zero exit code and `raise_err`\n    is True\n    \"\"\"\n    if raise_err is None:\n        raise_err = False if ret_err else True\n    cmd_is_seq = isinstance(cmd, (list, tuple))\n    proc = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=not cmd_is_seq)\n    out, err = proc.communicate()\n    retcode = proc.returncode\n    cmd_str = ' '.join(cmd) if cmd_is_seq else cmd\n    if retcode is None:\n        proc.terminate()\n        raise RuntimeError(cmd_str + ' process did not terminate')\n    if raise_err and retcode != 0:\n        raise RuntimeError('{0} returned code {1} with error {2}'.format(\n                           cmd_str, retcode, err.decode('latin-1')))\n    out = out.strip()\n    if as_str:\n        out = out.decode('latin-1')\n    if not ret_err:\n        return out\n    err = err.strip()\n    if as_str:\n        err = err.decode('latin-1')\n    return out, err",
    "reference": "Run command `cmd`, return stdout, or stdout, stderr if `ret_err`\n\n    Roughly equivalent to ``check_output`` in Python 2.7\n\n    Parameters\n    ----------\n    cmd : sequence\n        command to execute\n    ret_err : bool, optional\n        If True, return stderr in addition to stdout.  If False, just return\n        stdout\n    as_str : bool, optional\n        Whether to decode outputs to unicode string on exit.\n    raise_err : None or bool, optional\n        If True, raise RuntimeError for non-zero return code. If None, set to\n        True when `ret_err` is False, False if `ret_err` is True\n\n    Returns\n    -------\n    out : str or tuple\n        If `ret_err` is False, return stripped string containing stdout from\n        `cmd`.  If `ret_err` is True, return tuple of (stdout, stderr) where\n        ``stdout`` is the stripped stdout, and ``stderr`` is the stripped\n        stderr.\n\n    Raises\n    ------\n    Raises RuntimeError if command returns non-zero exit code and `raise_err`\n    is True",
    "generated": "Run command `cmd`, return stdout, or stdout, stderr if `ret_err`\n\n    Roughly equivalent to ``check_output`` in Python 2.7\n\n    Parameters\n    ----------\n    cmd : sequence\n        command to execute\n    ret_err : bool, optional\n        Whether to decode outputs to unicode string on exit.\n    raise_err : None or bool, optional\n        Whether to decode outputs to unicode string on exit."
  },
  {
    "code": "def unique_by_index(sequence):\n    \"\"\" unique elements in `sequence` in the order in which they occur\n\n    Parameters\n    ----------\n    sequence : iterable\n\n    Returns\n    -------\n    uniques : list\n        unique elements of sequence, ordered by the order in which the element\n        occurs in `sequence`\n    \"\"\"\n    uniques = []\n    for element in sequence:\n        if element not in uniques:\n            uniques.append(element)\n    return uniques",
    "reference": "unique elements in `sequence` in the order in which they occur\n\n    Parameters\n    ----------\n    sequence : iterable\n\n    Returns\n    -------\n    uniques : list\n        unique elements of sequence, ordered by the order in which the element\n        occurs in `sequence`",
    "generated": "unique elements in `sequence` in the order in which they occur\n\n    Parameters\n    ----------\n    sequence : iterable\n\n    Returns\n    -------\n    uniques : list\n        unique elements of sequence, ordered by the order in which the element\n        occurs in `sequence`"
  },
  {
    "code": "def ensure_permissions(mode_flags=stat.S_IWUSR):\n    \"\"\"decorator to ensure a filename has given permissions.\n\n    If changed, original permissions are restored after the decorated\n    modification.\n    \"\"\"\n\n    def decorator(f):\n        def modify(filename, *args, **kwargs):\n            m = chmod_perms(filename) if exists(filename) else mode_flags\n            if not m & mode_flags:\n                os.chmod(filename, m | mode_flags)\n            try:\n                return f(filename, *args, **kwargs)\n            finally:\n                # restore original permissions\n                if not m & mode_flags:\n                    os.chmod(filename, m)\n        return modify\n\n    return decorator",
    "reference": "decorator to ensure a filename has given permissions.\n\n    If changed, original permissions are restored after the decorated\n    modification.",
    "generated": "decorator to ensure a filename has given permissions.\n\n    If changed, original permissions are restored after the decorated\n    modification."
  },
  {
    "code": "def get_install_names(filename):\n    \"\"\" Return install names from library named in `filename`\n\n    Returns tuple of install names\n\n    tuple will be empty if no install names, or if this is not an object file.\n\n    Parameters\n    ----------\n    filename : str\n        filename of library\n\n    Returns\n    -------\n    install_names : tuple\n        tuple of install names for library `filename`\n    \"\"\"\n    lines = _cmd_out_err(['otool', '-L', filename])\n    if not _line0_says_object(lines[0], filename):\n        return ()\n    names = tuple(parse_install_name(line)[0] for line in lines[1:])\n    install_id = get_install_id(filename)\n    if not install_id is None:\n        assert names[0] == install_id\n        return names[1:]\n    return names",
    "reference": "Return install names from library named in `filename`\n\n    Returns tuple of install names\n\n    tuple will be empty if no install names, or if this is not an object file.\n\n    Parameters\n    ----------\n    filename : str\n        filename of library\n\n    Returns\n    -------\n    install_names : tuple\n        tuple of install names for library `filename`",
    "generated": "Return install names from library named in `filename`\n\n    Returns tuple of install names\n\n    tuple will be empty if no install names, or if this is not an object file.\n\n    Parameters\n    ----------\n    filename : str\n        filename of library\n\n    Returns\n    -------\n    install_names : tuple\n        tuple of install names for library `filename`"
  },
  {
    "code": "def get_install_id(filename):\n    \"\"\" Return install id from library named in `filename`\n\n    Returns None if no install id, or if this is not an object file.\n\n    Parameters\n    ----------\n    filename : str\n        filename of library\n\n    Returns\n    -------\n    install_id : str\n        install id of library `filename`, or None if no install id\n    \"\"\"\n    lines = _cmd_out_err(['otool', '-D', filename])\n    if not _line0_says_object(lines[0], filename):\n        return None\n    if len(lines) == 1:\n        return None\n    if len(lines) != 2:\n        raise InstallNameError('Unexpected otool output ' + '\\n'.join(lines))\n    return lines[1].strip()",
    "reference": "Return install id from library named in `filename`\n\n    Returns None if no install id, or if this is not an object file.\n\n    Parameters\n    ----------\n    filename : str\n        filename of library\n\n    Returns\n    -------\n    install_id : str\n        install id of library `filename`, or None if no install id",
    "generated": "Return install id from library named in `filename`\n\n    Returns None if no install id, or if this is not an object file.\n\n    Parameters\n    ----------\n    filename : str\n        filename of library\n\n    Returns\n    -------\n    install_id : str\n        install id of library `filename`, or None if no install id"
  },
  {
    "code": "def set_install_name(filename, oldname, newname):\n    \"\"\" Set install name `oldname` to `newname` in library filename\n\n    Parameters\n    ----------\n    filename : str\n        filename of library\n    oldname : str\n        current install name in library\n    newname : str\n        replacement name for `oldname`\n    \"\"\"\n    names = get_install_names(filename)\n    if oldname not in names:\n        raise InstallNameError('{0} not in install names for {1}'.format(\n            oldname, filename))\n    back_tick(['install_name_tool', '-change', oldname, newname, filename])",
    "reference": "Set install name `oldname` to `newname` in library filename\n\n    Parameters\n    ----------\n    filename : str\n        filename of library\n    oldname : str\n        current install name in library\n    newname : str\n        replacement name for `oldname`",
    "generated": "Set install name `oldname` to `newname` in library filename\n\n    Parameters\n    ----------\n    filename : str\n        filename of library\n    oldname : str\n        current install name in library\n    newname : str\n        replacement name for `oldname`"
  },
  {
    "code": "def set_install_id(filename, install_id):\n    \"\"\" Set install id for library named in `filename`\n\n    Parameters\n    ----------\n    filename : str\n        filename of library\n    install_id : str\n        install id for library `filename`\n\n    Raises\n    ------\n    RuntimeError if `filename` has not install id\n    \"\"\"\n    if get_install_id(filename) is None:\n        raise InstallNameError('{0} has no install id'.format(filename))\n    back_tick(['install_name_tool', '-id', install_id, filename])",
    "reference": "Set install id for library named in `filename`\n\n    Parameters\n    ----------\n    filename : str\n        filename of library\n    install_id : str\n        install id for library `filename`\n\n    Raises\n    ------\n    RuntimeError if `filename` has not install id",
    "generated": "Set install id for library named in `filename`\n\n    Parameters\n    ----------\n    filename : str\n        filename of library\n    install_id : str\n        install id for library `filename`"
  },
  {
    "code": "def get_rpaths(filename):\n    \"\"\" Return a tuple of rpaths from the library `filename`\n\n    If `filename` is not a library then the returned tuple will be empty.\n\n    Parameters\n    ----------\n    filaname : str\n        filename of library\n\n    Returns\n    -------\n    rpath : tuple\n        rpath paths in `filename`\n    \"\"\"\n    try:\n        lines = _cmd_out_err(['otool', '-l', filename])\n    except RuntimeError:\n        return ()\n    if not _line0_says_object(lines[0], filename):\n        return ()\n    lines = [line.strip() for line in lines]\n    paths = []\n    line_no = 1\n    while line_no < len(lines):\n        line = lines[line_no]\n        line_no += 1\n        if line != 'cmd LC_RPATH':\n            continue\n        cmdsize, path = lines[line_no:line_no+2]\n        assert cmdsize.startswith('cmdsize ')\n        paths.append(RPATH_RE.match(path).groups()[0])\n        line_no += 2\n    return tuple(paths)",
    "reference": "Return a tuple of rpaths from the library `filename`\n\n    If `filename` is not a library then the returned tuple will be empty.\n\n    Parameters\n    ----------\n    filaname : str\n        filename of library\n\n    Returns\n    -------\n    rpath : tuple\n        rpath paths in `filename`",
    "generated": "Return a tuple of rpaths from the library `filename`\n\n    If `filename` is a library then the returned tuple will be empty.\n\n    Parameters\n    ----------\n    filaname : str\n        filename of library\n\n    Returns\n    -------\n    rpath : tuple\n        rpath paths in `filename`"
  },
  {
    "code": "def dir2zip(in_dir, zip_fname):\n    \"\"\" Make a zip file `zip_fname` with contents of directory `in_dir`\n\n    The recorded filenames are relative to `in_dir`, so doing a standard zip\n    unpack of the resulting `zip_fname` in an empty directory will result in\n    the original directory contents.\n\n    Parameters\n    ----------\n    in_dir : str\n        Directory path containing files to go in the zip archive\n    zip_fname : str\n        Filename of zip archive to write\n    \"\"\"\n    z = zipfile.ZipFile(zip_fname, 'w',\n                        compression=zipfile.ZIP_DEFLATED)\n    for root, dirs, files in os.walk(in_dir):\n        for file in files:\n            in_fname = pjoin(root, file)\n            in_stat = os.stat(in_fname)\n            # Preserve file permissions, but allow copy\n            info = zipfile.ZipInfo(in_fname)\n            info.filename = relpath(in_fname, in_dir)\n            if os.path.sep == '\\\\':\n                # Make the path unix friendly on windows.\n                # PyPI won't accept wheels with windows path separators\n                info.filename = relpath(in_fname, in_dir).replace('\\\\', '/')\n            # Set time from modification time\n            info.date_time = time.localtime(in_stat.st_mtime)\n            # See https://stackoverflow.com/questions/434641/how-do-i-set-permissions-attributes-on-a-file-in-a-zip-file-using-pythons-zip/48435482#48435482\n            # Also set regular file permissions\n            perms = stat.S_IMODE(in_stat.st_mode) | stat.S_IFREG\n            info.external_attr = perms << 16\n            with open_readable(in_fname, 'rb') as fobj:\n                contents = fobj.read()\n            z.writestr(info, contents, zipfile.ZIP_DEFLATED)\n    z.close()",
    "reference": "Make a zip file `zip_fname` with contents of directory `in_dir`\n\n    The recorded filenames are relative to `in_dir`, so doing a standard zip\n    unpack of the resulting `zip_fname` in an empty directory will result in\n    the original directory contents.\n\n    Parameters\n    ----------\n    in_dir : str\n        Directory path containing files to go in the zip archive\n    zip_fname : str\n        Filename of zip archive to write",
    "generated": "Make a zip file `zip_fname` with contents of directory `in_dir`\n\n    The recorded filenames are relative to `in_dir`, so doing a standard zip\n    unpack of the resulting `zip_fname` in an empty directory will result in\n    the original directory contents.\n\n    Parameters\n    ----------\n    in_dir : str\n        Directory path containing files to go in the zip archive\n    zip_fname : str\n        Filename of zip archive to write"
  },
  {
    "code": "def find_package_dirs(root_path):\n    \"\"\" Find python package directories in directory `root_path`\n\n    Parameters\n    ----------\n    root_path : str\n        Directory to search for package subdirectories\n\n    Returns\n    -------\n    package_sdirs : set\n        Set of strings where each is a subdirectory of `root_path`, containing\n        an ``__init__.py`` file.  Paths prefixed by `root_path`\n    \"\"\"\n    package_sdirs = set()\n    for entry in os.listdir(root_path):\n        fname = entry if root_path == '.' else pjoin(root_path, entry)\n        if isdir(fname) and exists(pjoin(fname, '__init__.py')):\n            package_sdirs.add(fname)\n    return package_sdirs",
    "reference": "Find python package directories in directory `root_path`\n\n    Parameters\n    ----------\n    root_path : str\n        Directory to search for package subdirectories\n\n    Returns\n    -------\n    package_sdirs : set\n        Set of strings where each is a subdirectory of `root_path`, containing\n        an ``__init__.py`` file.  Paths prefixed by `root_path`",
    "generated": "Find python package directories in directory `root_path`\n\n    Parameters\n    ----------\n    root_path : str\n        Directory to search for package subdirectories\n\n    Returns\n    -------\n    package_sdirs : set\n        Set of strings where each is a subdirectory of `root_path`, containing\n        an ``__init__.py`` file."
  },
  {
    "code": "def cmp_contents(filename1, filename2):\n    \"\"\" Returns True if contents of the files are the same\n\n    Parameters\n    ----------\n    filename1 : str\n        filename of first file to compare\n    filename2 : str\n        filename of second file to compare\n\n    Returns\n    -------\n    tf : bool\n        True if binary contents of `filename1` is same as binary contents of\n        `filename2`, False otherwise.\n    \"\"\"\n    with open_readable(filename1, 'rb') as fobj:\n        contents1 = fobj.read()\n    with open_readable(filename2, 'rb') as fobj:\n        contents2 = fobj.read()\n    return contents1 == contents2",
    "reference": "Returns True if contents of the files are the same\n\n    Parameters\n    ----------\n    filename1 : str\n        filename of first file to compare\n    filename2 : str\n        filename of second file to compare\n\n    Returns\n    -------\n    tf : bool\n        True if binary contents of `filename1` is same as binary contents of\n        `filename2`, False otherwise.",
    "generated": "Returns True if contents of the files are the same\n\n    Parameters\n    ----------\n    filename1 : str\n        filename of first file to compare\n    filename2 : str\n        filename of second file to compare\n\n    Returns\n    -------\n    tf : bool\n        True if binary contents of `filename1` is same as binary contents of\n        `filename2`, False otherwise."
  },
  {
    "code": "def get_archs(libname):\n    \"\"\" Return architecture types from library `libname`\n\n    Parameters\n    ----------\n    libname : str\n        filename of binary for which to return arch codes\n\n    Returns\n    -------\n    arch_names : frozenset\n        Empty (frozen)set if no arch codes.  If not empty, contains one or more\n        of 'ppc', 'ppc64', 'i386', 'x86_64'\n    \"\"\"\n    if not exists(libname):\n        raise RuntimeError(libname + \" is not a file\")\n    try:\n        stdout = back_tick(['lipo', '-info', libname])\n    except RuntimeError:\n        return frozenset()\n    lines = [line.strip() for line in stdout.split('\\n') if line.strip()]\n    # For some reason, output from lipo -info on .a file generates this line\n    if lines[0] == \"input file {0} is not a fat file\".format(libname):\n        line = lines[1]\n    else:\n        assert len(lines) == 1\n        line = lines[0]\n    for reggie in (\n        'Non-fat file: {0} is architecture: (.*)'.format(libname),\n        'Architectures in the fat file: {0} are: (.*)'.format(libname)):\n        reggie = re.compile(reggie)\n        match = reggie.match(line)\n        if not match is None:\n            return frozenset(match.groups()[0].split(' '))\n    raise ValueError(\"Unexpected output: '{0}' for {1}\".format(\n        stdout, libname))",
    "reference": "Return architecture types from library `libname`\n\n    Parameters\n    ----------\n    libname : str\n        filename of binary for which to return arch codes\n\n    Returns\n    -------\n    arch_names : frozenset\n        Empty (frozen)set if no arch codes.  If not empty, contains one or more\n        of 'ppc', 'ppc64', 'i386', 'x86_64'",
    "generated": "Return architecture types from library `libname`\n\n    Parameters\n    ----------\n    libname : str\n        filename of binary for which to return arch codes\n\n    Returns\n    -------\n    arch_names : frozenset\n        Empty (frozen)set if no arch codes."
  },
  {
    "code": "def validate_signature(filename):\n    \"\"\" Remove invalid signatures from a binary file\n\n    If the file signature is missing or valid then it will be ignored\n\n    Invalid signatures are replaced with an ad-hoc signature.  This is the\n    closest you can get to removing a signature on MacOS\n\n    Parameters\n    ----------\n    filename : str\n        Filepath to a binary file\n    \"\"\"\n    out, err = back_tick(['codesign', '--verify', filename],\n                         ret_err=True, as_str=True, raise_err=False)\n    if not err:\n        return # The existing signature is valid\n    if 'code object is not signed at all' in err:\n        return # File has no signature, and adding a new one isn't necessary\n\n    # This file's signature is invalid and needs to be replaced\n    replace_signature(filename, '-')",
    "reference": "Remove invalid signatures from a binary file\n\n    If the file signature is missing or valid then it will be ignored\n\n    Invalid signatures are replaced with an ad-hoc signature.  This is the\n    closest you can get to removing a signature on MacOS\n\n    Parameters\n    ----------\n    filename : str\n        Filepath to a binary file",
    "generated": "Remove invalid signatures from a binary file\n\n    If the file signature is missing or valid then it will be ignored\n\n    Invalid signatures are replaced with an ad-hoc signature."
  },
  {
    "code": "def fuse_trees(to_tree, from_tree, lib_exts=('.so', '.dylib', '.a')):\n    \"\"\" Fuse path `from_tree` into path `to_tree`\n\n    For each file in `from_tree` - check for library file extension (in\n    `lib_exts` - if present, check if there is a file with matching relative\n    path in `to_tree`, if so, use :func:`delocate.tools.lipo_fuse` to fuse the\n    two libraries together and write into `to_tree`.  If any of these\n    conditions are not met, just copy the file from `from_tree` to `to_tree`.\n\n    Parameters\n    ---------\n    to_tree : str\n        path of tree to fuse into (update into)\n    from_tree : str\n        path of tree to fuse from (update from)\n    lib_exts : sequence, optional\n        filename extensions for libraries\n    \"\"\"\n    for from_dirpath, dirnames, filenames in os.walk(from_tree):\n        to_dirpath = pjoin(to_tree, relpath(from_dirpath, from_tree))\n        # Copy any missing directories in to_path\n        for dirname in tuple(dirnames):\n            to_path = pjoin(to_dirpath, dirname)\n            if not exists(to_path):\n                from_path = pjoin(from_dirpath, dirname)\n                shutil.copytree(from_path, to_path)\n                # If copying, don't further analyze this directory\n                dirnames.remove(dirname)\n        for fname in filenames:\n            root, ext = splitext(fname)\n            from_path = pjoin(from_dirpath, fname)\n            to_path = pjoin(to_dirpath, fname)\n            if not exists(to_path):\n                _copyfile(from_path, to_path)\n            elif cmp_contents(from_path, to_path):\n                pass\n            elif ext in lib_exts:\n                # existing lib that needs fuse\n                lipo_fuse(from_path, to_path, to_path)\n            else:\n                # existing not-lib file not identical to source\n                _copyfile(from_path, to_path)",
    "reference": "Fuse path `from_tree` into path `to_tree`\n\n    For each file in `from_tree` - check for library file extension (in\n    `lib_exts` - if present, check if there is a file with matching relative\n    path in `to_tree`, if so, use :func:`delocate.tools.lipo_fuse` to fuse the\n    two libraries together and write into `to_tree`.  If any of these\n    conditions are not met, just copy the file from `from_tree` to `to_tree`.\n\n    Parameters\n    ---------\n    to_tree : str\n        path of tree to fuse into (update into)\n    from_tree : str\n        path of tree to fuse from (update from)\n    lib_exts : sequence, optional\n        filename extensions for libraries",
    "generated": "Fuse path `from_tree` into path `to_tree`\n\n    For each file in `from_tree` - check for library file extension (in\n    `lib_exts` - if present, check if there is a file with matching relative\n    path in `to_tree`, if so, use :func:`delocate.tools.lipo_fuse` to fuse the\n    two libraries together and write into `to_tree`."
  },
  {
    "code": "def fuse_wheels(to_wheel, from_wheel, out_wheel):\n    \"\"\" Fuse `from_wheel` into `to_wheel`, write to `out_wheel`\n\n    Parameters\n    ---------\n    to_wheel : str\n        filename of wheel to fuse into\n    from_wheel : str\n        filename of wheel to fuse from\n    out_wheel : str\n        filename of new wheel from fusion of `to_wheel` and `from_wheel`\n    \"\"\"\n    to_wheel, from_wheel, out_wheel = [\n        abspath(w) for w in (to_wheel, from_wheel, out_wheel)]\n    with InTemporaryDirectory():\n        zip2dir(to_wheel, 'to_wheel')\n        zip2dir(from_wheel, 'from_wheel')\n        fuse_trees('to_wheel', 'from_wheel')\n        rewrite_record('to_wheel')\n        dir2zip('to_wheel', out_wheel)",
    "reference": "Fuse `from_wheel` into `to_wheel`, write to `out_wheel`\n\n    Parameters\n    ---------\n    to_wheel : str\n        filename of wheel to fuse into\n    from_wheel : str\n        filename of wheel to fuse from\n    out_wheel : str\n        filename of new wheel from fusion of `to_wheel` and `from_wheel`",
    "generated": "Fuse `from_wheel` into `to_wheel`, write to `out_wheel`\n\n    Parameters\n    ---------\n    to_wheel : str\n        filename of wheel to fuse into\n    from_wheel : str\n        filename of wheel to fuse from\n    out_wheel : str\n        filename of wheel to fuse from\n   "
  },
  {
    "code": "def delocate_tree_libs(lib_dict, lib_path, root_path):\n    \"\"\" Move needed libraries in `lib_dict` into `lib_path`\n\n    `lib_dict` has keys naming libraries required by the files in the\n    corresponding value.  Call the keys, \"required libs\".  Call the values\n    \"requiring objects\".\n\n    Copy all the required libs to `lib_path`.  Fix up the rpaths and install\n    names in the requiring objects to point to these new copies.\n\n    Exception: required libs within the directory tree pointed to by\n    `root_path` stay where they are, but we modify requiring objects to use\n    relative paths to these libraries.\n\n    Parameters\n    ----------\n    lib_dict : dict\n        Dictionary with (key, value) pairs of (``depended_lib_path``,\n        ``dependings_dict``) (see :func:`libsana.tree_libs`)\n    lib_path : str\n        Path in which to store copies of libs referred to in keys of\n        `lib_dict`.  Assumed to exist\n    root_path : str, optional\n        Root directory of tree analyzed in `lib_dict`.  Any required\n        library within the subtrees of `root_path` does not get copied, but\n        libraries linking to it have links adjusted to use relative path to\n        this library.\n\n    Returns\n    -------\n    copied_libs : dict\n        Filtered `lib_dict` dict containing only the (key, value) pairs from\n        `lib_dict` where the keys are the libraries copied to `lib_path``.\n    \"\"\"\n    copied_libs = {}\n    delocated_libs = set()\n    copied_basenames = set()\n    rp_root_path = realpath(root_path)\n    rp_lib_path = realpath(lib_path)\n    # Test for errors first to avoid getting half-way through changing the tree\n    for required, requirings in lib_dict.items():\n        if required.startswith('@'): # assume @rpath etc are correct\n            # But warn, because likely they are not\n            warnings.warn('Not processing required path {0} because it '\n                          'begins with @'.format(required))\n            continue\n        r_ed_base = basename(required)\n        if relpath(required, rp_root_path).startswith('..'):\n            # Not local, plan to copy\n            if r_ed_base in copied_basenames:\n                raise DelocationError('Already planning to copy library with '\n                                      'same basename as: ' + r_ed_base)\n            if not exists(required):\n                raise DelocationError('library \"{0}\" does not exist'.format(\n                    required))\n            copied_libs[required] = requirings\n            copied_basenames.add(r_ed_base)\n        else: # Is local, plan to set relative loader_path\n            delocated_libs.add(required)\n    # Modify in place now that we've checked for errors\n    for required in copied_libs:\n        shutil.copy(required, lib_path)\n        # Set rpath and install names for this copied library\n        for requiring, orig_install_name in lib_dict[required].items():\n            req_rel = relpath(rp_lib_path, dirname(requiring))\n            set_install_name(requiring, orig_install_name,\n                             '@loader_path/{0}/{1}'.format(\n                                 req_rel, basename(required)))\n    for required in delocated_libs:\n        # Set relative path for local library\n        for requiring, orig_install_name in lib_dict[required].items():\n            req_rel = relpath(required, dirname(requiring))\n            set_install_name(requiring, orig_install_name,\n                             '@loader_path/' + req_rel)\n    return copied_libs",
    "reference": "Move needed libraries in `lib_dict` into `lib_path`\n\n    `lib_dict` has keys naming libraries required by the files in the\n    corresponding value.  Call the keys, \"required libs\".  Call the values\n    \"requiring objects\".\n\n    Copy all the required libs to `lib_path`.  Fix up the rpaths and install\n    names in the requiring objects to point to these new copies.\n\n    Exception: required libs within the directory tree pointed to by\n    `root_path` stay where they are, but we modify requiring objects to use\n    relative paths to these libraries.\n\n    Parameters\n    ----------\n    lib_dict : dict\n        Dictionary with (key, value) pairs of (``depended_lib_path``,\n        ``dependings_dict``) (see :func:`libsana.tree_libs`)\n    lib_path : str\n        Path in which to store copies of libs referred to in keys of\n        `lib_dict`.  Assumed to exist\n    root_path : str, optional\n        Root directory of tree analyzed in `lib_dict`.  Any required\n        library within the subtrees of `root_path` does not get copied, but\n        libraries linking to it have links adjusted to use relative path to\n        this library.\n\n    Returns\n    -------\n    copied_libs : dict\n        Filtered `lib_dict` dict containing only the (key, value) pairs from\n        `lib_dict` where the keys are the libraries copied to `lib_path``.",
    "generated": "Move needed libraries in `lib_dict` into `lib_path`\n\n    `lib_dict` has keys naming libraries required by the files in the\n    corresponding value.  Call the keys, \"required libs\".  Call the values\n    \"requiring objects\"."
  },
  {
    "code": "def copy_recurse(lib_path, copy_filt_func = None, copied_libs = None):\n    \"\"\" Analyze `lib_path` for library dependencies and copy libraries\n\n    `lib_path` is a directory containing libraries.  The libraries might\n    themselves have dependencies.  This function analyzes the dependencies and\n    copies library dependencies that match the filter `copy_filt_func`. It also\n    adjusts the depending libraries to use the copy. It keeps iterating over\n    `lib_path` until all matching dependencies (of dependencies of dependencies\n    ...) have been copied.\n\n    Parameters\n    ----------\n    lib_path : str\n        Directory containing libraries\n    copy_filt_func : None or callable, optional\n        If None, copy any library that found libraries depend on.  If callable,\n        called on each depended library name; copy where\n        ``copy_filt_func(libname)`` is True, don't copy otherwise\n    copied_libs : dict\n        Dict with (key, value) pairs of (``copied_lib_path``,\n        ``dependings_dict``) where ``copied_lib_path`` is the canonical path of\n        a library that has been copied to `lib_path`, and ``dependings_dict``\n        is a dictionary with (key, value) pairs of (``depending_lib_path``,\n        ``install_name``).  ``depending_lib_path`` is the canonical path of the\n        library depending on ``copied_lib_path``, ``install_name`` is the name\n        that ``depending_lib_path`` uses to refer to ``copied_lib_path`` (in\n        its install names).\n\n    Returns\n    -------\n    copied_libs : dict\n        Input `copied_libs` dict with any extra libraries and / or dependencies\n        added.\n    \"\"\"\n    if copied_libs is None:\n        copied_libs = {}\n    else:\n        copied_libs = dict(copied_libs)\n    done = False\n    while not done:\n        in_len = len(copied_libs)\n        _copy_required(lib_path, copy_filt_func, copied_libs)\n        done = len(copied_libs) == in_len\n    return copied_libs",
    "reference": "Analyze `lib_path` for library dependencies and copy libraries\n\n    `lib_path` is a directory containing libraries.  The libraries might\n    themselves have dependencies.  This function analyzes the dependencies and\n    copies library dependencies that match the filter `copy_filt_func`. It also\n    adjusts the depending libraries to use the copy. It keeps iterating over\n    `lib_path` until all matching dependencies (of dependencies of dependencies\n    ...) have been copied.\n\n    Parameters\n    ----------\n    lib_path : str\n        Directory containing libraries\n    copy_filt_func : None or callable, optional\n        If None, copy any library that found libraries depend on.  If callable,\n        called on each depended library name; copy where\n        ``copy_filt_func(libname)`` is True, don't copy otherwise\n    copied_libs : dict\n        Dict with (key, value) pairs of (``copied_lib_path``,\n        ``dependings_dict``) where ``copied_lib_path`` is the canonical path of\n        a library that has been copied to `lib_path`, and ``dependings_dict``\n        is a dictionary with (key, value) pairs of (``depending_lib_path``,\n        ``install_name``).  ``depending_lib_path`` is the canonical path of the\n        library depending on ``copied_lib_path``, ``install_name`` is the name\n        that ``depending_lib_path`` uses to refer to ``copied_lib_path`` (in\n        its install names).\n\n    Returns\n    -------\n    copied_libs : dict\n        Input `copied_libs` dict with any extra libraries and / or dependencies\n        added.",
    "generated": "Analyzes the dependencies and\n    copies library dependencies that match the filter `copy_filt_func`. It also\n    adjusts the depending libraries to use the copy. It keeps iterating over\n    `lib_path` until all matching dependencies (of dependencies of dependencies\n    ...)."
  },
  {
    "code": "def _copy_required(lib_path, copy_filt_func, copied_libs):\n    \"\"\" Copy libraries required for files in `lib_path` to `lib_path`\n\n    Augment `copied_libs` dictionary with any newly copied libraries, modifying\n    `copied_libs` in-place - see Notes.\n\n    This is one pass of ``copy_recurse``\n\n    Parameters\n    ----------\n    lib_path : str\n        Directory containing libraries\n    copy_filt_func : None or callable, optional\n        If None, copy any library that found libraries depend on.  If callable,\n        called on each library name; copy where ``copy_filt_func(libname)`` is\n        True, don't copy otherwise\n    copied_libs : dict\n        See :func:`copy_recurse` for definition.\n\n    Notes\n    -----\n    If we need to copy another library, add that (``depended_lib_path``,\n    ``dependings_dict``) to `copied_libs`.  ``dependings_dict`` has (key,\n    value) pairs of (``depending_lib_path``, ``install_name``).\n    ``depending_lib_path`` will be the original (canonical) library name, not\n    the copy in ``lib_path``.\n\n    Sometimes we copy a library, that further depends on a library we have\n    already copied. In this case update ``copied_libs[depended_lib]`` with the\n    extra dependency (as well as fixing up the install names for the depending\n    library).\n\n    For example, imagine we've start with a lib path like this::\n\n        my_lib_path/\n            libA.dylib\n            libB.dylib\n\n    Our input `copied_libs` has keys ``/sys/libA.dylib``, ``/sys/libB.lib``\n    telling us we previously copied those guys from the ``/sys`` folder.\n\n    On a first pass, we discover that ``libA.dylib`` depends on\n    ``/sys/libC.dylib``, so we copy that.\n\n    On a second pass, we discover now that ``libC.dylib`` also depends on\n    ``/sys/libB.dylib``.  `copied_libs` tells us that we already have a copy of\n    ``/sys/libB.dylib``, so we fix our copy of `libC.dylib`` to point to\n    ``my_lib_path/libB.dylib`` and add ``/sys/libC.dylib`` as a\n    ``dependings_dict`` entry for ``copied_libs['/sys/libB.dylib']``\n    \"\"\"\n    # Paths will be prepended with `lib_path`\n    lib_dict = tree_libs(lib_path)\n    # Map library paths after copy ('copied') to path before copy ('orig')\n    rp_lp = realpath(lib_path)\n    copied2orig = dict((pjoin(rp_lp, basename(c)), c) for c in copied_libs)\n    for required, requirings in lib_dict.items():\n        if not copy_filt_func is None and not copy_filt_func(required):\n            continue\n        if required.startswith('@'):\n            # May have been processed by us, or have some rpath, loader_path of\n            # its own. Either way, leave alone\n            continue\n        # Requiring names may well be the copies in lib_path.  Replace the copy\n        # names with the original names for entry into `copied_libs`\n        procd_requirings = {}\n        # Set requiring lib install names to point to local copy\n        for requiring, orig_install_name in requirings.items():\n            set_install_name(requiring,\n                             orig_install_name,\n                             '@loader_path/' + basename(required))\n            # Make processed version of ``dependings_dict``\n            mapped_requiring = copied2orig.get(requiring, requiring)\n            procd_requirings[mapped_requiring] = orig_install_name\n        if required in copied_libs:\n            # Have copied this already, add any new requirings\n            copied_libs[required].update(procd_requirings)\n            continue\n        # Haven't see this one before, add entry to copied_libs\n        out_path = pjoin(lib_path, basename(required))\n        if exists(out_path):\n            raise DelocationError(out_path + ' already exists')\n        shutil.copy(required, lib_path)\n        copied2orig[out_path] = required\n        copied_libs[required] = procd_requirings",
    "reference": "Copy libraries required for files in `lib_path` to `lib_path`\n\n    Augment `copied_libs` dictionary with any newly copied libraries, modifying\n    `copied_libs` in-place - see Notes.\n\n    This is one pass of ``copy_recurse``\n\n    Parameters\n    ----------\n    lib_path : str\n        Directory containing libraries\n    copy_filt_func : None or callable, optional\n        If None, copy any library that found libraries depend on.  If callable,\n        called on each library name; copy where ``copy_filt_func(libname)`` is\n        True, don't copy otherwise\n    copied_libs : dict\n        See :func:`copy_recurse` for definition.\n\n    Notes\n    -----\n    If we need to copy another library, add that (``depended_lib_path``,\n    ``dependings_dict``) to `copied_libs`.  ``dependings_dict`` has (key,\n    value) pairs of (``depending_lib_path``, ``install_name``).\n    ``depending_lib_path`` will be the original (canonical) library name, not\n    the copy in ``lib_path``.\n\n    Sometimes we copy a library, that further depends on a library we have\n    already copied. In this case update ``copied_libs[depended_lib]`` with the\n    extra dependency (as well as fixing up the install names for the depending\n    library).\n\n    For example, imagine we've start with a lib path like this::\n\n        my_lib_path/\n            libA.dylib\n            libB.dylib\n\n    Our input `copied_libs` has keys ``/sys/libA.dylib``, ``/sys/libB.lib``\n    telling us we previously copied those guys from the ``/sys`` folder.\n\n    On a first pass, we discover that ``libA.dylib`` depends on\n    ``/sys/libC.dylib``, so we copy that.\n\n    On a second pass, we discover now that ``libC.dylib`` also depends on\n    ``/sys/libB.dylib``.  `copied_libs` tells us that we already have a copy of\n    ``/sys/libB.dylib``, so we fix our copy of `libC.dylib`` to point to\n    ``my_lib_path/libB.dylib`` and add ``/sys/libC.dylib`` as a\n    ``dependings_dict`` entry for ``copied_libs['/sys/libB.dylib']``",
    "generated": "Copy libraries required for files in `lib_path` to `lib_path`\n\n    Augment `copied_libs` dictionary with any newly copied libraries, modifying\n    `copied_libs` in-place - see Notes.\n\n    This is one pass of ``copy_recurse``\n    Parameters\n    ----------\n    lib_path : str\n        Directory containing libraries\n    copy_filt_func : None or callable, optional\n        If None, copy any library that found libraries depend on."
  },
  {
    "code": "def delocate_path(tree_path, lib_path,\n                  lib_filt_func = None,\n                  copy_filt_func = filter_system_libs):\n    \"\"\" Copy required libraries for files in `tree_path` into `lib_path`\n\n    Parameters\n    ----------\n    tree_path : str\n        Root path of tree to search for required libraries\n    lib_path : str\n        Directory into which we copy required libraries\n    lib_filt_func : None or str or callable, optional\n        If None, inspect all files for dependencies on dynamic libraries. If\n        callable, accepts filename as argument, returns True if we should\n        inspect the file, False otherwise. If str == \"dylibs-only\" then inspect\n        only files with known dynamic library extensions (``.dylib``, ``.so``).\n    copy_filt_func : None or callable, optional\n        If callable, called on each library name detected as a dependency; copy\n        where ``copy_filt_func(libname)`` is True, don't copy otherwise.\n        Default is callable rejecting only libraries beginning with\n        ``/usr/lib`` or ``/System``.  None means copy all libraries. This will\n        usually end up copying large parts of the system run-time.\n\n    Returns\n    -------\n    copied_libs : dict\n        dict containing the (key, value) pairs of (``copied_lib_path``,\n        ``dependings_dict``), where ``copied_lib_path`` is a library real path\n        that was copied into `lib_sdir` of the wheel packages, and\n        ``dependings_dict`` is a dictionary with key, value pairs where the key\n        is a file in the path depending on ``copied_lib_path``, and the value\n        is the ``install_name`` of ``copied_lib_path`` in the depending\n        library.\n    \"\"\"\n    if lib_filt_func == \"dylibs-only\":\n        lib_filt_func = _dylibs_only\n    if not exists(lib_path):\n        os.makedirs(lib_path)\n    lib_dict = tree_libs(tree_path, lib_filt_func)\n    if not copy_filt_func is None:\n        lib_dict = dict((key, value) for key, value in lib_dict.items()\n                        if copy_filt_func(key))\n    copied = delocate_tree_libs(lib_dict, lib_path, tree_path)\n    return copy_recurse(lib_path, copy_filt_func, copied)",
    "reference": "Copy required libraries for files in `tree_path` into `lib_path`\n\n    Parameters\n    ----------\n    tree_path : str\n        Root path of tree to search for required libraries\n    lib_path : str\n        Directory into which we copy required libraries\n    lib_filt_func : None or str or callable, optional\n        If None, inspect all files for dependencies on dynamic libraries. If\n        callable, accepts filename as argument, returns True if we should\n        inspect the file, False otherwise. If str == \"dylibs-only\" then inspect\n        only files with known dynamic library extensions (``.dylib``, ``.so``).\n    copy_filt_func : None or callable, optional\n        If callable, called on each library name detected as a dependency; copy\n        where ``copy_filt_func(libname)`` is True, don't copy otherwise.\n        Default is callable rejecting only libraries beginning with\n        ``/usr/lib`` or ``/System``.  None means copy all libraries. This will\n        usually end up copying large parts of the system run-time.\n\n    Returns\n    -------\n    copied_libs : dict\n        dict containing the (key, value) pairs of (``copied_lib_path``,\n        ``dependings_dict``), where ``copied_lib_path`` is a library real path\n        that was copied into `lib_sdir` of the wheel packages, and\n        ``dependings_dict`` is a dictionary with key, value pairs where the key\n        is a file in the path depending on ``copied_lib_path``, and the value\n        is the ``install_name`` of ``copied_lib_path`` in the depending\n        library.",
    "generated": "Copy required libraries for files in `tree_path` into `lib_path`\n\n    Parameters\n    ----------\n    tree_path : str\n        Root path of tree to search for required libraries\n    lib_path : str\n        Directory into which we copy required libraries\n    lib_filt_func : None or str or callable, optional\n        If None, inspect all files for dependencies on dynamic libraries. If\n        callable, accepts filename as argument, returns True if we should\n        inspect the file, False otherwise."
  },
  {
    "code": "def _merge_lib_dict(d1, d2):\n    \"\"\" Merges lib_dict `d2` into lib_dict `d1`\n    \"\"\"\n    for required, requirings in d2.items():\n        if required in d1:\n            d1[required].update(requirings)\n        else:\n            d1[required] = requirings\n    return None",
    "reference": "Merges lib_dict `d2` into lib_dict `d1`",
    "generated": "Merges lib_dict `d1` into lib_dict `d2`"
  },
  {
    "code": "def delocate_wheel(in_wheel,\n                   out_wheel = None,\n                   lib_sdir = '.dylibs',\n                   lib_filt_func = None,\n                   copy_filt_func = filter_system_libs,\n                   require_archs = None,\n                   check_verbose = False,\n                  ):\n    \"\"\" Update wheel by copying required libraries to `lib_sdir` in wheel\n\n    Create `lib_sdir` in wheel tree only if we are copying one or more\n    libraries.\n\n    If `out_wheel` is None (the default), overwrite the wheel `in_wheel`\n    in-place.\n\n    Parameters\n    ----------\n    in_wheel : str\n        Filename of wheel to process\n    out_wheel : None or str\n        Filename of processed wheel to write.  If None, overwrite `in_wheel`\n    lib_sdir : str, optional\n        Subdirectory name in wheel package directory (or directories) to store\n        needed libraries.\n    lib_filt_func : None or str or callable, optional\n        If None, inspect all files for dependencies on dynamic libraries. If\n        callable, accepts filename as argument, returns True if we should\n        inspect the file, False otherwise. If str == \"dylibs-only\" then inspect\n        only files with known dynamic library extensions (``.dylib``, ``.so``).\n    copy_filt_func : None or callable, optional\n        If callable, called on each library name detected as a dependency; copy\n        where ``copy_filt_func(libname)`` is True, don't copy otherwise.\n        Default is callable rejecting only libraries beginning with\n        ``/usr/lib`` or ``/System``.  None means copy all libraries. This will\n        usually end up copying large parts of the system run-time.\n    require_archs : None or str or sequence, optional\n        If None, do no checks of architectures in libraries.  If sequence,\n        sequence of architectures (output from ``lipo -info``) that every\n        library in the wheels should have (e.g. ``['x86_64, 'i386']``). An\n        empty sequence results in checks that depended libraries have the same\n        archs as depending libraries.  If string, either \"intel\" (corresponds\n        to sequence ``['x86_64, 'i386']``) or name of required architecture\n        (e.g \"i386\" or \"x86_64\").\n    check_verbose : bool, optional\n        If True, print warning messages about missing required architectures\n\n    Returns\n    -------\n    copied_libs : dict\n        dict containing the (key, value) pairs of (``copied_lib_path``,\n        ``dependings_dict``), where ``copied_lib_path`` is a library real path\n        that was copied into `lib_sdir` of the wheel packages, and\n        ``dependings_dict`` is a dictionary with key, value pairs where the key\n        is a path in the wheel depending on ``copied_lib_path``, and the value\n        is the ``install_name`` of ``copied_lib_path`` in the depending\n        library. The filenames in the keys are relative to the wheel root path.\n    \"\"\"\n    if lib_filt_func == \"dylibs-only\":\n        lib_filt_func = _dylibs_only\n    in_wheel = abspath(in_wheel)\n    if out_wheel is None:\n        out_wheel = in_wheel\n    else:\n        out_wheel = abspath(out_wheel)\n    in_place = in_wheel == out_wheel\n    with TemporaryDirectory() as tmpdir:\n        all_copied = {}\n        wheel_dir = realpath(pjoin(tmpdir, 'wheel'))\n        zip2dir(in_wheel, wheel_dir)\n        for package_path in find_package_dirs(wheel_dir):\n            lib_path = pjoin(package_path, lib_sdir)\n            lib_path_exists = exists(lib_path)\n            copied_libs = delocate_path(package_path, lib_path,\n                                        lib_filt_func, copy_filt_func)\n            if copied_libs and lib_path_exists:\n                raise DelocationError(\n                    '{0} already exists in wheel but need to copy '\n                    '{1}'.format(lib_path, '; '.join(copied_libs)))\n            if len(os.listdir(lib_path)) == 0:\n                shutil.rmtree(lib_path)\n            # Check architectures\n            if not require_archs is None:\n                stop_fast = not check_verbose\n                bads = check_archs(copied_libs, require_archs, stop_fast)\n                if len(bads) != 0:\n                    if check_verbose:\n                        print(bads_report(bads, pjoin(tmpdir, 'wheel')))\n                    raise DelocationError(\n                        \"Some missing architectures in wheel\")\n            # Change install ids to be unique within Python space\n            install_id_root = (DLC_PREFIX +\n                               relpath(package_path, wheel_dir) +\n                               '/')\n            for lib in copied_libs:\n                lib_base = basename(lib)\n                copied_path = pjoin(lib_path, lib_base)\n                set_install_id(copied_path, install_id_root + lib_base)\n                validate_signature(copied_path)\n            _merge_lib_dict(all_copied, copied_libs)\n        if len(all_copied):\n            rewrite_record(wheel_dir)\n        if len(all_copied) or not in_place:\n            dir2zip(wheel_dir, out_wheel)\n    return stripped_lib_dict(all_copied, wheel_dir + os.path.sep)",
    "reference": "Update wheel by copying required libraries to `lib_sdir` in wheel\n\n    Create `lib_sdir` in wheel tree only if we are copying one or more\n    libraries.\n\n    If `out_wheel` is None (the default), overwrite the wheel `in_wheel`\n    in-place.\n\n    Parameters\n    ----------\n    in_wheel : str\n        Filename of wheel to process\n    out_wheel : None or str\n        Filename of processed wheel to write.  If None, overwrite `in_wheel`\n    lib_sdir : str, optional\n        Subdirectory name in wheel package directory (or directories) to store\n        needed libraries.\n    lib_filt_func : None or str or callable, optional\n        If None, inspect all files for dependencies on dynamic libraries. If\n        callable, accepts filename as argument, returns True if we should\n        inspect the file, False otherwise. If str == \"dylibs-only\" then inspect\n        only files with known dynamic library extensions (``.dylib``, ``.so``).\n    copy_filt_func : None or callable, optional\n        If callable, called on each library name detected as a dependency; copy\n        where ``copy_filt_func(libname)`` is True, don't copy otherwise.\n        Default is callable rejecting only libraries beginning with\n        ``/usr/lib`` or ``/System``.  None means copy all libraries. This will\n        usually end up copying large parts of the system run-time.\n    require_archs : None or str or sequence, optional\n        If None, do no checks of architectures in libraries.  If sequence,\n        sequence of architectures (output from ``lipo -info``) that every\n        library in the wheels should have (e.g. ``['x86_64, 'i386']``). An\n        empty sequence results in checks that depended libraries have the same\n        archs as depending libraries.  If string, either \"intel\" (corresponds\n        to sequence ``['x86_64, 'i386']``) or name of required architecture\n        (e.g \"i386\" or \"x86_64\").\n    check_verbose : bool, optional\n        If True, print warning messages about missing required architectures\n\n    Returns\n    -------\n    copied_libs : dict\n        dict containing the (key, value) pairs of (``copied_lib_path``,\n        ``dependings_dict``), where ``copied_lib_path`` is a library real path\n        that was copied into `lib_sdir` of the wheel packages, and\n        ``dependings_dict`` is a dictionary with key, value pairs where the key\n        is a path in the wheel depending on ``copied_lib_path``, and the value\n        is the ``install_name`` of ``copied_lib_path`` in the depending\n        library. The filenames in the keys are relative to the wheel root path.",
    "generated": "Update wheel by copying required libraries to `lib_sdir` in wheel\n\n    Create `lib_sdir` in wheel\n\n    Create `lib_sdir` in wheel tree only if we are copying one or more\n    libraries.\n\n    lib_filt_func : None or str\n        Filename of processed wheel to write.  If None, overwrite the wheel `in_wheel`\n    in-place.\n\n    Parameters\n    ----------\n    in_wheel : str\n        Filename of wheel to process\n    out_wheel : None or str\n        Filename of wheel"
  },
  {
    "code": "def patch_wheel(in_wheel, patch_fname, out_wheel=None):\n    \"\"\" Apply ``-p1`` style patch in `patch_fname` to contents of `in_wheel`\n\n    If `out_wheel` is None (the default), overwrite the wheel `in_wheel`\n    in-place.\n\n    Parameters\n    ----------\n    in_wheel : str\n        Filename of wheel to process\n    patch_fname : str\n        Filename of patch file.  Will be applied with ``patch -p1 <\n        patch_fname``\n    out_wheel : None or str\n        Filename of patched wheel to write.  If None, overwrite `in_wheel`\n    \"\"\"\n    in_wheel = abspath(in_wheel)\n    patch_fname = abspath(patch_fname)\n    if out_wheel is None:\n        out_wheel = in_wheel\n    else:\n        out_wheel = abspath(out_wheel)\n    if not exists(patch_fname):\n        raise ValueError(\"patch file {0} does not exist\".format(patch_fname))\n    with InWheel(in_wheel, out_wheel):\n        with open(patch_fname, 'rb') as fobj:\n            patch_proc = Popen(['patch', '-p1'],\n                               stdin = fobj,\n                               stdout = PIPE,\n                               stderr = PIPE)\n            stdout, stderr = patch_proc.communicate()\n            if patch_proc.returncode != 0:\n                raise RuntimeError(\"Patch failed with stdout:\\n\" +\n                                   stdout.decode('latin1'))",
    "reference": "Apply ``-p1`` style patch in `patch_fname` to contents of `in_wheel`\n\n    If `out_wheel` is None (the default), overwrite the wheel `in_wheel`\n    in-place.\n\n    Parameters\n    ----------\n    in_wheel : str\n        Filename of wheel to process\n    patch_fname : str\n        Filename of patch file.  Will be applied with ``patch -p1 <\n        patch_fname``\n    out_wheel : None or str\n        Filename of patched wheel to write.  If None, overwrite `in_wheel`",
    "generated": "Apply ``-p1`` style patch in `patch_fname` to contents of `in_wheel`\n\n    If `out_wheel` is None (the default), overwrite the wheel `in_wheel`\n    in-place.\n\n    Parameters\n    ----------\n    in_wheel : str\n        Filename of patch file.  Will be applied with ``patch -p1 <\n        patch_fname``\n    out_wheel : None or str\n        patch_fname``\n    out_wheel : None or str\n        patch_fname``\n        patch_fname``\n"
  },
  {
    "code": "def check_archs(copied_libs, require_archs=(), stop_fast=False):\n    \"\"\" Check compatibility of archs in `copied_libs` dict\n\n    Parameters\n    ----------\n    copied_libs : dict\n        dict containing the (key, value) pairs of (``copied_lib_path``,\n        ``dependings_dict``), where ``copied_lib_path`` is a library real path\n        that has been copied during delocation, and ``dependings_dict`` is a\n        dictionary with key, value pairs where the key is a path in the target\n        being delocated (a wheel or path) depending on ``copied_lib_path``, and\n        the value is the ``install_name`` of ``copied_lib_path`` in the\n        depending library.\n    require_archs : str or sequence, optional\n        Architectures we require to be present in all library files in wheel.\n        If an empty sequence, just check that depended libraries do have the\n        architectures of the depending libraries, with no constraints on what\n        these architectures are. If a sequence, then a set of required\n        architectures e.g. ``['i386', 'x86_64']`` to specify dual Intel\n        architectures.  If a string, then a standard architecture name as\n        returned by ``lipo -info`` or the string \"intel\", corresponding to the\n        sequence ``['i386', 'x86_64']``\n    stop_fast : bool, optional\n        Whether to give up collecting errors after the first\n\n    Returns\n    -------\n    bads : set\n        set of length 2 or 3 tuples. A length 2 tuple is of form\n        ``(depending_lib, missing_archs)`` meaning that an arch in\n        `require_archs` was missing from ``depending_lib``.  A length 3 tuple\n        is of form ``(depended_lib, depending_lib, missing_archs)`` where\n        ``depended_lib`` is the filename of the library depended on,\n        ``depending_lib`` is the library depending on ``depending_lib`` and\n        ``missing_archs`` is a set of missing architecture strings giving\n        architectures present in ``depending_lib`` and missing in\n        ``depended_lib``.  An empty set means all architectures were present as\n        required.\n    \"\"\"\n    if isinstance(require_archs, string_types):\n        require_archs = (['i386', 'x86_64'] if require_archs == 'intel'\n                         else [require_archs])\n    require_archs = frozenset(require_archs)\n    bads = []\n    for depended_lib, dep_dict in copied_libs.items():\n        depended_archs = get_archs(depended_lib)\n        for depending_lib, install_name in dep_dict.items():\n            depending_archs = get_archs(depending_lib)\n            all_required = depending_archs | require_archs\n            all_missing = all_required.difference(depended_archs)\n            if len(all_missing) == 0:\n                continue\n            required_missing = require_archs.difference(depended_archs)\n            if len(required_missing):\n                bads.append((depending_lib, required_missing))\n            else:\n                bads.append((depended_lib, depending_lib, all_missing))\n            if stop_fast:\n                return set(bads)\n    return set(bads)",
    "reference": "Check compatibility of archs in `copied_libs` dict\n\n    Parameters\n    ----------\n    copied_libs : dict\n        dict containing the (key, value) pairs of (``copied_lib_path``,\n        ``dependings_dict``), where ``copied_lib_path`` is a library real path\n        that has been copied during delocation, and ``dependings_dict`` is a\n        dictionary with key, value pairs where the key is a path in the target\n        being delocated (a wheel or path) depending on ``copied_lib_path``, and\n        the value is the ``install_name`` of ``copied_lib_path`` in the\n        depending library.\n    require_archs : str or sequence, optional\n        Architectures we require to be present in all library files in wheel.\n        If an empty sequence, just check that depended libraries do have the\n        architectures of the depending libraries, with no constraints on what\n        these architectures are. If a sequence, then a set of required\n        architectures e.g. ``['i386', 'x86_64']`` to specify dual Intel\n        architectures.  If a string, then a standard architecture name as\n        returned by ``lipo -info`` or the string \"intel\", corresponding to the\n        sequence ``['i386', 'x86_64']``\n    stop_fast : bool, optional\n        Whether to give up collecting errors after the first\n\n    Returns\n    -------\n    bads : set\n        set of length 2 or 3 tuples. A length 2 tuple is of form\n        ``(depending_lib, missing_archs)`` meaning that an arch in\n        `require_archs` was missing from ``depending_lib``.  A length 3 tuple\n        is of form ``(depended_lib, depending_lib, missing_archs)`` where\n        ``depended_lib`` is the filename of the library depended on,\n        ``depending_lib`` is the library depending on ``depending_lib`` and\n        ``missing_archs`` is a set of missing architecture strings giving\n        architectures present in ``depending_lib`` and missing in\n        ``depended_lib``.  An empty set means all architectures were present as\n        required.",
    "generated": "Check compatibility of archs in `copied_libs` dict\n\n    Parameters\n    ----------\n    copied_libs : dict\n        dict containing the (key, value) pairs of (``copied_lib_path`,\n        ``dependings_dict``), where ``copied_lib_path`` is a library real path\n        that has been copied during delocation, and ``dependings_dict`` is a\n        dictionary with key, value pairs where the key is a path in the target\n        being delocated (a wheel or path) where the key is a path in"
  },
  {
    "code": "def bads_report(bads, path_prefix=None):\n    \"\"\" Return a nice report of bad architectures in `bads`\n\n    Parameters\n    ----------\n    bads : set\n        set of length 2 or 3 tuples. A length 2 tuple is of form\n        ``(depending_lib, missing_archs)`` meaning that an arch in\n        `require_archs` was missing from ``depending_lib``.  A length 3 tuple\n        is of form ``(depended_lib, depending_lib, missing_archs)`` where\n        ``depended_lib`` is the filename of the library depended on,\n        ``depending_lib`` is the library depending on ``depending_lib`` and\n        ``missing_archs`` is a set of missing architecture strings giving\n        architectures present in ``depending_lib`` and missing in\n        ``depended_lib``.  An empty set means all architectures were present as\n        required.\n    path_prefix : None or str, optional\n        Path prefix to strip from ``depended_lib`` and ``depending_lib``. None\n        means do not strip anything.\n\n    Returns\n    -------\n    report : str\n        A nice report for printing\n    \"\"\"\n    path_processor = ((lambda x : x) if path_prefix is None\n                      else get_rp_stripper(path_prefix))\n    reports = []\n    for result in bads:\n        if len(result) == 3:\n            depended_lib, depending_lib, missing_archs = result\n            reports.append(\"{0} needs {1} {2} missing from {3}\".format(\n                path_processor(depending_lib),\n                'archs' if len(missing_archs) > 1 else 'arch',\n                ', '.join(sorted(missing_archs)),\n                path_processor(depended_lib)))\n        elif len(result) == 2:\n            depending_lib, missing_archs = result\n            reports.append(\"Required {0} {1} missing from {2}\".format(\n                'archs' if len(missing_archs) > 1 else 'arch',\n                ', '.join(sorted(missing_archs)),\n                path_processor(depending_lib)))\n        else:\n            raise ValueError('Report tuple should be length 2 or 3')\n    return '\\n'.join(sorted(reports))",
    "reference": "Return a nice report of bad architectures in `bads`\n\n    Parameters\n    ----------\n    bads : set\n        set of length 2 or 3 tuples. A length 2 tuple is of form\n        ``(depending_lib, missing_archs)`` meaning that an arch in\n        `require_archs` was missing from ``depending_lib``.  A length 3 tuple\n        is of form ``(depended_lib, depending_lib, missing_archs)`` where\n        ``depended_lib`` is the filename of the library depended on,\n        ``depending_lib`` is the library depending on ``depending_lib`` and\n        ``missing_archs`` is a set of missing architecture strings giving\n        architectures present in ``depending_lib`` and missing in\n        ``depended_lib``.  An empty set means all architectures were present as\n        required.\n    path_prefix : None or str, optional\n        Path prefix to strip from ``depended_lib`` and ``depending_lib``. None\n        means do not strip anything.\n\n    Returns\n    -------\n    report : str\n        A nice report for printing",
    "generated": "Return a nice report of bad architectures in `bads`\n\n    Parameters\n    ----------\n    bads : set\n        set of length 2 or 3 tuples. A length 2 tuple is of form\n        ``(depended_lib, depending_lib, missing_archs)`` meaning that an arch in\n        `require_archs` was missing from ``depending_lib``.  A length 3 tuple\n        is of form ``(depended_lib, depending_lib, missing_archs)`` meaning that an arch in\n        `require_archs`"
  },
  {
    "code": "def tree_libs(start_path, filt_func=None):\n    \"\"\" Return analysis of library dependencies within `start_path`\n\n    Parameters\n    ----------\n    start_path : str\n        root path of tree to search for libraries depending on other libraries.\n    filt_func : None or callable, optional\n        If None, inspect all files for library dependencies. If callable,\n        accepts filename as argument, returns True if we should inspect the\n        file, False otherwise.\n\n    Returns\n    -------\n    lib_dict : dict\n        dictionary with (key, value) pairs of (``libpath``,\n        ``dependings_dict``).\n\n        ``libpath`` is canonical (``os.path.realpath``) filename of library, or\n        library name starting with {'@rpath', '@loader_path',\n        '@executable_path'}.\n\n        ``dependings_dict`` is a dict with (key, value) pairs of\n        (``depending_libpath``, ``install_name``), where ``dependings_libpath``\n        is the canonical (``os.path.realpath``) filename of the library\n        depending on ``libpath``, and ``install_name`` is the \"install_name\" by\n        which ``depending_libpath`` refers to ``libpath``.\n\n    Notes\n    -----\n\n    See:\n\n    * https://developer.apple.com/library/mac/documentation/Darwin/Reference/ManPages/man1/dyld.1.html\n    * http://matthew-brett.github.io/pydagogue/mac_runtime_link.html\n    \"\"\"\n    lib_dict = {}\n    for dirpath, dirnames, basenames in os.walk(start_path):\n        for base in basenames:\n            depending_libpath = realpath(pjoin(dirpath, base))\n            if not filt_func is None and not filt_func(depending_libpath):\n                continue\n            rpaths = get_rpaths(depending_libpath)\n            for install_name in get_install_names(depending_libpath):\n                lib_path = (install_name if install_name.startswith('@')\n                            else realpath(install_name))\n                lib_path = resolve_rpath(lib_path, rpaths)\n                if lib_path in lib_dict:\n                    lib_dict[lib_path][depending_libpath] = install_name\n                else:\n                    lib_dict[lib_path] = {depending_libpath: install_name}\n    return lib_dict",
    "reference": "Return analysis of library dependencies within `start_path`\n\n    Parameters\n    ----------\n    start_path : str\n        root path of tree to search for libraries depending on other libraries.\n    filt_func : None or callable, optional\n        If None, inspect all files for library dependencies. If callable,\n        accepts filename as argument, returns True if we should inspect the\n        file, False otherwise.\n\n    Returns\n    -------\n    lib_dict : dict\n        dictionary with (key, value) pairs of (``libpath``,\n        ``dependings_dict``).\n\n        ``libpath`` is canonical (``os.path.realpath``) filename of library, or\n        library name starting with {'@rpath', '@loader_path',\n        '@executable_path'}.\n\n        ``dependings_dict`` is a dict with (key, value) pairs of\n        (``depending_libpath``, ``install_name``), where ``dependings_libpath``\n        is the canonical (``os.path.realpath``) filename of the library\n        depending on ``libpath``, and ``install_name`` is the \"install_name\" by\n        which ``depending_libpath`` refers to ``libpath``.\n\n    Notes\n    -----\n\n    See:\n\n    * https://developer.apple.com/library/mac/documentation/Darwin/Reference/ManPages/man1/dyld.1.html\n    * http://matthew-brett.github.io/pydagogue/mac_runtime_link.html",
    "generated": "Return analysis of library dependencies within `start_path`\n\n    Parameters\n    ----------\n    start_path : str\n        root path of tree to search for libraries depending on other libraries.\n    filt_func : None or callable, optional\n        If None, inspect all files for library dependencies. If callable,\n        accepts filename as argument, returns True if we should inspect the\n        file, False otherwise."
  },
  {
    "code": "def get_prefix_stripper(strip_prefix):\n    \"\"\" Return function to strip `strip_prefix` prefix from string if present\n\n    Parameters\n    ----------\n    prefix : str\n        Prefix to strip from the beginning of string if present\n\n    Returns\n    -------\n    stripper : func\n        function such that ``stripper(a_string)`` will strip `prefix` from\n        ``a_string`` if present, otherwise pass ``a_string`` unmodified\n    \"\"\"\n    n = len(strip_prefix)\n    def stripper(path):\n        return path if not path.startswith(strip_prefix) else path[n:]\n    return stripper",
    "reference": "Return function to strip `strip_prefix` prefix from string if present\n\n    Parameters\n    ----------\n    prefix : str\n        Prefix to strip from the beginning of string if present\n\n    Returns\n    -------\n    stripper : func\n        function such that ``stripper(a_string)`` will strip `prefix` from\n        ``a_string`` if present, otherwise pass ``a_string`` unmodified",
    "generated": "Return function to strip `strip_prefix` prefix from string if present\n\n    Returns\n    -------\n    prefix : str\n        Prefix to strip from the beginning of string if present\n\n    Returns\n    -------\n    stripper : func\n        function such that ``stripper(a_string)`` will strip `prefix` from\n        ``a_string`` if present, otherwise pass ``a_string`` unmodified"
  },
  {
    "code": "def stripped_lib_dict(lib_dict, strip_prefix):\n    \"\"\" Return `lib_dict` with `strip_prefix` removed from start of paths\n\n    Use to give form of `lib_dict` that appears relative to some base path\n    given by `strip_prefix`.  Particularly useful for analyzing wheels where we\n    unpack to a temporary path before analyzing.\n\n    Parameters\n    ----------\n    lib_dict : dict\n        See :func:`tree_libs` for definition.  All depending and depended paths\n        are canonical (therefore absolute)\n    strip_prefix : str\n        Prefix to remove (if present) from all depended and depending library\n        paths in `lib_dict`\n\n    Returns\n    -------\n    relative_dict : dict\n        `lib_dict` with `strip_prefix` removed from beginning of all depended\n        and depending library paths.\n    \"\"\"\n    relative_dict = {}\n    stripper = get_prefix_stripper(strip_prefix)\n\n    for lib_path, dependings_dict in lib_dict.items():\n        ding_dict = {}\n        for depending_libpath, install_name in dependings_dict.items():\n            ding_dict[stripper(depending_libpath)] = install_name\n        relative_dict[stripper(lib_path)] = ding_dict\n    return relative_dict",
    "reference": "Return `lib_dict` with `strip_prefix` removed from start of paths\n\n    Use to give form of `lib_dict` that appears relative to some base path\n    given by `strip_prefix`.  Particularly useful for analyzing wheels where we\n    unpack to a temporary path before analyzing.\n\n    Parameters\n    ----------\n    lib_dict : dict\n        See :func:`tree_libs` for definition.  All depending and depended paths\n        are canonical (therefore absolute)\n    strip_prefix : str\n        Prefix to remove (if present) from all depended and depending library\n        paths in `lib_dict`\n\n    Returns\n    -------\n    relative_dict : dict\n        `lib_dict` with `strip_prefix` removed from beginning of all depended\n        and depending library paths.",
    "generated": "Return `lib_dict` with `strip_prefix` removed from start of paths\n\n    Use to give form of `lib_dict` that appears relative to some base path\n    given by `strip_prefix`.  Particularly useful for analyzing wheels where we\n    unpack to a temporary path before analyzing."
  },
  {
    "code": "def wheel_libs(wheel_fname, filt_func = None):\n    \"\"\" Return analysis of library dependencies with a Python wheel\n\n    Use this routine for a dump of the dependency tree.\n\n    Parameters\n    ----------\n    wheel_fname : str\n        Filename of wheel\n    filt_func : None or callable, optional\n        If None, inspect all files for library dependencies. If callable,\n        accepts filename as argument, returns True if we should inspect the\n        file, False otherwise.\n\n    Returns\n    -------\n    lib_dict : dict\n        dictionary with (key, value) pairs of (``libpath``,\n        ``dependings_dict``).  ``libpath`` is library being depended on,\n        relative to wheel root path if within wheel tree.  ``dependings_dict``\n        is (key, value) of (``depending_lib_path``, ``install_name``).  Again,\n        ``depending_lib_path`` is library relative to wheel root path, if\n        within wheel tree.\n    \"\"\"\n    with TemporaryDirectory() as tmpdir:\n        zip2dir(wheel_fname, tmpdir)\n        lib_dict = tree_libs(tmpdir, filt_func)\n    return stripped_lib_dict(lib_dict, realpath(tmpdir) + os.path.sep)",
    "reference": "Return analysis of library dependencies with a Python wheel\n\n    Use this routine for a dump of the dependency tree.\n\n    Parameters\n    ----------\n    wheel_fname : str\n        Filename of wheel\n    filt_func : None or callable, optional\n        If None, inspect all files for library dependencies. If callable,\n        accepts filename as argument, returns True if we should inspect the\n        file, False otherwise.\n\n    Returns\n    -------\n    lib_dict : dict\n        dictionary with (key, value) pairs of (``libpath``,\n        ``dependings_dict``).  ``libpath`` is library being depended on,\n        relative to wheel root path if within wheel tree.  ``dependings_dict``\n        is (key, value) of (``depending_lib_path``, ``install_name``).  Again,\n        ``depending_lib_path`` is library relative to wheel root path, if\n        within wheel tree.",
    "generated": "Return analysis of library dependencies with a Python wheel\n\n    Use this routine for a dump of the dependency tree."
  },
  {
    "code": "def rewrite_record(bdist_dir):\n    \"\"\" Rewrite RECORD file with hashes for all files in `wheel_sdir`\n\n    Copied from :method:`wheel.bdist_wheel.bdist_wheel.write_record`\n\n    Will also unsign wheel\n\n    Parameters\n    ----------\n    bdist_dir : str\n        Path of unpacked wheel file\n    \"\"\"\n    info_dirs = glob.glob(pjoin(bdist_dir, '*.dist-info'))\n    if len(info_dirs) != 1:\n        raise WheelToolsError(\"Should be exactly one `*.dist_info` directory\")\n    record_path = pjoin(info_dirs[0], 'RECORD')\n    record_relpath = relpath(record_path, bdist_dir)\n    # Unsign wheel - because we're invalidating the record hash\n    sig_path = pjoin(info_dirs[0], 'RECORD.jws')\n    if exists(sig_path):\n        os.unlink(sig_path)\n\n    def walk():\n        for dir, dirs, files in os.walk(bdist_dir):\n            for f in files:\n                yield pjoin(dir, f)\n\n    def skip(path):\n        \"\"\"Wheel hashes every possible file.\"\"\"\n        return (path == record_relpath)\n\n    with _open_for_csv(record_path, 'w+') as record_file:\n        writer = csv.writer(record_file)\n        for path in walk():\n            relative_path = relpath(path, bdist_dir)\n            if skip(relative_path):\n                hash = ''\n                size = ''\n            else:\n                with open(path, 'rb') as f:\n                    data = f.read()\n                digest = hashlib.sha256(data).digest()\n                hash = 'sha256=' + native(urlsafe_b64encode(digest))\n                size = len(data)\n            path_for_record = relpath(\n                path, bdist_dir).replace(psep, '/')\n            writer.writerow((path_for_record, hash, size))",
    "reference": "Rewrite RECORD file with hashes for all files in `wheel_sdir`\n\n    Copied from :method:`wheel.bdist_wheel.bdist_wheel.write_record`\n\n    Will also unsign wheel\n\n    Parameters\n    ----------\n    bdist_dir : str\n        Path of unpacked wheel file",
    "generated": "Rewrite RECORD file with hashes for all files in `wheel_sdir`\n\n    Copied from :method:`wheel.bdist_wheel.bdist_wheel.write_record`\n\n    Will also unsign wheel\n\n    Parameters\n    ----------\n    bdist_dir : str\n        Path of unpacked wheel file\n   "
  },
  {
    "code": "def add_platforms(in_wheel, platforms, out_path=None, clobber=False):\n    \"\"\" Add platform tags `platforms` to `in_wheel` filename and WHEEL tags\n\n    Add any platform tags in `platforms` that are missing from `in_wheel`\n    filename.\n\n    Add any platform tags in `platforms` that are missing from `in_wheel`\n    ``WHEEL`` file.\n\n    Parameters\n    ----------\n    in_wheel : str\n        Filename of wheel to which to add platform tags\n    platforms : iterable\n        platform tags to add to wheel filename and WHEEL tags - e.g.\n        ``('macosx_10_9_intel', 'macosx_10_9_x86_64')\n    out_path : None or str, optional\n        Directory to which to write new wheel.  Default is directory containing\n        `in_wheel`\n    clobber : bool, optional\n        If True, overwrite existing output filename, otherwise raise error\n\n    Returns\n    -------\n    out_wheel : None or str\n        Absolute path of wheel file written, or None if no wheel file written.\n    \"\"\"\n    in_wheel = abspath(in_wheel)\n    out_path = dirname(in_wheel) if out_path is None else abspath(out_path)\n    wf = WheelFile(in_wheel)\n    info_fname = _get_wheelinfo_name(wf)\n    # Check what tags we have\n    in_fname_tags = wf.parsed_filename.groupdict()['plat'].split('.')\n    extra_fname_tags = [tag for tag in platforms if tag not in in_fname_tags]\n    in_wheel_base, ext = splitext(basename(in_wheel))\n    out_wheel_base = '.'.join([in_wheel_base] + list(extra_fname_tags))\n    out_wheel = pjoin(out_path, out_wheel_base + ext)\n    if exists(out_wheel) and not clobber:\n        raise WheelToolsError('Not overwriting {0}; set clobber=True '\n                              'to overwrite'.format(out_wheel))\n    with InWheelCtx(in_wheel) as ctx:\n        info = read_pkg_info(info_fname)\n        if info['Root-Is-Purelib'] == 'true':\n            raise WheelToolsError('Cannot add platforms to pure wheel')\n        in_info_tags = [tag for name, tag in info.items() if name == 'Tag']\n        # Python version, C-API version combinations\n        pyc_apis = ['-'.join(tag.split('-')[:2]) for tag in in_info_tags]\n        # unique Python version, C-API version combinations\n        pyc_apis = unique_by_index(pyc_apis)\n        # Add new platform tags for each Python version, C-API combination\n        required_tags = ['-'.join(tup) for tup in product(pyc_apis, platforms)]\n        needs_write = False\n        for req_tag in required_tags:\n            if req_tag in in_info_tags: continue\n            needs_write = True\n            info.add_header('Tag', req_tag)\n        if needs_write:\n            write_pkg_info(info_fname, info)\n            # Tell context manager to write wheel on exit by setting filename\n            ctx.out_wheel = out_wheel\n    return ctx.out_wheel",
    "reference": "Add platform tags `platforms` to `in_wheel` filename and WHEEL tags\n\n    Add any platform tags in `platforms` that are missing from `in_wheel`\n    filename.\n\n    Add any platform tags in `platforms` that are missing from `in_wheel`\n    ``WHEEL`` file.\n\n    Parameters\n    ----------\n    in_wheel : str\n        Filename of wheel to which to add platform tags\n    platforms : iterable\n        platform tags to add to wheel filename and WHEEL tags - e.g.\n        ``('macosx_10_9_intel', 'macosx_10_9_x86_64')\n    out_path : None or str, optional\n        Directory to which to write new wheel.  Default is directory containing\n        `in_wheel`\n    clobber : bool, optional\n        If True, overwrite existing output filename, otherwise raise error\n\n    Returns\n    -------\n    out_wheel : None or str\n        Absolute path of wheel file written, or None if no wheel file written.",
    "generated": "Add platform tags `platforms` to `in_wheel` filename and WHEEL tags\n    Add any platform tags in `platforms` that are missing from `in_wheel`\n    filename.\n\n    Add any platform tags in `platforms` that are missing from `in_wheel`\n    filename.\n\n    Add any platform tags\n    platforms : iterable\n        platform tags to add to wheel filename and WHEEL tags - e.g.\n        ``('macosx_10_9_intel', 'macosx_10_x86_x86"
  },
  {
    "code": "def temporal_betweenness_centrality(tnet=None, paths=None, calc='time'):\n    '''\n    Returns temporal betweenness centrality per node.\n\n    Parameters\n    -----------\n\n    Input should be *either* tnet or paths.\n\n    data : array or dict\n\n        Temporal network input (graphlet or contact). nettype: 'bu', 'bd'.\n\n    calc : str\n\n        either 'global' or 'time'\n\n    paths : pandas dataframe\n\n        Output of TenetoBIDS.networkmeasure.shortest_temporal_paths\n\n\n    Returns\n    --------\n\n    :close: array\n\n        normalized temporal betweenness centrality.\n\n            If calc = 'time', returns (node,time)\n\n            If calc = 'global', returns (node)\n\n    '''\n\n    if tnet is not None and paths is not None:\n        raise ValueError('Only network or path input allowed.')\n    if tnet is None and paths is None:\n        raise ValueError('No input.')\n    # if shortest paths are not calculated, calculate them\n    if tnet is not None:\n        paths = shortest_temporal_path(tnet)\n\n    bet = np.zeros([paths[['from', 'to']].max().max() +\n                    1, paths['t_start'].max()+1])\n\n    for row in paths.iterrows():\n        if (np.isnan(row[1]['path includes'])).all():\n            pass\n        else:\n            nodes_in_path = np.unique(np.concatenate(\n                row[1]['path includes'])).astype(int).tolist()\n            nodes_in_path.remove(row[1]['from'])\n            nodes_in_path.remove(row[1]['to'])\n            if len(nodes_in_path) > 0:\n                bet[nodes_in_path, row[1]['t_start']] += 1\n\n    # Normalise bet\n    bet = (1/((bet.shape[0]-1)*(bet.shape[0]-2))) * bet\n\n    if calc == 'global':\n        bet = np.mean(bet, axis=1)\n\n    return bet",
    "reference": "Returns temporal betweenness centrality per node.\n\n    Parameters\n    -----------\n\n    Input should be *either* tnet or paths.\n\n    data : array or dict\n\n        Temporal network input (graphlet or contact). nettype: 'bu', 'bd'.\n\n    calc : str\n\n        either 'global' or 'time'\n\n    paths : pandas dataframe\n\n        Output of TenetoBIDS.networkmeasure.shortest_temporal_paths\n\n\n    Returns\n    --------\n\n    :close: array\n\n        normalized temporal betweenness centrality.\n\n            If calc = 'time', returns (node,time)\n\n            If calc = 'global', returns (node)",
    "generated": "Returns temporal betweenness centrality per node.\n\n    Parameters\n    -----------\n\n    Input should be *either* tnet or paths.\n\n    data : array or dict\n\n        Temporal network input (graphlet or contact). nettype: 'bu', 'bd'.\n\n    calc : str\n        either 'global' or 'time'"
  },
  {
    "code": "def allegiance(community):\n    \"\"\"\n    Computes the allegiance matrix with values representing the probability that\n    nodes i and j were assigned to the same community by time-varying clustering methods.\n\n    parameters\n    ----------\n    community : array\n        array of community assignment of size node,time\n\n    returns\n    -------\n    P : array\n        module allegiance matrix, with P_ij probability that area i and j are in the same community\n\n    Reference:\n    ----------\n    Bassett, et al. (2013) \u201cRobust detection of dynamic community structure in networks\u201d, Chaos, 23, 1\n\n    \"\"\"\n    N = community.shape[0]\n    C = community.shape[1]\n    T = P = np.zeros([N, N])\n\n    for t in range(len(community[0, :])):\n        for i in range(len(community[:, 0])):\n            for j in range(len(community[:, 0])):\n                if i == j:\n                    continue\n                # T_ij indicates the number of times that i and j are assigned to the same community across time\n                if community[i][t] == community[j][t]:\n                    T[i, j] += 1\n\n    # module allegiance matrix, probability that ij were assigned to the same community\n    P = (1/C)*T\n\n    return P",
    "reference": "Computes the allegiance matrix with values representing the probability that\n    nodes i and j were assigned to the same community by time-varying clustering methods.\n\n    parameters\n    ----------\n    community : array\n        array of community assignment of size node,time\n\n    returns\n    -------\n    P : array\n        module allegiance matrix, with P_ij probability that area i and j are in the same community\n\n    Reference:\n    ----------\n    Bassett, et al. (2013) \u201cRobust detection of dynamic community structure in networks\u201d, Chaos, 23, 1",
    "generated": "Computes the allegiance matrix with values representing the probability that\n    nodes i and j were assigned to the same community by time-varying clustering methods.\n\n    parameters\n    ----------\n    community : array\n        array of community assignment of size node,time\n\n    returns\n    -------\n    P : array\n        module allegiance matrix, with P_ij probability that area i and j are in the same community\n\n    Reference:\n    ----------\n    Bassett, et al. (2013) \u201cRobust detection of dynamic community structure"
  },
  {
    "code": "def rand_poisson(nnodes, ncontacts, lam=1, nettype='bu', netinfo=None, netrep='graphlet'):\n    \"\"\"\n    Generate a random network where intervals between contacts are distributed by a poisson distribution\n\n    Parameters\n    ----------\n\n    nnodes : int\n        Number of nodes in networks\n\n    ncontacts : int or list\n        Number of expected contacts (i.e. edges). If list, number of contacts for each node.\n        Any zeros drawn are ignored so returned degree of network can be smaller than ncontacts.\n\n    lam : int or list\n        Expectation of interval.\n\n    nettype : str\n        'bu' or 'bd'\n\n    netinfo : dict\n        Dictionary of additional information\n\n    netrep : str\n        How the output should be.\n\n    If ncontacts is a list, so should lam.\n\n    Returns\n    -------\n        net : array or dict\n            Random network with intervals between active edges being Poisson distributed.\n\n    \"\"\"\n\n    if isinstance(ncontacts, list):\n        if len(ncontacts) != nnodes:\n            raise ValueError(\n                'Number of contacts, if a list, should be one per node')\n    if isinstance(lam, list):\n        if len(lam) != nnodes:\n            raise ValueError(\n                'Lambda value of Poisson distribution, if a list, should be one per node')\n    if isinstance(lam, list) and not isinstance(ncontacts, list) or not isinstance(lam, list) and isinstance(ncontacts, list):\n        raise ValueError(\n            'When one of lambda or ncontacts is given as a list, the other argument must also be a list.')\n\n    if nettype == 'bu':\n        edgen = int((nnodes*(nnodes-1))/2)\n    elif nettype == 'bd':\n        edgen = int(nnodes*nnodes)\n\n    if not isinstance(lam, list) and not isinstance(ncontacts, list):\n        icts = np.random.poisson(lam, size=(edgen, ncontacts))\n        net = np.zeros([edgen, icts.sum(axis=1).max()+1])\n        for n in range(edgen):\n            net[n, np.unique(np.cumsum(icts[n]))] = 1\n    else:\n        icts = []\n        ict_max = 0\n        for n in range(edgen):\n            icts.append(np.random.poisson(lam[n], size=ncontacts[n]))\n            if sum(icts[-1]) > ict_max:\n                ict_max = sum(icts[-1])\n        net = np.zeros([nnodes, ict_max+1])\n        for n in range(nnodes):\n            net[n, np.unique(np.cumsum(icts[n]))] = 1\n\n    if nettype == 'bu':\n        nettmp = np.zeros([nnodes, nnodes, net.shape[-1]])\n        ind = np.triu_indices(nnodes, k=1)\n        nettmp[ind[0], ind[1], :] = net\n        net = nettmp + nettmp.transpose([1, 0, 2])\n    elif nettype == 'bd':\n        net = net.reshape([nnodes, nnodes, net.shape[-1]], order='F')\n        net = set_diagonal(net, 0)\n\n    if netrep == 'contact':\n        if not netinfo:\n            netinfo = {}\n        netinfo['nettype'] = 'b' + nettype[-1]\n        net = graphlet2contact(net, netinfo)\n\n    return net",
    "reference": "Generate a random network where intervals between contacts are distributed by a poisson distribution\n\n    Parameters\n    ----------\n\n    nnodes : int\n        Number of nodes in networks\n\n    ncontacts : int or list\n        Number of expected contacts (i.e. edges). If list, number of contacts for each node.\n        Any zeros drawn are ignored so returned degree of network can be smaller than ncontacts.\n\n    lam : int or list\n        Expectation of interval.\n\n    nettype : str\n        'bu' or 'bd'\n\n    netinfo : dict\n        Dictionary of additional information\n\n    netrep : str\n        How the output should be.\n\n    If ncontacts is a list, so should lam.\n\n    Returns\n    -------\n        net : array or dict\n            Random network with intervals between active edges being Poisson distributed.",
    "generated": "Generate a random network where intervals between contacts are distributed by a poisson distribution\n\n    Parameters\n    ----------\n\n    nnodes : int\n        Number of nodes in networks\n\n    ncontacts : int or list\n        Number of expected contacts (i.e. edges). If list, number of contacts for each node.\n        Any zeros drawn are ignored so returned degree of network can be smaller than ncontacts.\n\n    nettype : str\n        '"
  },
  {
    "code": "def temporal_efficiency(tnet=None, paths=None, calc='global'):\n    r\"\"\"\n    Returns temporal efficiency estimate. BU networks only.\n\n    Parameters\n    ----------\n    Input should be *either* tnet or paths.\n\n    data : array or dict\n\n        Temporal network input (graphlet or contact). nettype: 'bu', 'bd'.\n\n    paths : pandas dataframe\n\n        Output of TenetoBIDS.networkmeasure.shortest_temporal_paths\n\n    calc : str\n        Options: 'global' (default) - measure averages over time and nodes;\n        'node' or 'node_from' average over nodes (i) and time. Giving average efficiency for i to j;\n        'node_to' measure average over nodes j and time;\n         Giving average efficiency using paths to j from  i;\n\n    Returns\n    -------\n\n    E : array\n        Global temporal efficiency\n\n    \"\"\"\n\n    if tnet is not None and paths is not None:\n        raise ValueError('Only network or path input allowed.')\n    if tnet is None and paths is None:\n        raise ValueError('No input.')\n    # if shortest paths are not calculated, calculate them\n    if tnet is not None:\n        paths = shortest_temporal_path(tnet)\n\n    pathmat = np.zeros([paths[['from', 'to']].max().max(\n    )+1, paths[['from', 'to']].max().max()+1, paths[['t_start']].max().max()+1]) * np.nan\n    pathmat[paths['from'].values, paths['to'].values,\n            paths['t_start'].values] = paths['temporal-distance']\n\n    # Calculate efficiency which is 1 over the mean path.\n    if calc == 'global':\n        eff = 1 / np.nanmean(pathmat)\n    elif calc == 'node' or calc == 'node_from':\n        eff = 1 / np.nanmean(np.nanmean(pathmat, axis=2), axis=1)\n    elif calc == 'node_to':\n        eff = 1 / np.nanmean(np.nanmean(pathmat, axis=2), axis=0)\n\n    return eff",
    "reference": "r\"\"\"\n    Returns temporal efficiency estimate. BU networks only.\n\n    Parameters\n    ----------\n    Input should be *either* tnet or paths.\n\n    data : array or dict\n\n        Temporal network input (graphlet or contact). nettype: 'bu', 'bd'.\n\n    paths : pandas dataframe\n\n        Output of TenetoBIDS.networkmeasure.shortest_temporal_paths\n\n    calc : str\n        Options: 'global' (default) - measure averages over time and nodes;\n        'node' or 'node_from' average over nodes (i) and time. Giving average efficiency for i to j;\n        'node_to' measure average over nodes j and time;\n         Giving average efficiency using paths to j from  i;\n\n    Returns\n    -------\n\n    E : array\n        Global temporal efficiency",
    "generated": "Returns temporal efficiency estimate. BU networks only.\n\n    Parameters\n    ----------\n    Input should be *either* tnet or paths.\n\n    data : array or dict\n\n        Temporal network input (graphlet or contact). nettype: 'bu', 'bd'.\n\n    paths : pandas dataframe\n\n        Output of TenetoBIDS.networkmeasure.shortest_temporal_paths\n\n    calc : str\n        Options: 'global' (default) - measure averages over time and nodes"
  },
  {
    "code": "def network_from_array(self, array):\n        \"\"\"impo\n        Defines a network from an array.\n\n        Parameters\n        ----------\n        array : array\n            3D numpy array.\n        \"\"\"\n        if len(array.shape) == 2:\n            array = np.array(array, ndmin=3).transpose([1, 2, 0])\n        teneto.utils.check_TemporalNetwork_input(array, 'array')\n        uvals = np.unique(array)\n        if len(uvals) == 2 and 1 in uvals and 0 in uvals:\n            i, j, t = np.where(array == 1)\n            self.network = pd.DataFrame(data={'i': i, 'j': j, 't': t})\n        else:\n            i, j, t = np.where(array != 0)\n            w = array[array != 0]\n            self.network = pd.DataFrame(\n                data={'i': i, 'j': j, 't': t, 'weight': w})\n        self.N = int(array.shape[0])\n        self.T = int(array.shape[-1])\n        self._update_network()",
    "reference": "impo\n        Defines a network from an array.\n\n        Parameters\n        ----------\n        array : array\n            3D numpy array.",
    "generated": "impo\n        Defines a network from an array.\n\n        Parameters\n        ----------\n        array : array\n            3D numpy array."
  },
  {
    "code": "def _drop_duplicate_ij(self):\n        \"\"\"\n        Drops duplicate entries from the network dataframe.\n        \"\"\"\n        self.network['ij'] = list(map(lambda x: tuple(sorted(x)), list(\n            zip(*[self.network['i'].values, self.network['j'].values]))))\n        self.network.drop_duplicates(['ij', 't'], inplace=True)\n        self.network.reset_index(inplace=True, drop=True)\n        self.network.drop('ij', inplace=True, axis=1)",
    "reference": "Drops duplicate entries from the network dataframe.",
    "generated": "Deletes duplicate entries from the network dataframe.\n       "
  },
  {
    "code": "def _drop_diagonal(self):\n        \"\"\"\n        Drops self-contacts from the network dataframe.\n        \"\"\"\n        self.network = self.network.where(\n            self.network['i'] != self.network['j']).dropna()\n        self.network.reset_index(inplace=True, drop=True)",
    "reference": "Drops self-contacts from the network dataframe.",
    "generated": "Drops self-contacts from the network dataframe."
  },
  {
    "code": "def add_edge(self, edgelist):\n        \"\"\"\n        Adds an edge from network.\n\n        Parameters\n        ----------\n\n        edgelist : list\n            a list (or list of lists) containing the i,j and t indicies to be added. For weighted networks list should also contain a 'weight' key.\n\n        Returns\n        --------\n            Updates TenetoBIDS.network dataframe with new edge\n        \"\"\"\n        if not isinstance(edgelist[0], list):\n            edgelist = [edgelist]\n        teneto.utils.check_TemporalNetwork_input(edgelist, 'edgelist')\n        if len(edgelist[0]) == 4:\n            colnames = ['i', 'j', 't', 'weight']\n        elif len(edgelist[0]) == 3:\n            colnames = ['i', 'j', 't']\n        if self.hdf5:\n            with pd.HDFStore(self.network) as hdf:\n                rows = hdf.get_storer('network').nrows\n                hdf.append('network', pd.DataFrame(edgelist, columns=colnames, index=np.arange(\n                    rows, rows+len(edgelist))), format='table', data_columns=True)\n            edgelist = np.array(edgelist)\n            if np.max(edgelist[:, :2]) > self.netshape[0]:\n                self.netshape[0] = np.max(edgelist[:, :2])\n            if np.max(edgelist[:, 2]) > self.netshape[1]:\n                self.netshape[1] = np.max(edgelist[:, 2])\n        else:\n            newedges = pd.DataFrame(edgelist, columns=colnames)\n            self.network = pd.concat(\n                [self.network, newedges], ignore_index=True, sort=True)\n            self._update_network()",
    "reference": "Adds an edge from network.\n\n        Parameters\n        ----------\n\n        edgelist : list\n            a list (or list of lists) containing the i,j and t indicies to be added. For weighted networks list should also contain a 'weight' key.\n\n        Returns\n        --------\n            Updates TenetoBIDS.network dataframe with new edge",
    "generated": "Adds an edge from network.\n\n        Parameters\n        ----------\n\n        edgelist : list\n            a list (or list of lists) containing the i,j and t indicies to be added. For weighted networks list should also contain a 'weight' key.\n\n        Returns\n        --------\n            Updates TenetoBIDS.network dataframe with new edge\n       "
  },
  {
    "code": "def drop_edge(self, edgelist):\n        \"\"\"\n        Removes an edge from network.\n\n        Parameters\n        ----------\n\n        edgelist : list\n            a list (or list of lists) containing the i,j and t indicies to be removes.\n\n        Returns\n        --------\n            Updates TenetoBIDS.network dataframe\n        \"\"\"\n        if not isinstance(edgelist[0], list):\n            edgelist = [edgelist]\n        teneto.utils.check_TemporalNetwork_input(edgelist, 'edgelist')\n        if self.hdf5:\n            with pd.HDFStore(self.network) as hdf:\n                for e in edgelist:\n                    hdf.remove(\n                        'network', 'i == ' + str(e[0]) + ' & ' + 'j == ' + str(e[1]) + ' & ' + 't == ' + str(e[2]))\n            print('HDF5 delete warning. This will not reduce the size of the file.')\n        else:\n            for e in edgelist:\n                idx = self.network[(self.network['i'] == e[0]) & (\n                    self.network['j'] == e[1]) & (self.network['t'] == e[2])].index\n                self.network.drop(idx, inplace=True)\n            self.network.reset_index(inplace=True, drop=True)\n            self._update_network()",
    "reference": "Removes an edge from network.\n\n        Parameters\n        ----------\n\n        edgelist : list\n            a list (or list of lists) containing the i,j and t indicies to be removes.\n\n        Returns\n        --------\n            Updates TenetoBIDS.network dataframe",
    "generated": "Removes an edge from network.\n\n        Parameters\n        ----------\n\n        edgelist : list\n            a list (or list of lists) containing the i,j and t indicies to be removes.\n\n        Returns\n        --------\n            Updates TenetoBIDS.network dataframe\n       "
  },
  {
    "code": "def calc_networkmeasure(self, networkmeasure, **measureparams):\n        \"\"\"\n        Calculate network measure.\n\n        Parameters\n        -----------\n        networkmeasure : str\n            Function to call. Functions available are in teneto.networkmeasures\n\n        measureparams : kwargs\n            kwargs for teneto.networkmeasure.[networkmeasure]\n        \"\"\"\n        availablemeasures = [f for f in dir(\n            teneto.networkmeasures) if not f.startswith('__')]\n        if networkmeasure not in availablemeasures:\n            raise ValueError(\n                'Unknown network measure. Available network measures are: ' + ', '.join(availablemeasures))\n        funs = inspect.getmembers(teneto.networkmeasures)\n        funs = {m[0]: m[1] for m in funs if not m[0].startswith('__')}\n        measure = funs[networkmeasure](self, **measureparams)\n        return measure",
    "reference": "Calculate network measure.\n\n        Parameters\n        -----------\n        networkmeasure : str\n            Function to call. Functions available are in teneto.networkmeasures\n\n        measureparams : kwargs\n            kwargs for teneto.networkmeasure.[networkmeasure]",
    "generated": "Calculate network measure.\n\n        Parameters\n        -----------\n        networkmeasure : str\n            Function to call. Functions available are in teneto.networkmeasure.[networkmeasure]"
  },
  {
    "code": "def generatenetwork(self, networktype, **networkparams):\n        \"\"\"\n        Generate a network\n\n        Parameters\n        -----------\n        networktype : str\n            Function to call. Functions available are in teneto.generatenetwork\n\n        measureparams : kwargs\n            kwargs for teneto.generatenetwork.[networktype]\n\n        Returns\n        --------\n        TenetoBIDS.network is made with the generated network.\n        \"\"\"\n        availabletypes = [f for f in dir(\n            teneto.generatenetwork) if not f.startswith('__')]\n        if networktype not in availabletypes:\n            raise ValueError(\n                'Unknown network measure. Available networks to generate are: ' + ', '.join(availabletypes))\n        funs = inspect.getmembers(teneto.generatenetwork)\n        funs = {m[0]: m[1] for m in funs if not m[0].startswith('__')}\n        network = funs[networktype](**networkparams)\n        self.network_from_array(network)\n        if self.nettype[1] == 'u':\n            self._drop_duplicate_ij()",
    "reference": "Generate a network\n\n        Parameters\n        -----------\n        networktype : str\n            Function to call. Functions available are in teneto.generatenetwork\n\n        measureparams : kwargs\n            kwargs for teneto.generatenetwork.[networktype]\n\n        Returns\n        --------\n        TenetoBIDS.network is made with the generated network.",
    "generated": "Generate a network\n\n        Parameters\n        -----------\n        networktype : str\n            Function to call. Functions available are in teneto.generatenetwork\n\n        measureparams : kwargs\n            kwargs for teneto.generatenetwork.[networktype]\n\n        Returns\n        --------\n        TenetoBIDS.network is made with the generated network."
  },
  {
    "code": "def save_aspickle(self, fname):\n        \"\"\"\n        Saves object as pickle.\n\n        fname : str\n            file path.\n        \"\"\"\n        if fname[-4:] != '.pkl':\n            fname += '.pkl'\n        with open(fname, 'wb') as f:\n            pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)",
    "reference": "Saves object as pickle.\n\n        fname : str\n            file path.",
    "generated": "Save object as pickle.\n\n        fname : str\n            file path."
  },
  {
    "code": "def postpro_fisher(data, report=None):\n    \"\"\"\n    Performs fisher transform on everything in data.\n\n    If report variable is passed, this is added to the report.\n    \"\"\"\n    if not report:\n        report = {}\n    # Due to rounding errors\n    data[data < -0.99999999999999] = -1\n    data[data > 0.99999999999999] = 1\n    fisher_data = 0.5 * np.log((1 + data) / (1 - data))\n    report['fisher'] = {}\n    report['fisher']['performed'] = 'yes'\n    #report['fisher']['diagonal'] = 'zeroed'\n    return fisher_data, report",
    "reference": "Performs fisher transform on everything in data.\n\n    If report variable is passed, this is added to the report.",
    "generated": "Performs fisher transform on everything in data.\n\n    If report variable is passed, this is added to the report."
  },
  {
    "code": "def postpro_boxcox(data, report=None):\n    \"\"\"\n    Performs box cox transform on everything in data.\n\n    If report variable is passed, this is added to the report.\n    \"\"\"\n    if not report:\n        report = {}\n    # Note the min value of all time series will now be at least 1.\n    mindata = 1 - np.nanmin(data)\n    data = data + mindata\n    ind = np.triu_indices(data.shape[0], k=1)\n\n    boxcox_list = np.array([sp.stats.boxcox(np.squeeze(\n        data[ind[0][n], ind[1][n], :])) for n in range(0, len(ind[0]))])\n\n    boxcox_data = np.zeros(data.shape)\n    boxcox_data[ind[0], ind[1], :] = np.vstack(boxcox_list[:, 0])\n    boxcox_data[ind[1], ind[0], :] = np.vstack(boxcox_list[:, 0])\n\n    bccheck = np.array(np.transpose(boxcox_data, [2, 0, 1]))\n    bccheck = (bccheck - bccheck.mean(axis=0)) / bccheck.std(axis=0)\n    bccheck = np.squeeze(np.mean(bccheck, axis=0))\n    np.fill_diagonal(bccheck, 0)\n\n    report['boxcox'] = {}\n    report['boxcox']['performed'] = 'yes'\n    report['boxcox']['lambda'] = [\n        tuple([ind[0][n], ind[1][n], boxcox_list[n, -1]]) for n in range(0, len(ind[0]))]\n    report['boxcox']['shift'] = mindata\n    report['boxcox']['shited_to'] = 1\n\n    if np.sum(np.isnan(bccheck)) > 0:\n        report['boxcox'] = {}\n        report['boxcox']['performed'] = 'FAILED'\n        report['boxcox']['failure_reason'] = (\n            'Box cox transform is returning edges with uniform values through time. '\n            'This is probabaly due to one or more outliers or a very skewed distribution. '\n            'Have you corrected for sources of noise (e.g. movement)? '\n            'If yes, some time-series might need additional transforms to approximate to Gaussian.'\n        )\n        report['boxcox']['failure_consequence'] = (\n            'Box cox transform was skipped from the postprocess pipeline.'\n        )\n        boxcox_data = data - mindata\n        error_msg = ('TENETO WARNING: Box Cox transform problem. \\n'\n                     'Box Cox transform not performed. \\n'\n                     'See report for more details.')\n        print(error_msg)\n\n    return boxcox_data, report",
    "reference": "Performs box cox transform on everything in data.\n\n    If report variable is passed, this is added to the report.",
    "generated": "Performs box cox transform on everything in data.\n\n    If report variable is passed, this is added to the report."
  },
  {
    "code": "def binarize_rdp(netin, level, sign='pos', axis='time'):\n    \"\"\"\n    Binarizes a network based on RDP compression.\n\n    Parameters\n    ----------\n\n    netin : array or dict\n        Network (graphlet or contact representation),\n    level : float\n        Delta parameter which is the tolorated error in RDP compression.\n    sign : str, default='pos'\n        States the sign of the thresholding. Can be 'pos', 'neg' or 'both'. If \"neg\", only negative values are thresholded and vice versa.\n\n    Returns\n    -------\n\n    netout : array or dict (dependning on input)\n        Binarized network\n    \"\"\"\n    netin, netinfo = process_input(netin, ['C', 'G', 'TO'])\n    trajectory = rdp(netin, level)\n\n    contacts = []\n    # Use the trajectory points as threshold\n    for n in range(trajectory['index'].shape[0]):\n        if sign == 'pos':\n            sel = trajectory['trajectory_points'][n][trajectory['trajectory']\n                                                     [n][trajectory['trajectory_points'][n]] > 0]\n        elif sign == 'neg':\n            sel = trajectory['trajectory_points'][n][trajectory['trajectory']\n                                                     [n][trajectory['trajectory_points'][n]] < 0]\n        else:\n            sel = trajectory['trajectory_points']\n        i_ind = np.repeat(trajectory['index'][n, 0], len(sel))\n        j_ind = np.repeat(trajectory['index'][n, 1], len(sel))\n        contacts.append(np.array([i_ind, j_ind, sel]).transpose())\n    contacts = np.concatenate(contacts)\n\n    # Create output dictionary\n    netout = dict(netinfo)\n    netout['contacts'] = contacts\n    netout['nettype'] = 'b' + netout['nettype'][1]\n    netout['dimord'] = 'node,node,time'\n    netout['timetype'] = 'discrete'\n    netout['diagonal'] = 0\n    # If input is graphlet, output graphlet\n    if netinfo['inputtype'] == 'G':\n        netout = contact2graphlet(netout)\n    else:\n        netout.pop('inputtype')\n\n    return netout",
    "reference": "Binarizes a network based on RDP compression.\n\n    Parameters\n    ----------\n\n    netin : array or dict\n        Network (graphlet or contact representation),\n    level : float\n        Delta parameter which is the tolorated error in RDP compression.\n    sign : str, default='pos'\n        States the sign of the thresholding. Can be 'pos', 'neg' or 'both'. If \"neg\", only negative values are thresholded and vice versa.\n\n    Returns\n    -------\n\n    netout : array or dict (dependning on input)\n        Binarized network",
    "generated": "Binarizes a network based on RDP compression.\n\n    Parameters\n    ----------\n\n    netin : array or dict\n        Network (graphlet or contact representation),\n    level : float\n        Delta parameter which is the tolorated error in RDP compression.\n    sign : str, default='pos'\n        States the sign of the thresholding. Can be 'pos', 'neg' or 'both'. If \"pos\", only negative values are thresholded and vice versa."
  },
  {
    "code": "def binarize(netin, threshold_type, threshold_level, sign='pos', axis='time'):\n    \"\"\"\n    Binarizes a network, returning the network. General wrapper function for different binarization functions.\n\n    Parameters\n    ----------\n\n    netin : array or dict\n       Network (graphlet or contact representation),\n\n    threshold_type : str\n        What type of thresholds to make binarization. Options: 'rdp', 'percent', 'magnitude'.\n\n    threshold_level : str\n        Paramter dependent on threshold type.\n        If 'rdp', it is the delta (i.e. error allowed in compression).\n        If 'percent', it is the percentage to keep (e.g. 0.1, means keep 10% of signal).\n        If 'magnitude', it is the amplitude of signal to keep.\n\n    sign : str, default='pos'\n        States the sign of the thresholding. Can be 'pos', 'neg' or 'both'. If \"neg\", only negative values are thresholded and vice versa.\n\n    axis : str\n        Threshold over specfied axis. Valid for percent and rdp. Can be time or graphlet.\n\n    Returns\n    -------\n\n    netout : array or dict (depending on input)\n        Binarized network\n\n    \"\"\"\n    if threshold_type == 'percent':\n        netout = binarize_percent(netin, threshold_level, sign, axis)\n    elif threshold_type == 'magnitude':\n        netout = binarize_magnitude(netin, threshold_level, sign)\n    elif threshold_type == 'rdp':\n        netout = binarize_rdp(netin, threshold_level, sign, axis)\n    else:\n        raise ValueError('Unknown value to parameter: threshold_type.')\n    return netout",
    "reference": "Binarizes a network, returning the network. General wrapper function for different binarization functions.\n\n    Parameters\n    ----------\n\n    netin : array or dict\n       Network (graphlet or contact representation),\n\n    threshold_type : str\n        What type of thresholds to make binarization. Options: 'rdp', 'percent', 'magnitude'.\n\n    threshold_level : str\n        Paramter dependent on threshold type.\n        If 'rdp', it is the delta (i.e. error allowed in compression).\n        If 'percent', it is the percentage to keep (e.g. 0.1, means keep 10% of signal).\n        If 'magnitude', it is the amplitude of signal to keep.\n\n    sign : str, default='pos'\n        States the sign of the thresholding. Can be 'pos', 'neg' or 'both'. If \"neg\", only negative values are thresholded and vice versa.\n\n    axis : str\n        Threshold over specfied axis. Valid for percent and rdp. Can be time or graphlet.\n\n    Returns\n    -------\n\n    netout : array or dict (depending on input)\n        Binarized network",
    "generated": "Binarizes a network, returning the network. General wrapper function for different binarization functions.\n\n    Parameters\n    ----------\n\n    netin : array or dict\n       Network (graphlet or contact representation),\n\n    threshold_type : str\n        What type of thresholds to make binarization. Options: 'rdp', 'percent', 'magnitude'.\n\n    threshold_level : str\n        Paramter dependent on threshold type.\n        If 'rdp', it is the amplitude of signal to keep."
  },
  {
    "code": "def process_input(netIn, allowedformats, outputformat='G'):\n    \"\"\"\n    Takes input network and checks what the input is.\n\n    Parameters\n    ----------\n\n    netIn : array, dict, or TemporalNetwork\n        Network (graphlet, contact or object)\n    allowedformats : str\n        Which format of network objects that are allowed. Options: 'C', 'TN', 'G'.\n    outputformat: str, default=G\n        Target output format. Options: 'C' or 'G'.\n\n\n    Returns\n    -------\n\n    C : dict\n\n    OR\n\n    G : array\n        Graphlet representation.\n    netInfo : dict\n        Metainformation about network.\n\n    OR\n\n    tnet : object\n        object of TemporalNetwork class\n\n    \"\"\"\n    inputtype = checkInput(netIn)\n    # Convert TN to G representation\n    if inputtype == 'TN' and 'TN' in allowedformats and outputformat != 'TN':\n        G = netIn.df_to_array()\n        netInfo = {'nettype': netIn.nettype, 'netshape': netIn.netshape}\n    elif inputtype == 'TN' and 'TN' in allowedformats and outputformat == 'TN':\n        TN = netIn\n    elif inputtype == 'C' and 'C' in allowedformats and outputformat == 'G':\n        G = contact2graphlet(netIn)\n        netInfo = dict(netIn)\n        netInfo.pop('contacts')\n    elif inputtype == 'C' and 'C' in allowedformats and outputformat == 'TN':\n        TN = TemporalNetwork(from_dict=netIn)\n    elif inputtype == 'G' and 'G' in allowedformats and outputformat == 'TN':\n        TN = TemporalNetwork(from_array=netIn)\n    # Get network type if not set yet\n    elif inputtype == 'G' and 'G' in allowedformats:\n        netInfo = {}\n        netInfo['netshape'] = netIn.shape\n        netInfo['nettype'] = gen_nettype(netIn)\n        G = netIn\n    elif inputtype == 'C' and outputformat == 'C':\n        pass\n    else:\n        raise ValueError('Input invalid.')\n    if outputformat == 'TN' and not isinstance(TN.network, str):\n        TN.network['i'] = TN.network['i'].astype(int)\n        TN.network['j'] = TN.network['j'].astype(int)\n        TN.network['t'] = TN.network['t'].astype(int)\n    if outputformat == 'C' or outputformat == 'G':\n        netInfo['inputtype'] = inputtype\n    if inputtype != 'C' and outputformat == 'C':\n        C = graphlet2contact(G, netInfo)\n    if outputformat == 'G':\n        return G, netInfo\n    elif outputformat == 'C':\n        return C\n    elif outputformat == 'TN':\n        return TN",
    "reference": "Takes input network and checks what the input is.\n\n    Parameters\n    ----------\n\n    netIn : array, dict, or TemporalNetwork\n        Network (graphlet, contact or object)\n    allowedformats : str\n        Which format of network objects that are allowed. Options: 'C', 'TN', 'G'.\n    outputformat: str, default=G\n        Target output format. Options: 'C' or 'G'.\n\n\n    Returns\n    -------\n\n    C : dict\n\n    OR\n\n    G : array\n        Graphlet representation.\n    netInfo : dict\n        Metainformation about network.\n\n    OR\n\n    tnet : object\n        object of TemporalNetwork class",
    "generated": "Reads input network and checks what the input is.\n\n    Parameters\n    ----------\n\n    netIn : array, dict, or TemporalNetwork\n        Network (graphlet, contact or object)\n    allowedformats : str\n        Which format of network objects that are allowed. Options: 'C', 'TN', 'G'.\n    outputformat: str, default=G\n        Target output format. Options: 'C' or 'G'.\n    outputformat: str, default=G\n        Target output format. Options: 'C' or 'G'."
  },
  {
    "code": "def clean_community_indexes(communityID):\n    \"\"\"\n    Takes input of community assignments. Returns reindexed community assignment by using smallest numbers possible.\n\n    Parameters\n    ----------\n\n    communityID : array-like\n        list or array of integers. Output from community detection algorithems.\n\n    Returns\n    -------\n\n    new_communityID : array\n        cleaned list going from 0 to len(np.unique(communityID))-1\n\n    Note\n    -----\n\n    Behaviour of funciton entails that the lowest community integer in communityID will recieve the lowest integer in new_communityID.\n\n    \"\"\"\n    communityID = np.array(communityID)\n    cid_shape = communityID.shape\n    if len(cid_shape) > 1:\n        communityID = communityID.flatten()\n    new_communityID = np.zeros(len(communityID))\n    for i, n in enumerate(np.unique(communityID)):\n        new_communityID[communityID == n] = i\n    if len(cid_shape) > 1:\n        new_communityID = new_communityID.reshape(cid_shape)\n    return new_communityID",
    "reference": "Takes input of community assignments. Returns reindexed community assignment by using smallest numbers possible.\n\n    Parameters\n    ----------\n\n    communityID : array-like\n        list or array of integers. Output from community detection algorithems.\n\n    Returns\n    -------\n\n    new_communityID : array\n        cleaned list going from 0 to len(np.unique(communityID))-1\n\n    Note\n    -----\n\n    Behaviour of funciton entails that the lowest community integer in communityID will recieve the lowest integer in new_communityID.",
    "generated": "Returns reindexed community assignment by using smallest numbers possible.\n\n    Parameters\n    ----------\n\n    communityID : array-like\n        list or array of integers. Output from community detection algorithems."
  },
  {
    "code": "def multiple_contacts_get_values(C):\n    \"\"\"\n    Given an contact representation with repeated contacts, this function removes duplicates and creates a value\n\n    Parameters\n    ----------\n\n    C : dict\n\n        contact representation with multiple repeated contacts.\n\n    Returns\n    -------\n\n    :C_out: dict\n\n        Contact representation with duplicate contacts removed and the number of duplicates is now in the 'values' field.\n\n    \"\"\"\n    d = collections.OrderedDict()\n    for c in C['contacts']:\n        ct = tuple(c)\n        if ct in d:\n            d[ct] += 1\n        else:\n            d[ct] = 1\n\n    new_contacts = []\n    new_values = []\n    for (key, value) in d.items():\n        new_values.append(value)\n        new_contacts.append(key)\n    C_out = C\n    C_out['contacts'] = new_contacts\n    C_out['values'] = new_values\n    return C_out",
    "reference": "Given an contact representation with repeated contacts, this function removes duplicates and creates a value\n\n    Parameters\n    ----------\n\n    C : dict\n\n        contact representation with multiple repeated contacts.\n\n    Returns\n    -------\n\n    :C_out: dict\n\n        Contact representation with duplicate contacts removed and the number of duplicates is now in the 'values' field.",
    "generated": "Given an contact representation with repeated contacts, this function removes duplicates and creates a value\n\n    Parameters\n    ----------\n\n    C : dict\n        contact representation with multiple repeated contacts.\n\n    Returns\n    ----------\n\n    :C_out: dict\n        Contact representation with multiple repeated contacts.\n\n    Returns\n    -------\n\n    :C_out: dict\n        Contact representation with multiple repeated contacts.\n\n    Returns\n    ----------\n\n    :C_out: dict\n        Contact representation with multiple repeated contacts."
  },
  {
    "code": "def check_distance_funciton_input(distance_func_name, netinfo):\n    \"\"\"\n    Funciton checks distance_func_name, if it is specified as 'default'. Then given the type of the network selects a default distance function.\n\n    Parameters\n    ----------\n\n    distance_func_name : str\n        distance function name.\n\n    netinfo : dict\n        the output of utils.process_input\n\n    Returns\n    -------\n\n    distance_func_name : str\n        distance function name.\n    \"\"\"\n\n    if distance_func_name == 'default' and netinfo['nettype'][0] == 'b':\n        print('Default distance funciton specified. As network is binary, using Hamming')\n        distance_func_name = 'hamming'\n    elif distance_func_name == 'default' and netinfo['nettype'][0] == 'w':\n        distance_func_name = 'euclidean'\n        print(\n            'Default distance funciton specified. '\n            'As network is weighted, using Euclidean')\n\n    return distance_func_name",
    "reference": "Funciton checks distance_func_name, if it is specified as 'default'. Then given the type of the network selects a default distance function.\n\n    Parameters\n    ----------\n\n    distance_func_name : str\n        distance function name.\n\n    netinfo : dict\n        the output of utils.process_input\n\n    Returns\n    -------\n\n    distance_func_name : str\n        distance function name.",
    "generated": "Funciton checks distance_func_name, if it is specified as 'default'. Then given the type of the network selects a default distance function.\n\n    Parameters\n    ----------\n\n    distance_func_name : str\n        distance function name.\n\n    netinfo : dict\n        the output of utils.process_input\n\n    Returns\n    -------\n\n    distance_func_name : str\n        distance function name.\n\n    netinfo : dict\n        the output of utils.process_input\n\n    Returns\n    -------\n\n    distance_func_name :"
  },
  {
    "code": "public static boolean check(String passwd, String hashed) {\n        try {\n            String[] parts = hashed.split(\"\\\\$\");\n\n            if (parts.length != 5 || !parts[1].equals(\"s0\")) {\n                throw new IllegalArgumentException(\"Invalid hashed value\");\n            }\n\n            long params = Long.parseLong(parts[2], 16);\n            byte[] salt = decode(parts[3].toCharArray());\n            byte[] derived0 = decode(parts[4].toCharArray());\n\n            int N = (int) Math.pow(2, params >> 16 & 0xffff);\n            int r = (int) params >> 8 & 0xff;\n            int p = (int) params      & 0xff;\n\n            byte[] derived1 = SCrypt.scrypt(passwd.getBytes(\"UTF-8\"), salt, N, r, p, 32);\n\n            if (derived0.length != derived1.length) return false;\n\n            int result = 0;\n            for (int i = 0; i < derived0.length; i++) {\n                result |= derived0[i] ^ derived1[i];\n            }\n            return result == 0;\n        } catch (UnsupportedEncodingException e) {\n            throw new IllegalStateException(\"JVM doesn't support UTF-8?\");\n        } catch (GeneralSecurityException e) {\n            throw new IllegalStateException(\"JVM doesn't support SHA1PRNG or HMAC_SHA256?\");\n        }\n    }",
    "reference": "Compare the supplied plaintext password to a hashed password.\n\n@param   passwd  Plaintext password.\n@param   hashed  scrypt hashed password.\n\n@return true if passwd matches hashed value.",
    "generated": "Check the hashed value.\n\n@param passwd the password\n@param hashed the hashed value\n@return true if hashed value is valid"
  },
  {
    "code": "public static Platform detect() throws UnsupportedPlatformException {\n        String osArch = getProperty(\"os.arch\");\n        String osName = getProperty(\"os.name\");\n\n        for (Arch arch : Arch.values()) {\n            if (arch.pattern.matcher(osArch).matches()) {\n                for (OS os : OS.values()) {\n                    if (os.pattern.matcher(osName).matches()) {\n                        return new Platform(arch, os);\n                    }\n                }\n            }\n        }\n\n        String msg = String.format(\"Unsupported platform %s %s\", osArch, osName);\n        throw new UnsupportedPlatformException(msg);\n    }",
    "reference": "Attempt to detect the current platform.\n\n@return The current platform.\n\n@throws UnsupportedPlatformException if the platform cannot be detected.",
    "generated": "Detect a platform.\n\n@param osArch the platform\n@param osName the platform\n@return the platform"
  },
  {
    "code": "public <T> T getNodeMetaData(Object key) {\n        if (metaDataMap == null) {\n            return (T) null;\n        }\n        return (T) metaDataMap.get(key);\n    }",
    "reference": "Gets the node meta data.\n\n@param key - the meta data key\n@return the node meta data value for this key",
    "generated": "Returns the node meta data for the given key.\n\n@param key key\n@return node meta data"
  },
  {
    "code": "public void copyNodeMetaData(ASTNode other) {\n        if (other.metaDataMap == null) {\n            return;\n        }\n        if (metaDataMap == null) {\n            metaDataMap = new ListHashMap();\n        }\n        metaDataMap.putAll(other.metaDataMap);\n    }",
    "reference": "Copies all node meta data from the other node to this one\n@param other - the other node",
    "generated": "Copy the node meta data.\n\n@param other ASTNode"
  },
  {
    "code": "public void setNodeMetaData(Object key, Object value) {\n        if (key==null) throw new GroovyBugError(\"Tried to set meta data with null key on \"+this+\".\");\n        if (metaDataMap == null) {\n            metaDataMap = new ListHashMap();\n        }\n        Object old = metaDataMap.put(key,value);\n        if (old!=null) throw new GroovyBugError(\"Tried to overwrite existing meta data \"+this+\".\");\n    }",
    "reference": "Sets the node meta data.\n\n@param key - the meta data key\n@param value - the meta data value\n@throws GroovyBugError if key is null or there is already meta\ndata under that key",
    "generated": "Set the meta data with the given key.\n\n@param key value"
  },
  {
    "code": "public Object putNodeMetaData(Object key, Object value) {\n        if (key == null) throw new GroovyBugError(\"Tried to set meta data with null key on \" + this + \".\");\n        if (metaDataMap == null) {\n            metaDataMap = new ListHashMap();\n        }\n        return metaDataMap.put(key, value);\n    }",
    "reference": "Sets the node meta data but allows overwriting values.\n\n@param key   - the meta data key\n@param value - the meta data value\n@return the old node meta data value for this key\n@throws GroovyBugError if key is null",
    "generated": "Put the meta data with the given key."
  },
  {
    "code": "public void removeNodeMetaData(Object key) {\n        if (key==null) throw new GroovyBugError(\"Tried to remove meta data with null key \"+this+\".\");\n        if (metaDataMap == null) {\n            return;\n        }\n        metaDataMap.remove(key);\n    }",
    "reference": "Removes a node meta data entry.\n\n@param key - the meta data key\n@throws GroovyBugError if the key is null",
    "generated": "Remove meta data with the given key.\n\n@param key the key to remove\n@return"
  },
  {
    "code": "public RangeInfo subListBorders(int size) {\n        if (inclusive == null) throw new IllegalStateException(\"Should not call subListBorders on a non-inclusive aware IntRange\");\n        int tempFrom = from;\n        if (tempFrom < 0) {\n            tempFrom += size;\n        }\n        int tempTo = to;\n        if (tempTo < 0) {\n            tempTo += size;\n        }\n        if (tempFrom > tempTo) {\n            return new RangeInfo(inclusive ? tempTo : tempTo + 1, tempFrom + 1, true);\n        }\n        return new RangeInfo(tempFrom, inclusive ? tempTo + 1 : tempTo, false);\n    }",
    "reference": "A method for determining from and to information when using this IntRange to index an aggregate object of the specified size.\nNormally only used internally within Groovy but useful if adding range indexing support for your own aggregates.\n\n@param size the size of the aggregate being indexed\n@return the calculated range information (with 1 added to the to value, ready for providing to subList",
    "generated": "Returns the subListBorders of the specified size.\n\n@param size size"
  },
  {
    "code": "protected void createSetterMethod(ClassNode declaringClass, PropertyNode propertyNode, String setterName, Statement setterBlock) {\r\n        MethodNode setter = new MethodNode(\r\n                setterName,\r\n                propertyNode.getModifiers(),\r\n                ClassHelper.VOID_TYPE,\r\n                params(param(propertyNode.getType(), \"value\")),\r\n                ClassNode.EMPTY_ARRAY,\r\n                setterBlock);\r\n        setter.setSynthetic(true);\r\n        // add it to the class\r\n        declaringClass.addMethod(setter);\r\n    }",
    "reference": "Creates a setter method with the given body.\n\n@param declaringClass the class to which we will add the setter\n@param propertyNode          the field to back the setter\n@param setterName     the name of the setter\n@param setterBlock    the statement representing the setter block",
    "generated": "Creates a setter method."
  },
  {
    "code": "public void applyToPrimaryClassNodes(PrimaryClassNodeOperation body) throws CompilationFailedException {\n        Iterator classNodes = getPrimaryClassNodes(body.needSortedInput()).iterator();\n        while (classNodes.hasNext()) {\n            SourceUnit context = null;\n            try {\n                ClassNode classNode = (ClassNode) classNodes.next();\n                context = classNode.getModule().getContext();\n                if (context == null || context.phase < phase || (context.phase == phase && !context.phaseComplete)) {\n                    int offset = 1;\n                    Iterator<InnerClassNode> iterator = classNode.getInnerClasses();\n                    while (iterator.hasNext()) {\n                        iterator.next();\n                        offset++;\n                    }\n                    body.call(context, new GeneratorContext(this.ast, offset), classNode);\n                }\n            } catch (CompilationFailedException e) {\n                // fall through, getErrorReporter().failIfErrors() will trigger\n            } catch (NullPointerException npe) {\n                GroovyBugError gbe = new GroovyBugError(\"unexpected NullpointerException\", npe);\n                changeBugText(gbe, context);\n                throw gbe;\n            } catch (GroovyBugError e) {\n                changeBugText(e, context);\n                throw e;\n            } catch (NoClassDefFoundError e) {\n                // effort to get more logging in case a dependency of a class is loaded\n                // although it shouldn't have\n                convertUncaughtExceptionToCompilationError(e);\n            } catch (Exception e) {\n                convertUncaughtExceptionToCompilationError(e);\n            }\n        }\n\n        getErrorCollector().failIfErrors();\n    }",
    "reference": "A loop driver for applying operations to all primary ClassNodes in\nour AST.  Automatically skips units that have already been processed\nthrough the current phase.",
    "generated": "Apply the primary class nodes.\n\n@param body PrimaryClassNodeOperation body"
  },
  {
    "code": "public static <T> T withStreams(Socket socket, @ClosureParams(value=SimpleType.class, options={\"java.io.InputStream\",\"java.io.OutputStream\"}) Closure<T> closure) throws IOException {\n        InputStream input = socket.getInputStream();\n        OutputStream output = socket.getOutputStream();\n        try {\n            T result = closure.call(new Object[]{input, output});\n\n            InputStream temp1 = input;\n            input = null;\n            temp1.close();\n            OutputStream temp2 = output;\n            output = null;\n            temp2.close();\n\n            return result;\n        } finally {\n            closeWithWarning(input);\n            closeWithWarning(output);\n        }\n    }",
    "reference": "Passes the Socket's InputStream and OutputStream to the closure.  The\nstreams will be closed after the closure returns, even if an exception\nis thrown.\n\n@param socket  a Socket\n@param closure a Closure\n@return the value returned by the closure\n@throws IOException if an IOException occurs.\n@since 1.5.2",
    "generated": "Creates a stream of streams.\n\n@param socket the socket\n@param closure the closure to call."
  },
  {
    "code": "public static <T> T withObjectStreams(Socket socket, @ClosureParams(value=SimpleType.class, options={\"java.io.ObjectInputStream\",\"java.io.ObjectOutputStream\"}) Closure<T> closure) throws IOException {\n        InputStream input = socket.getInputStream();\n        OutputStream output = socket.getOutputStream();\n        ObjectOutputStream oos = new ObjectOutputStream(output);\n        ObjectInputStream ois = new ObjectInputStream(input);\n        try {\n            T result = closure.call(new Object[]{ois, oos});\n\n            InputStream temp1 = ois;\n            ois = null;\n            temp1.close();\n            temp1 = input;\n            input = null;\n            temp1.close();\n            OutputStream temp2 = oos;\n            oos = null;\n            temp2.close();\n            temp2 = output;\n            output = null;\n            temp2.close();\n\n            return result;\n        } finally {\n            closeWithWarning(ois);\n            closeWithWarning(input);\n            closeWithWarning(oos);\n            closeWithWarning(output);\n        }\n    }",
    "reference": "Creates an InputObjectStream and an OutputObjectStream from a Socket, and\npasses them to the closure.  The streams will be closed after the closure\nreturns, even if an exception is thrown.\n\n@param socket  this Socket\n@param closure a Closure\n@return the value returned by the closure\n@throws IOException if an IOException occurs.\n@since 1.5.0",
    "generated": "Call a closure to call the given object streams.\n\n@param socket the socket\n@param closure the closure to call\n@return the object stream"
  },
  {
    "code": "public MethodKey createCopy() {\n        int size = getParameterCount();\n        Class[] paramTypes = new Class[size];\n        for (int i = 0; i < size; i++) {\n            paramTypes[i] = getParameterType(i);\n        }\n        return new DefaultMethodKey(sender, name, paramTypes, isCallToSuper);\n    }",
    "reference": "Creates an immutable copy that we can cache.",
    "generated": "Create a copy of this method.\n\n@param sender sender the sender\n@param name the name\n@param isCallToSuper the call to super method"
  },
  {
    "code": "public static void doExtendTraits(final ClassNode cNode, final SourceUnit unit, final CompilationUnit cu) {\n        if (cNode.isInterface()) return;\n        boolean isItselfTrait = Traits.isTrait(cNode);\n        SuperCallTraitTransformer superCallTransformer = new SuperCallTraitTransformer(unit);\n        if (isItselfTrait) {\n            checkTraitAllowed(cNode, unit);\n            return;\n        }\n        if (!cNode.getNameWithoutPackage().endsWith(Traits.TRAIT_HELPER)) {\n            List<ClassNode> traits = findTraits(cNode);\n            for (ClassNode trait : traits) {\n                TraitHelpersTuple helpers = Traits.findHelpers(trait);\n                applyTrait(trait, cNode, helpers);\n                superCallTransformer.visitClass(cNode);\n                if (unit!=null) {\n                    ASTTransformationCollectorCodeVisitor collector = new ASTTransformationCollectorCodeVisitor(unit, cu.getTransformLoader());\n                    collector.visitClass(cNode);\n                }\n            }\n        }\n    }",
    "reference": "Given a class node, if this class node implements a trait, then generate all the appropriate\ncode which delegates calls to the trait. It is safe to call this method on a class node which\ndoes not implement a trait.\n@param cNode a class node\n@param unit the source unit",
    "generated": "Extend a class node.\n\n@param cNode Class node\n@param unit CompilationUnit"
  },
  {
    "code": "public SyntaxException getSyntaxError(int index) {\n        SyntaxException exception = null;\n\n        Message message = getError(index);\n        if (message != null && message instanceof SyntaxErrorMessage) {\n            exception = ((SyntaxErrorMessage) message).getCause();\n        }\n        return exception;\n    }",
    "reference": "Convenience routine to return the specified error's\nunderlying SyntaxException, or null if it isn't one.",
    "generated": "Get the syntax error.\n\n@param index the index of the syntax error\n@return the syntax error"
  },
  {
    "code": "private void gobble(Iterator iter)\n    {\n        if (eatTheRest)\n        {\n            while (iter.hasNext())\n            {\n                tokens.add(iter.next());\n            }\n        }\n    }",
    "reference": "Adds the remaining tokens to the processed tokens list.\n\n@param iter An iterator over the remaining tokens",
    "generated": "Generate a Gobble generator.\n\n@param iter iterator to be used"
  },
  {
    "code": "public static int allParametersAndArgumentsMatch(Parameter[] params, ClassNode[] args) {\n        if (params==null) {\n            params = Parameter.EMPTY_ARRAY;\n        }\n        int dist = 0;\n        if (args.length<params.length) return -1;\n        // we already know the lengths are equal\n        for (int i = 0; i < params.length; i++) {\n            ClassNode paramType = params[i].getType();\n            ClassNode argType = args[i];\n            if (!isAssignableTo(argType, paramType)) return -1;\n            else {\n                if (!paramType.equals(argType)) dist+=getDistance(argType, paramType);\n            }\n        }\n        return dist;\n    }",
    "reference": "Checks that arguments and parameter types match.\n@param params method parameters\n@param args type arguments\n@return -1 if arguments do not match, 0 if arguments are of the exact type and >0 when one or more argument is\nnot of the exact type but still match",
    "generated": "Determine the distance between parameters and arguments.\n\n@param params parameters\n@param args parameters\n@return distance between parameters and arguments"
  },
  {
    "code": "static int excessArgumentsMatchesVargsParameter(Parameter[] params, ClassNode[] args) {\n        // we already know parameter length is bigger zero and last is a vargs\n        // the excess arguments are all put in an array for the vargs call\n        // so check against the component type\n        int dist = 0;\n        ClassNode vargsBase = params[params.length - 1].getType().getComponentType();\n        for (int i = params.length; i < args.length; i++) {\n            if (!isAssignableTo(args[i],vargsBase)) return -1;\n            else if (!args[i].equals(vargsBase)) dist+=getDistance(args[i], vargsBase);\n        }\n        return dist;\n    }",
    "reference": "Checks that excess arguments match the vararg signature parameter.\n@param params\n@param args\n@return -1 if no match, 0 if all arguments matches the vararg type and >0 if one or more vararg argument is\nassignable to the vararg type, but still not an exact match",
    "generated": "Check if excess arguments are all put in an array for the vargs call\n       "
  },
  {
    "code": "static int lastArgMatchesVarg(Parameter[] params, ClassNode... args) {\n        if (!isVargs(params)) return -1;\n        // case length ==0 handled already\n        // we have now two cases,\n        // the argument is wrapped in the vargs array or\n        // the argument is an array that can be used for the vargs part directly\n        // we test only the wrapping part, since the non wrapping is done already\n        ClassNode lastParamType = params[params.length - 1].getType();\n        ClassNode ptype = lastParamType.getComponentType();\n        ClassNode arg = args[args.length - 1];\n        if (isNumberType(ptype) && isNumberType(arg) && !ptype.equals(arg)) return -1;\n        return isAssignableTo(arg, ptype)?Math.min(getDistance(arg, lastParamType), getDistance(arg, ptype)):-1;\n    }",
    "reference": "Checks if the last argument matches the vararg type.\n@param params\n@param args\n@return -1 if no match, 0 if the last argument is exactly the vararg type and 1 if of an assignable type",
    "generated": "The last argument matches the vargs array and\n        // the argument is an array that can be used for the vargs part directly\n        // the argument is an array that can be used for the vargs part directly\n        // we test only the wrapping part directly\n        // we test only the wrapping part directly\n        // we test only the wrapping part directly\n        // we test only the wrapping part directly\n        // we test only the wrapping part directly\n        //"
  },
  {
    "code": "private static Parameter buildParameter(final Map<String, GenericsType> genericFromReceiver, final Map<String, GenericsType> placeholdersFromContext, final Parameter methodParameter, final ClassNode paramType) {\n        if (genericFromReceiver.isEmpty() && (placeholdersFromContext==null||placeholdersFromContext.isEmpty())) {\n            return methodParameter;\n        }\n        if (paramType.isArray()) {\n            ClassNode componentType = paramType.getComponentType();\n            Parameter subMethodParameter = new Parameter(componentType, methodParameter.getName());\n            Parameter component = buildParameter(genericFromReceiver, placeholdersFromContext, subMethodParameter, componentType);\n            return new Parameter(component.getType().makeArray(), component.getName());\n        }\n        ClassNode resolved = resolveClassNodeGenerics(genericFromReceiver, placeholdersFromContext, paramType);\n\n        return new Parameter(resolved, methodParameter.getName());\n    }",
    "reference": "Given a parameter, builds a new parameter for which the known generics placeholders are resolved.\n@param genericFromReceiver resolved generics from the receiver of the message\n@param placeholdersFromContext, resolved generics from the method context\n@param methodParameter the method parameter for which we want to resolve generic types\n@param paramType the (unresolved) type of the method parameter\n@return a new parameter with the same name and type as the original one, but with resolved generic types",
    "generated": "Build a method parameter.\n\n@param genericFromReceiver the generic from receiver\n@param placeholdersFromContext the placeholdersFromContext parameter\n@param paramType the type of the method parameter\n@return method parameter"
  },
  {
    "code": "public static boolean isClassClassNodeWrappingConcreteType(ClassNode classNode) {\n        GenericsType[] genericsTypes = classNode.getGenericsTypes();\n        return ClassHelper.CLASS_Type.equals(classNode)\n                && classNode.isUsingGenerics()\n                && genericsTypes!=null\n                && !genericsTypes[0].isPlaceholder()\n                && !genericsTypes[0].isWildcard();\n    }",
    "reference": "Returns true if the class node represents a the class node for the Class class\nand if the parametrized type is a neither a placeholder or a wildcard. For example,\nthe class node Class&lt;Foo&gt; where Foo is a class would return true, but the class\nnode for Class&lt;?&gt; would return false.\n@param classNode a class node to be tested\n@return true if it is the class node for Class and its generic type is a real class",
    "generated": "Returns true if the class node is wrapping concrete type."
  },
  {
    "code": "public static <T> T splitEachLine(InputStream stream, String regex, String charset, @ClosureParams(value=FromString.class,options=\"List<String>\") Closure<T> closure) throws IOException {\n        return splitEachLine(new BufferedReader(new InputStreamReader(stream, charset)), regex, closure);\n    }",
    "reference": "Iterates through the given InputStream line by line using the specified\nencoding, splitting each line using the given separator.  The list of tokens\nfor each line is then passed to the given closure. Finally, the stream\nis closed.\n\n@param stream  an InputStream\n@param regex   the delimiting regular expression\n@param charset opens the stream with a specified charset\n@param closure a closure\n@return the last value returned by the closure\n@throws IOException if an IOException occurs.\n@throws java.util.regex.PatternSyntaxException\nif the regular expression's syntax is invalid\n@see #splitEachLine(java.io.Reader, java.lang.String, groovy.lang.Closure)\n@since 1.5.5",
    "generated": "Split each line.\n\n@param stream InputStream\n@param charset @ClosureParams(value=FromString.class,options=\"List<String>\") Closure<T> closure"
  },
  {
    "code": "public static void transformChar(Reader self, Writer writer, @ClosureParams(value=SimpleType.class, options=\"java.lang.String\") Closure closure) throws IOException {\n        int c;\n        try {\n            char[] chars = new char[1];\n            while ((c = self.read()) != -1) {\n                chars[0] = (char) c;\n                writer.write((String) closure.call(new String(chars)));\n            }\n            writer.flush();\n\n            Writer temp2 = writer;\n            writer = null;\n            temp2.close();\n            Reader temp1 = self;\n            self = null;\n            temp1.close();\n        } finally {\n            closeWithWarning(self);\n            closeWithWarning(writer);\n        }\n    }",
    "reference": "Transforms each character from this reader by passing it to the given\nclosure.  The Closure should return each transformed character, which\nwill be passed to the Writer.  The reader and writer will be both be\nclosed before this method returns.\n\n@param self    a Reader object\n@param writer  a Writer to receive the transformed characters\n@param closure a closure that performs the required transformation\n@throws IOException if an IOException occurs.\n@since 1.5.0",
    "generated": "Transform a character.\n\n@param self the reader\n@param writer the writer\n@param closure the closure to be called"
  },
  {
    "code": "public static <T, U extends Closeable> T withCloseable(U self, @ClosureParams(value=FirstParam.class) Closure<T> action) throws IOException {\n        try {\n            T result = action.call(self);\n\n            Closeable temp = self;\n            self = null;\n            temp.close();\n\n            return result;\n        } finally {\n            DefaultGroovyMethodsSupport.closeWithWarning(self);\n        }\n    }",
    "reference": "Allows this closeable to be used within the closure, ensuring that it\nis closed once the closure has been executed and before this method returns.\n\n@param self the Closeable\n@param action the closure taking the Closeable as parameter\n@return the value returned by the closure\n@throws IOException if an IOException occurs.\n@since 2.4.0",
    "generated": "This method will call the given closure.\n\n@param self the closure\n@param action the closure to call\n@return the closure to call\n@throws IOException"
  },
  {
    "code": "public Object getProperty(Object object) {\n        return java.lang.reflect.Array.getLength(object);\n    }",
    "reference": "Get this property from the given object.\n@param object an array\n@return the length of the array object\n@throws IllegalArgumentException if object is not an array",
    "generated": "Get the length of an object.\n\n@param object the object\n@return the length of the property.\n@since 1.5.0"
  },
  {
    "code": "protected MethodVisitor makeDelegateCall(final String name, final String desc, final String signature, final String[] exceptions, final int accessFlags) {\n        MethodVisitor mv = super.visitMethod(accessFlags, name, desc, signature, exceptions);\n        mv.visitVarInsn(ALOAD, 0); // load this\n        mv.visitFieldInsn(GETFIELD, proxyName, DELEGATE_OBJECT_FIELD, BytecodeHelper.getTypeDescription(delegateClass)); // load delegate\n        // using InvokerHelper to allow potential intercepted calls\n        int size;\n        mv.visitLdcInsn(name); // method name\n        Type[] args = Type.getArgumentTypes(desc);\n        BytecodeHelper.pushConstant(mv, args.length);\n        mv.visitTypeInsn(ANEWARRAY, \"java/lang/Object\");\n        size = 6;\n        int idx = 1;\n        for (int i = 0; i < args.length; i++) {\n            Type arg = args[i];\n            mv.visitInsn(DUP);\n            BytecodeHelper.pushConstant(mv, i);\n            // primitive types must be boxed\n            if (isPrimitive(arg)) {\n                mv.visitIntInsn(getLoadInsn(arg), idx);\n                String wrappedType = getWrappedClassDescriptor(arg);\n                mv.visitMethodInsn(INVOKESTATIC, wrappedType, \"valueOf\", \"(\" + arg.getDescriptor() + \")L\" + wrappedType + \";\", false);\n            } else {\n                mv.visitVarInsn(ALOAD, idx); // load argument i\n            }\n            size = Math.max(size, 5+registerLen(arg));\n            idx += registerLen(arg);\n            mv.visitInsn(AASTORE); // store value into array\n        }\n        mv.visitMethodInsn(INVOKESTATIC, \"org/codehaus/groovy/runtime/InvokerHelper\", \"invokeMethod\", \"(Ljava/lang/Object;Ljava/lang/String;Ljava/lang/Object;)Ljava/lang/Object;\", false);\n        unwrapResult(mv, desc);\n        mv.visitMaxs(size, registerLen(args) + 1);\n\n        return mv;\n    }",
    "reference": "Generate a call to the delegate object.",
    "generated": "Make a delegate call.\n\n@param name desc the name of the method\n@param signature the signature\n@param exceptions the exceptions to be thrown"
  },
  {
    "code": "public static Method getSAMMethod(Class<?> c) {\n        // SAM = single public abstract method\n        // if the class is not abstract there is no abstract method\n        if (!Modifier.isAbstract(c.getModifiers())) return null;\n        if (c.isInterface()) {\n            Method[] methods = c.getMethods();\n            // res stores the first found abstract method\n            Method res = null;\n            for (Method mi : methods) {\n                // ignore methods, that are not abstract and from Object\n                if (!Modifier.isAbstract(mi.getModifiers())) continue;\n                // ignore trait methods which have a default implementation\n                if (mi.getAnnotation(Traits.Implemented.class)!=null) continue;\n                try {\n                    Object.class.getMethod(mi.getName(), mi.getParameterTypes());\n                    continue;\n                } catch (NoSuchMethodException e) {/*ignore*/}\n\n                // we have two methods, so no SAM\n                if (res!=null) return null;\n                res = mi;\n            }\n            return res;\n\n        } else {\n\n            LinkedList<Method> methods = new LinkedList();\n            getAbstractMethods(c, methods);\n            if (methods.isEmpty()) return null;\n            ListIterator<Method> it = methods.listIterator();\n            while (it.hasNext()) {\n                Method m = it.next();\n                if (hasUsableImplementation(c, m)) it.remove();\n            }\n            return getSingleNonDuplicateMethod(methods);\n        }\n    }",
    "reference": "returns the abstract method from a SAM type, if it is a SAM type.\n@param c the SAM class\n@return null if nothing was found, the method otherwise",
    "generated": "Get the SAM method for the class.\n\n@param c class\n@return the SAM method"
  },
  {
    "code": "protected void generateMopCalls(LinkedList<MethodNode> mopCalls, boolean useThis) {\n        for (MethodNode method : mopCalls) {\n            String name = getMopMethodName(method, useThis);\n            Parameter[] parameters = method.getParameters();\n            String methodDescriptor = BytecodeHelper.getMethodDescriptor(method.getReturnType(), method.getParameters());\n            MethodVisitor mv = controller.getClassVisitor().visitMethod(ACC_PUBLIC | ACC_SYNTHETIC, name, methodDescriptor, null, null);\n            controller.setMethodVisitor(mv);\n            mv.visitVarInsn(ALOAD, 0);\n            int newRegister = 1;\n            OperandStack operandStack = controller.getOperandStack();\n            for (Parameter parameter : parameters) {\n                ClassNode type = parameter.getType();\n                operandStack.load(parameter.getType(), newRegister);\n                // increment to next register, double/long are using two places\n                newRegister++;\n                if (type == ClassHelper.double_TYPE || type == ClassHelper.long_TYPE) newRegister++;\n            }\n            operandStack.remove(parameters.length);\n            ClassNode declaringClass = method.getDeclaringClass();\n            // JDK 8 support for default methods in interfaces\n            // this should probably be strenghtened when we support the A.super.foo() syntax\n            int opcode = declaringClass.isInterface()?INVOKEINTERFACE:INVOKESPECIAL;\n            mv.visitMethodInsn(opcode, BytecodeHelper.getClassInternalName(declaringClass), method.getName(), methodDescriptor, opcode == INVOKEINTERFACE);\n            BytecodeHelper.doReturn(mv, method.getReturnType());\n            mv.visitMaxs(0, 0);\n            mv.visitEnd();\n            controller.getClassNode().addMethod(name, ACC_PUBLIC | ACC_SYNTHETIC, method.getReturnType(), parameters, null, null);\n        }\n    }",
    "reference": "generates a Meta Object Protocol method, that is used to call a non public\nmethod, or to make a call to super.\n\n@param mopCalls list of methods a mop call method should be generated for\n@param useThis  true if \"this\" should be used for the naming",
    "generated": "Generate a list of mop calls.\n\n@param mopCalls list of mopCalls to generate\n@return mopCalls"
  },
  {
    "code": "public static ClassNode getWrapper(ClassNode cn) {\n        cn = cn.redirect();\n        if (!isPrimitiveType(cn)) return cn;\n        if (cn==boolean_TYPE) {\n            return Boolean_TYPE;\n        } else if (cn==byte_TYPE) {\n            return Byte_TYPE;\n        } else if (cn==char_TYPE) {\n            return Character_TYPE;\n        } else if (cn==short_TYPE) {\n            return Short_TYPE;\n        } else if (cn==int_TYPE) {\n            return Integer_TYPE;\n        } else if (cn==long_TYPE) {\n            return Long_TYPE;\n        } else if (cn==float_TYPE) {\n            return Float_TYPE;\n        } else if (cn==double_TYPE) {\n            return Double_TYPE;\n        } else if (cn==VOID_TYPE) {\n            return void_WRAPPER_TYPE;\n        }\n        else {\n            return cn;\n        }\n    }",
    "reference": "Creates a ClassNode containing the wrapper of a ClassNode\nof primitive type. Any ClassNode representing a primitive\ntype should be created using the predefined types used in\nclass. The method will check the parameter for known\nreferences of ClassNode representing a primitive type. If\nReference is found, then a ClassNode will be contained that\nrepresents the wrapper class. For example for boolean, the\nwrapper class is java.lang.Boolean.\n\nIf the parameter is no primitive type, the redirected\nClassNode will be returned\n\n@see #make(Class)\n@see #make(String)\n@param cn the ClassNode containing a possible primitive type",
    "generated": "Get the wrapper class.\n\n@param cn the class node to return"
  },
  {
    "code": "public static MethodNode findSAM(ClassNode type) {\n        if (!Modifier.isAbstract(type.getModifiers())) return null;\n        if (type.isInterface()) {\n            List<MethodNode> methods = type.getMethods();\n            MethodNode found=null;\n            for (MethodNode mi : methods) {\n                // ignore methods, that are not abstract and from Object\n                if (!Modifier.isAbstract(mi.getModifiers())) continue;\n                // ignore trait methods which have a default implementation\n                if (Traits.hasDefaultImplementation(mi)) continue;\n                if (mi.getDeclaringClass().equals(OBJECT_TYPE)) continue;\n                if (OBJECT_TYPE.getDeclaredMethod(mi.getName(), mi.getParameters())!=null) continue;\n\n                // we have two methods, so no SAM\n                if (found!=null) return null;\n                found = mi;\n            }\n            return found;\n\n        } else {\n\n            List<MethodNode> methods = type.getAbstractMethods();\n            MethodNode found = null;\n            if (methods!=null) {\n                for (MethodNode mi : methods) {\n                    if (!hasUsableImplementation(type, mi)) {\n                        if (found!=null) return null;\n                        found = mi;\n                    }\n                }\n            }\n            return found;\n        }\n    }",
    "reference": "Returns the single abstract method of a class node, if it is a SAM type, or null otherwise.\n@param type a type for which to search for a single abstract method\n@return the method node if type is a SAM type, null otherwise",
    "generated": "Find the SAM method node.\n\n@param type the class node\n@return SAM method node"
  },
  {
    "code": "public static int getPrecedence( int type, boolean throwIfInvalid ) {\n\n        switch( type ) {\n\n            case LEFT_PARENTHESIS:\n                return 0;\n\n            case EQUAL:\n            case PLUS_EQUAL:\n            case MINUS_EQUAL:\n            case MULTIPLY_EQUAL:\n            case DIVIDE_EQUAL:\n            case INTDIV_EQUAL:\n            case MOD_EQUAL:\n            case POWER_EQUAL:\n            case LOGICAL_OR_EQUAL:\n            case LOGICAL_AND_EQUAL:\n            case LEFT_SHIFT_EQUAL:\n            case RIGHT_SHIFT_EQUAL:\n            case RIGHT_SHIFT_UNSIGNED_EQUAL:\n            case BITWISE_OR_EQUAL:\n            case BITWISE_AND_EQUAL:\n            case BITWISE_XOR_EQUAL:\n                return 5;\n\n            case QUESTION:\n                return 10;\n\n            case LOGICAL_OR:\n                return 15;\n\n            case LOGICAL_AND:\n                return 20;\n\n            case BITWISE_OR:\n            case BITWISE_AND:\n            case BITWISE_XOR:\n                return 22;\n\n            case COMPARE_IDENTICAL:\n            case COMPARE_NOT_IDENTICAL:\n                return 24;\n\n            case COMPARE_NOT_EQUAL:\n            case COMPARE_EQUAL:\n            case COMPARE_LESS_THAN:\n            case COMPARE_LESS_THAN_EQUAL:\n            case COMPARE_GREATER_THAN:\n            case COMPARE_GREATER_THAN_EQUAL:\n            case COMPARE_TO:\n            case FIND_REGEX:\n            case MATCH_REGEX:\n            case KEYWORD_INSTANCEOF:\n                return 25;\n\n            case DOT_DOT:\n            case DOT_DOT_DOT:\n                return 30;\n\n            case LEFT_SHIFT:\n            case RIGHT_SHIFT:\n            case RIGHT_SHIFT_UNSIGNED:\n                return 35;\n\n            case PLUS:\n            case MINUS:\n                return 40;\n\n            case MULTIPLY:\n            case DIVIDE:\n            case INTDIV:\n            case MOD:\n                return 45;\n\n            case NOT:\n            case REGEX_PATTERN:\n                return 50;\n\n            case SYNTH_CAST:\n                return 55;\n\n            case PLUS_PLUS:\n            case MINUS_MINUS:\n            case PREFIX_PLUS_PLUS:\n            case PREFIX_MINUS_MINUS:\n            case POSTFIX_PLUS_PLUS:\n            case POSTFIX_MINUS_MINUS:\n                return 65;\n\n            case PREFIX_PLUS:\n            case PREFIX_MINUS:\n                return 70;\n\n            case POWER:\n                return 72;\n\n            case SYNTH_METHOD:\n            case LEFT_SQUARE_BRACKET:\n                return 75;\n\n            case DOT:\n            case NAVIGATE:\n                return 80;\n\n            case KEYWORD_NEW:\n                return 85;\n        }\n\n        if( throwIfInvalid ) {\n            throw new GroovyBugError( \"precedence requested for non-operator\" );\n        }\n\n        return -1;\n    }",
    "reference": "Returns the precedence of the specified operator.  Non-operator's will\nreceive -1 or a GroovyBugError, depending on your preference.",
    "generated": "Returns the precedence of the given type."
  },
  {
    "code": "public <T> T with( Closure<T> closure ) {\n        return DefaultGroovyMethods.with( null, closure ) ;\n    }",
    "reference": "Allows the closure to be called for NullObject\n\n@param closure the closure to call on the object\n@return result of calling the closure",
    "generated": "Generate a documentation string for this function.\n\n@param closure Closure"
  },
  {
    "code": "private static boolean implementsMethod(ClassNode classNode, String methodName, Class[] argTypes) {\r\n        List methods = classNode.getMethods();\r\n        if (argTypes == null || argTypes.length ==0) {\r\n            for (Iterator i = methods.iterator(); i.hasNext();) {\r\n                MethodNode mn = (MethodNode) i.next();\r\n                boolean methodMatch = mn.getName().equals(methodName);\r\n                if(methodMatch)return true;\r\n                // TODO Implement further parameter analysis\r\n            }\r\n        }\r\n        return false;\r\n    }",
    "reference": "Tests whether the ClassNode implements the specified method name\n\n@param classNode The ClassNode\n@param methodName The method name\n@param argTypes\n@return True if it implements the method",
    "generated": "Implements a method."
  },
  {
    "code": "public String getTypeDescriptor() {\n        if (typeDescriptor == null) {\n            StringBuilder buf = new StringBuilder(name.length() + parameters.length * 10);\n            buf.append(returnType.getName());\n            buf.append(' ');\n            buf.append(name);\n            buf.append('(');\n            for (int i = 0; i < parameters.length; i++) {\n                if (i > 0) {\n                    buf.append(\", \");\n                }\n                Parameter param = parameters[i];\n                buf.append(formatTypeName(param.getType()));\n            }\n            buf.append(')');\n            typeDescriptor = buf.toString();\n        }\n        return typeDescriptor;\n    }",
    "reference": "The type descriptor for a method node is a string containing the name of the method, its return type,\nand its parameter types in a canonical form. For simplicity, I'm using the format of a Java declaration\nwithout parameter names.\n\n@return the type descriptor",
    "generated": "Generate a type descriptor.\n\n@param type type"
  },
  {
    "code": "@Override\n    public String getText() {\n        String retType = AstToTextHelper.getClassText(returnType);\n        String exceptionTypes = AstToTextHelper.getThrowsClauseText(exceptions);\n        String parms = AstToTextHelper.getParametersText(parameters);\n        return AstToTextHelper.getModifiersText(modifiers) + \" \" + retType + \" \" + name + \"(\" + parms + \") \" + exceptionTypes + \" { ... }\";\n    }",
    "reference": "Provides a nicely formatted string of the method definition. For simplicity, generic types on some of the elements\nare not displayed.\n@return\nstring form of node with some generic elements suppressed",
    "generated": "Returns the return type of the return type."
  },
  {
    "code": "public GroovyConstructorDoc[] constructors() {\n        Collections.sort(constructors);\n        return constructors.toArray(new GroovyConstructorDoc[constructors.size()]);\n    }",
    "reference": "returns a sorted array of constructors",
    "generated": "Generate a list of constructors.\n\n@param constructors the list of constructors\n@return the list of constructors"
  },
  {
    "code": "public GroovyClassDoc[] innerClasses() {\n        Collections.sort(nested);\n        return nested.toArray(new GroovyClassDoc[nested.size()]);\n    }",
    "reference": "returns a sorted array of nested classes and interfaces",
    "generated": "Generate a list of nested classes.\n\n@param nested list of nested classes\n@return a list of nested classes"
  },
  {
    "code": "public GroovyFieldDoc[] fields() {\n        Collections.sort(fields);\n        return fields.toArray(new GroovyFieldDoc[fields.size()]);\n    }",
    "reference": "returns a sorted array of fields",
    "generated": "Generate a list of fields.\n\n@param fields the list of fields\n@return the list of fields"
  },
  {
    "code": "public GroovyFieldDoc[] properties() {\n        Collections.sort(properties);\n        return properties.toArray(new GroovyFieldDoc[properties.size()]);\n    }",
    "reference": "returns a sorted array of properties",
    "generated": "Generate a list of fields.\n\n@param properties a list of fields\n@return a list of fields"
  },
  {
    "code": "public GroovyFieldDoc[] enumConstants() {\n        Collections.sort(enumConstants);\n        return enumConstants.toArray(new GroovyFieldDoc[enumConstants.size()]);\n    }",
    "reference": "returns a sorted array of enum constants",
    "generated": "Generate a list of enum constants.\n\n@param enumConstants the list of enum constants"
  },
  {
    "code": "public GroovyMethodDoc[] methods() {\n        Collections.sort(methods);\n        return methods.toArray(new GroovyMethodDoc[methods.size()]);\n    }",
    "reference": "returns a sorted array of methods",
    "generated": "Generate a list of methods.\n\n@param methods the list of methods\n@return the list of methods"
  },
  {
    "code": "public void add(Map<String, Object> map) throws SQLException {\n        if (withinDataSetBatch) {\n            if (batchData.size() == 0) {\n                batchKeys = map.keySet();\n            } else {\n                if (!map.keySet().equals(batchKeys)) {\n                    throw new IllegalArgumentException(\"Inconsistent keys found for batch add!\");\n                }\n            }\n            batchData.add(map);\n            return;\n        }\n        int answer = executeUpdate(buildListQuery(map), new ArrayList<Object>(map.values()));\n        if (answer != 1) {\n            LOG.warning(\"Should have updated 1 row not \" + answer + \" when trying to add: \" + map);\n        }\n    }",
    "reference": "Adds the provided map of key-value pairs as a new row in the table represented by this DataSet.\n\n@param map the key (column-name), value pairs to add as a new row\n@throws SQLException if a database error occurs",
    "generated": "Add a map to the database.\n\n@param map map to add\n@return"
  },
  {
    "code": "public void each(int offset, int maxRows, Closure closure) throws SQLException {\n        eachRow(getSql(), getParameters(), offset, maxRows, closure);\n    }",
    "reference": "Calls the provided closure for a \"page\" of rows from the table represented by this DataSet.\nA page is defined as starting at a 1-based offset, and containing a maximum number of rows.\n\n@param offset  the 1-based offset for the first row to be processed\n@param maxRows the maximum number of rows to be processed\n@param closure called for each row with a GroovyResultSet\n@throws SQLException if a database access error occurs\n@see groovy.sql.Sql#eachRow(String, java.util.List, int, int, groovy.lang.Closure)",
    "generated": "Each row.\n\n@param offset offset of the row\n@param maxRows closure"
  },
  {
    "code": "private void wrapSetterMethod(ClassNode classNode, boolean bindable, String propertyName) {\r\n        String getterName = \"get\" + MetaClassHelper.capitalize(propertyName);\r\n        MethodNode setter = classNode.getSetterMethod(\"set\" + MetaClassHelper.capitalize(propertyName));\r\n\r\n        if (setter != null) {\r\n            // Get the existing code block\r\n            Statement code = setter.getCode();\r\n\r\n            Expression oldValue = varX(\"$oldValue\");\r\n            Expression newValue = varX(\"$newValue\");\r\n            Expression proposedValue = varX(setter.getParameters()[0].getName());\r\n            BlockStatement block = new BlockStatement();\r\n\r\n            // create a local variable to hold the old value from the getter\r\n            block.addStatement(declS(oldValue, callThisX(getterName)));\r\n\r\n            // add the fireVetoableChange method call\r\n            block.addStatement(stmt(callThisX(\"fireVetoableChange\", args(\r\n                    constX(propertyName), oldValue, proposedValue))));\r\n\r\n            // call the existing block, which will presumably set the value properly\r\n            block.addStatement(code);\r\n\r\n            if (bindable) {\r\n                // get the new value to emit in the event\r\n                block.addStatement(declS(newValue, callThisX(getterName)));\r\n\r\n                // add the firePropertyChange method call\r\n                block.addStatement(stmt(callThisX(\"firePropertyChange\", args(constX(propertyName), oldValue, newValue))));\r\n            }\r\n\r\n            // replace the existing code block with our new one\r\n            setter.setCode(block);\r\n        }\r\n    }",
    "reference": "Wrap an existing setter.",
    "generated": "Wrap the setter method."
  },
  {
    "code": "public static long directorySize(File self) throws IOException, IllegalArgumentException\n    {\n        final long[] size = {0L};\n\n        eachFileRecurse(self, FileType.FILES, new Closure<Void>(null) {\n            public void doCall(Object[] args) {\n                size[0] += ((File) args[0]).length();\n            }\n        });\n\n        return size[0];\n    }",
    "reference": "Calculates directory size as total size of all its files, recursively.\n\n@param self a file object\n@return directory size (length)\n@since 2.1\n\n@throws IOException if File object specified does not exist\n@throws IllegalArgumentException if the provided File object does not represent a directory",
    "generated": "The directory size of the file."
  },
  {
    "code": "public static <T> T splitEachLine(File self, String regex, @ClosureParams(value=SimpleType.class, options=\"java.lang.String[]\") Closure<T> closure) throws IOException {\n        return IOGroovyMethods.splitEachLine(newReader(self), regex, closure);\n    }",
    "reference": "Iterates through this file line by line, splitting each line using\nthe given regex separator. For each line, the given closure is called with\na single parameter being the list of strings computed by splitting the line\naround matches of the given regular expression.\nFinally the resources used for processing the file are closed.\n\n@param self    a File\n@param regex   the delimiting regular expression\n@param closure a closure\n@return the last value returned by the closure\n@throws IOException if an IOException occurs.\n@throws java.util.regex.PatternSyntaxException\nif the regular expression's syntax is invalid\n@see IOGroovyMethods#splitEachLine(java.io.Reader, java.lang.String, groovy.lang.Closure)\n@since 1.5.5",
    "generated": "Split each line of a file.\n\n@param self file\n@param regex the regex to split\n@param closure the closure to split\n@return the value of the file\n@throws IOException"
  },
  {
    "code": "public static void write(File file, String text, String charset) throws IOException {\n        Writer writer = null;\n        try {\n            FileOutputStream out = new FileOutputStream(file);\n            writeUTF16BomIfRequired(charset, out);\n            writer = new OutputStreamWriter(out, charset);\n            writer.write(text);\n            writer.flush();\n\n            Writer temp = writer;\n            writer = null;\n            temp.close();\n        } finally {\n            closeWithWarning(writer);\n        }\n    }",
    "reference": "Write the text to the File, using the specified encoding.\n\n@param file    a File\n@param text    the text to write to the File\n@param charset the charset used\n@throws IOException if an IOException occurs.\n@since 1.0",
    "generated": "Write a text to a file.\n\n@param file File\n@param text text\n@param charset UTF16Bom ifRequired"
  },
  {
    "code": "public static void append(File file, Object text) throws IOException {\n        Writer writer = null;\n        try {\n            writer = new FileWriter(file, true);\n            InvokerHelper.write(writer, text);\n            writer.flush();\n\n            Writer temp = writer;\n            writer = null;\n            temp.close();\n        } finally {\n            closeWithWarning(writer);\n        }\n    }",
    "reference": "Append the text at the end of the File.\n\n@param file a File\n@param text the text to append at the end of the File\n@throws IOException if an IOException occurs.\n@since 1.0",
    "generated": "Append a text to a file.\n\n@param file file\n@param text text\n@return text"
  },
  {
    "code": "public static void append(File file, Object text, String charset) throws IOException {\n        Writer writer = null;\n        try {\n            FileOutputStream out = new FileOutputStream(file, true);\n            if (!file.exists()) {\n                writeUTF16BomIfRequired(charset, out);\n            }\n            writer = new OutputStreamWriter(out, charset);\n            InvokerHelper.write(writer, text);\n            writer.flush();\n\n            Writer temp = writer;\n            writer = null;\n            temp.close();\n        } finally {\n            closeWithWarning(writer);\n        }\n    }",
    "reference": "Append the text at the end of the File, using a specified encoding.\n\n@param file    a File\n@param text    the text to append at the end of the File\n@param charset the charset used\n@throws IOException if an IOException occurs.\n@since 1.0",
    "generated": "Append an object to a file.\n\n@param file file to write\n@param charset the charset\n@return text"
  },
  {
    "code": "public static void append(File file, Writer writer, String charset) throws IOException {\n        appendBuffered(file, writer, charset);\n    }",
    "reference": "Append the text supplied by the Writer at the end of the File, using a specified encoding.\n\n@param file a File\n@param writer the Writer supplying the text to append at the end of the File\n@param charset the charset used\n@throws IOException if an IOException occurs.\n@since 2.3",
    "generated": "Append a file to a writer.\n\n@param file file\n@param writer the writer\n@param charset the charset to append\n@return"
  },
  {
    "code": "private static void writeUtf16Bom(OutputStream stream, boolean bigEndian) throws IOException {\n        if (bigEndian) {\n            stream.write(-2);\n            stream.write(-1);\n        } else {\n            stream.write(-1);\n            stream.write(-2);\n        }\n    }",
    "reference": "Write a Byte Order Mark at the beginning of the file\n\n@param stream    the FileOutputStream to write the BOM to\n@param bigEndian true if UTF 16 Big Endian or false if Low Endian\n@throws IOException if an IOException occurs.\n@since 1.0",
    "generated": "Write a UTF16 Bom stream.\n\n@param stream the stream to write\n@param bigEndian"
  },
  {
    "code": "protected Class getClassCacheEntry(String name) {\n        if (name == null) return null;\n        synchronized (classCache) {\n            return classCache.get(name);\n        }\n    }",
    "reference": "gets a class from the class cache. This cache contains only classes loaded through\nthis class loader or an InnerLoader instance. If no class is stored for a\nspecific name, then the method should return null.\n\n@param name of the class\n@return the class stored for the given name\n@see #removeClassCacheEntry(String)\n@see #setClassCacheEntry(Class)\n@see #clearCache()",
    "generated": "Returns the class entry for the given name.\n\n@param name the name of the class entry\n@return the class entry"
  },
  {
    "code": "public MetaClassRegistryChangeEventListener[] getMetaClassRegistryChangeEventListeners() {\n        synchronized (changeListenerList) {\n            ArrayList<MetaClassRegistryChangeEventListener> ret =\n                    new ArrayList<MetaClassRegistryChangeEventListener>(changeListenerList.size()+nonRemoveableChangeListenerList.size());\n            ret.addAll(nonRemoveableChangeListenerList);\n            ret.addAll(changeListenerList);\n            return ret.toArray(new MetaClassRegistryChangeEventListener[ret.size()]);\n        }\n    }",
    "reference": "Gets an array of of all registered ConstantMetaClassListener instances.",
    "generated": "Generate a list of meta class registry change listeners.\n\n@param changeListenerList list of meta class registry change listeners"
  },
  {
    "code": "public static MetaClassRegistry getInstance(int includeExtension) {\n        if (includeExtension != DONT_LOAD_DEFAULT) {\n            if (instanceInclude == null) {\n                instanceInclude = new MetaClassRegistryImpl();\n            }\n            return instanceInclude;\n        } else {\n            if (instanceExclude == null) {\n                instanceExclude = new MetaClassRegistryImpl(DONT_LOAD_DEFAULT);\n            }\n            return instanceExclude;\n        }\n    }",
    "reference": "Singleton of MetaClassRegistry.\n\n@param includeExtension\n@return the registry",
    "generated": "Creates a MetaClassRegistry instance.\n\n@param includeExtension Extension"
  },
  {
    "code": "public CSTNode get( int index ) \n    {\n        CSTNode element = null;\n\n        if( index < size() ) \n        {\n            element = (CSTNode)elements.get( index );\n        }\n\n        return element;\n    }",
    "reference": "Returns the specified element, or null.",
    "generated": "Get the element at the specified index.\n\n@param index index index\n@return the element"
  },
  {
    "code": "public CSTNode set( int index, CSTNode element ) \n    {\n        \n        if( elements == null ) \n        {\n            throw new GroovyBugError( \"attempt to set() on a EMPTY Reduction\" );\n        }\n\n        if( index == 0 && !(element instanceof Token) ) \n        {\n\n            //\n            // It's not the greatest of design that the interface allows this, but it\n            // is a tradeoff with convenience, and the convenience is more important.\n\n            throw new GroovyBugError( \"attempt to set() a non-Token as root of a Reduction\" );\n        }\n\n\n        //\n        // Fill slots with nulls, if necessary.\n\n        int count = elements.size();\n        if( index >= count ) \n        {\n            for( int i = count; i <= index; i++ ) \n            {\n                elements.add( null );\n            }\n        }\n\n        //\n        // Then set in the element.\n\n        elements.set( index, element );\n\n        return element;\n    }",
    "reference": "Sets an element in at the specified index.",
    "generated": "Set a CSTNode.\n\n@param index index of the index to set\n@param element CSTNode"
  },
  {
    "code": "public void add(T value) {\n        Element<T> element = new Element<T>(bundle, value);\n        element.previous = tail;\n        if (tail != null) tail.next = element;\n        tail = element;\n        if (head == null) head = element;\n    }",
    "reference": "adds a value to the list\n\n@param value the value",
    "generated": "Add a value to the bundle.\n\n@param value the value to add\n@return"
  },
  {
    "code": "public T[] toArray(T[] tArray) {\n        List<T> array = new ArrayList<T>(100);\n        for (Iterator<T> it = iterator(); it.hasNext();) {\n            T val = it.next();\n            if (val != null) array.add(val);\n        }\n        return array.toArray(tArray);\n    }",
    "reference": "Returns an array of non null elements from the source array.\n\n@param tArray the source array\n@return the array",
    "generated": "This method returns an array of values.\n\n@param tArray array"
  },
  {
    "code": "public Value get(Object key) {\n        /* If the length is under and we are asking for the key, then just look for the key. Don't build the map. */\n        if (map == null && items.length < 20) {\n            for (Object item : items) {\n                MapItemValue miv = (MapItemValue) item;\n                if (key.equals(miv.name.toValue())) {\n                    return miv.value;\n                }\n            }\n            return null;\n        } else {\n            if (map == null) buildIfNeededMap();\n            return map.get(key);\n        }\n    }",
    "reference": "Get the items for the key.\n\n@param key\n@return the items for the given key",
    "generated": "Get the value of the key."
  },
  {
    "code": "public static <T> Iterator<T> unique(Iterator<T> self) {\n        return toList((Iterable<T>) unique(toList(self))).listIterator();\n    }",
    "reference": "Returns an iterator equivalent to this iterator with all duplicated items removed\nby using the default comparator. The original iterator will become\nexhausted of elements after determining the unique values. A new iterator\nfor the unique values will be returned.\n\n@param self an Iterator\n@return the modified Iterator\n@since 1.5.5",
    "generated": "Generate a unique iterator.\n\n@param self iterator to be unique"
  },
  {
    "code": "public static int numberAwareCompareTo(Comparable self, Comparable other) {\n        NumberAwareComparator<Comparable> numberAwareComparator = new NumberAwareComparator<Comparable>();\n        return numberAwareComparator.compare(self, other);\n    }",
    "reference": "Provides a method that compares two comparables using Groovy's\ndefault number aware comparator.\n\n@param self a Comparable\n@param other another Comparable\n@return a -ve number, 0 or a +ve number according to Groovy's compareTo contract\n@since 1.6.0",
    "generated": "Compare the two Comparable objects.\n\n@param self Comparable\n@param other Comparable"
  },
  {
    "code": "public static boolean any(Object self, Closure closure) {\n        BooleanClosureWrapper bcw = new BooleanClosureWrapper(closure);\n        for (Iterator iter = InvokerHelper.asIterator(self); iter.hasNext();) {\n            if (bcw.call(iter.next())) return true;\n        }\n        return false;\n    }",
    "reference": "Iterates over the contents of an object or collection, and checks whether a\npredicate is valid for at least one element.\n\n@param self    the object over which we iterate\n@param closure the closure predicate used for matching\n@return true   if any iteration for the object matches the closure predicate\n@since 1.0",
    "generated": "Returns true if the given object is a boolean.\n\n@param self the object\n@param closure the closure\n@return true if the object is a boolean"
  },
  {
    "code": "public static Object findResult(Object self, Object defaultResult, Closure closure) {\n        Object result = findResult(self, closure);\n        if (result == null) return defaultResult;\n        return result;\n    }",
    "reference": "Treats the object as iterable, iterating through the values it represents and returns the first non-null result obtained from calling the closure, otherwise returns the defaultResult.\n\n@param self    an Object with an iterator returning its values\n@param defaultResult an Object that should be returned if all closure results are null\n@param closure a closure that returns a non-null value when processing should stop\n@return the first non-null result of the closure, otherwise the default value\n@since 1.7.5",
    "generated": "Find the result of the given closure.\n\n@param self the object\n@param defaultResult the default result"
  },
  {
    "code": "protected static <K, T> void groupAnswer(final Map<K, List<T>> answer, T element, K value) {\n        if (answer.containsKey(value)) {\n            answer.get(value).add(element);\n        } else {\n            List<T> groupedElements = new ArrayList<T>();\n            groupedElements.add(element);\n            answer.put(value, groupedElements);\n        }\n    }",
    "reference": "Groups the current element according to the value\n\n@param answer  the map containing the results\n@param element the element to be placed\n@param value   the value according to which the element will be placed\n@since 1.5.0",
    "generated": "Group a answer.\n\n@param answer Map to add to the map\n@param element the element\n@param value the value\n@return"
  },
  {
    "code": "public static <K,V> Map<K,V> asImmutable(Map<? extends K, ? extends V> self) {\n        return Collections.unmodifiableMap(self);\n    }",
    "reference": "A convenience method for creating an immutable map.\n\n@param self a Map\n@return an immutable Map\n@see java.util.Collections#unmodifiableMap(java.util.Map)\n@since 1.0",
    "generated": "This method returns a immutable map."
  },
  {
    "code": "public static <K,V> SortedMap<K,V> asImmutable(SortedMap<K, ? extends V> self) {\n        return Collections.unmodifiableSortedMap(self);\n    }",
    "reference": "A convenience method for creating an immutable sorted map.\n\n@param self a SortedMap\n@return an immutable SortedMap\n@see java.util.Collections#unmodifiableSortedMap(java.util.SortedMap)\n@since 1.0",
    "generated": "This method returns a immutable SortedMap."
  },
  {
    "code": "public static <T> List<T> asImmutable(List<? extends T> self) {\n        return Collections.unmodifiableList(self);\n    }",
    "reference": "A convenience method for creating an immutable list\n\n@param self a List\n@return an immutable List\n@see java.util.Collections#unmodifiableList(java.util.List)\n@since 1.0",
    "generated": "This method returns a immutable list.\n\n@param self list to be immutable"
  },
  {
    "code": "public static <T> Set<T> asImmutable(Set<? extends T> self) {\n        return Collections.unmodifiableSet(self);\n    }",
    "reference": "A convenience method for creating an immutable list.\n\n@param self a Set\n@return an immutable Set\n@see java.util.Collections#unmodifiableSet(java.util.Set)\n@since 1.0",
    "generated": "Returns a immutable set.\n\n@param self Set to be immutable"
  },
  {
    "code": "public static <T> SortedSet<T> asImmutable(SortedSet<T> self) {\n        return Collections.unmodifiableSortedSet(self);\n    }",
    "reference": "A convenience method for creating an immutable sorted set.\n\n@param self a SortedSet\n@return an immutable SortedSet\n@see java.util.Collections#unmodifiableSortedSet(java.util.SortedSet)\n@since 1.0",
    "generated": "This method returns an immutable SortedSet.\n\n@param self SortedSet to be immutable"
  },
  {
    "code": "public static <T> T[] sort(T[] self, Comparator<T> comparator) {\n        return sort(self, true, comparator);\n    }",
    "reference": "Sorts the given array into sorted order using the given comparator.\n\n@param self the array to be sorted\n@param comparator a Comparator used for the comparison\n@return the sorted array\n@since 1.5.5",
    "generated": "Sort a collection of objects.\n\n@param self Collection of objects\n@param comparator Comparator"
  },
  {
    "code": "public static <T> boolean addAll(Collection<T> self, Iterator<T> items) {\n        boolean changed = false;\n        while (items.hasNext()) {\n            T next =  items.next();\n            if (self.add(next)) changed = true;\n        }\n        return changed;\n    }",
    "reference": "Adds all items from the iterator to the Collection.\n\n@param self the collection\n@param items the items to add\n@return true if the collection changed",
    "generated": "Add a collection of items.\n\n@param self Collection\n@param items Collection\n@return true if the collection has changed"
  },
  {
    "code": "public static <T> boolean addAll(Collection<T> self, Iterable<T> items) {\n        boolean changed = false;\n        for (T next : items) {\n            if (self.add(next)) changed = true;\n        }\n        return changed;\n    }",
    "reference": "Adds all items from the iterable to the Collection.\n\n@param self the collection\n@param items the items to add\n@return true if the collection changed",
    "generated": "Add a collection of items.\n\n@param self Collection\n@param items Collection\n@return true if the collection has changed."
  },
  {
    "code": "public static <K,V> Map<K,V> minus(Map<K,V> self, Map removeMe) {\n        final Map<K,V> ansMap = createSimilarMap(self);\n        ansMap.putAll(self);\n        if (removeMe != null && removeMe.size() > 0) {\n            for (Map.Entry<K, V> e1 : self.entrySet()) {\n                for (Object e2 : removeMe.entrySet()) {\n                    if (DefaultTypeTransformation.compareEqual(e1, e2)) {\n                        ansMap.remove(e1.getKey());\n                    }\n                }\n            }\n        }\n        return ansMap;\n    }",
    "reference": "Create a Map composed of the entries of the first map minus the\nentries of the given map.\n\n@param self     a map object\n@param removeMe the entries to remove from the map\n@return the resulting map\n@since 1.7.4",
    "generated": "Add a minus to the map.\n\n@param self the map\n@param removeMe the map\n@return the map"
  },
  {
    "code": "public static int findLastIndexOf(Object self, int startIndex, Closure closure) {\n        int result = -1;\n        int i = 0;\n        BooleanClosureWrapper bcw = new BooleanClosureWrapper(closure);\n        for (Iterator iter = InvokerHelper.asIterator(self); iter.hasNext(); i++) {\n            Object value = iter.next();\n            if (i < startIndex) {\n                continue;\n            }\n            if (bcw.call(value)) {\n                result = i;\n            }\n        }\n        return result;\n    }",
    "reference": "Iterates over the elements of an iterable collection of items, starting\nfrom a specified startIndex, and returns the index of the last item that\nmatches the condition specified in the closure.\n\n@param self       the iteration object over which to iterate\n@param startIndex start matching from this index\n@param closure    the filter to perform a match on the collection\n@return an integer that is the index of the last matched object or -1 if no match was found\n@since 1.5.2",
    "generated": "Find the last index of the given object.\n\n@param self the object\n@param startIndex the starting index\n@return index of the given object\n@since 1.0"
  },
  {
    "code": "public static List<Number> findIndexValues(Object self, Closure closure) {\n        return findIndexValues(self, 0, closure);\n    }",
    "reference": "Iterates over the elements of an iterable collection of items and returns\nthe index values of the items that match the condition specified in the closure.\n\n@param self    the iteration object over which to iterate\n@param closure the filter to perform a match on the collection\n@return a list of numbers corresponding to the index values of all matched objects\n@since 1.5.2",
    "generated": "Find the index values.\n\n@param self the object\n@param closure the closure\n@return the list of index values"
  },
  {
    "code": "public static List<Number> findIndexValues(Object self, Number startIndex, Closure closure) {\n        List<Number> result = new ArrayList<Number>();\n        long count = 0;\n        long startCount = startIndex.longValue();\n        BooleanClosureWrapper bcw = new BooleanClosureWrapper(closure);\n        for (Iterator iter = InvokerHelper.asIterator(self); iter.hasNext(); count++) {\n            Object value = iter.next();\n            if (count < startCount) {\n                continue;\n            }\n            if (bcw.call(value)) {\n                result.add(count);\n            }\n        }\n        return result;\n    }",
    "reference": "Iterates over the elements of an iterable collection of items, starting from\na specified startIndex, and returns the index values of the items that match\nthe condition specified in the closure.\n\n@param self       the iteration object over which to iterate\n@param startIndex start matching from this index\n@param closure    the filter to perform a match on the collection\n@return a list of numbers corresponding to the index values of all matched objects\n@since 1.5.2",
    "generated": "Find index values.\n\n@param self the object\n@param startIndex the startIndex\n@param closure the closure\n@return index values"
  },
  {
    "code": "public Object getProperty(String property) {\n        if(ExpandoMetaClass.isValidExpandoProperty(property)) {\n            if(property.equals(ExpandoMetaClass.STATIC_QUALIFIER) ||\n               property.equals(ExpandoMetaClass.CONSTRUCTOR) ||\n               myMetaClass.hasProperty(this, property) == null) {\n                  return replaceDelegate().getProperty(property);\n            }\n        }\n        return myMetaClass.getProperty(this, property);\n    }",
    "reference": "this method mimics EMC behavior",
    "generated": "Get a property.\n\n@param property the property\n@return the value of the property\n@return the value of the property"
  },
  {
    "code": "public static Message create( String text, Object data, ProcessingUnit owner  )\n    {\n        return new SimpleMessage( text, data, owner);\n    }",
    "reference": "Creates a new Message from the specified text.",
    "generated": "Create a new message.\n\n@param text the text\n@param data the data\n@param owner the ProcessingUnit"
  },
  {
    "code": "public Object invokeMethod(String name, Object args) {\n        Object val = null;\n        if (args != null && Object[].class.isAssignableFrom(args.getClass())) {\n            Object[] arr = (Object[]) args;\n\n            if (arr.length == 1) {\n                val = arr[0];\n            } else if (arr.length == 2 && arr[0] instanceof Collection && arr[1] instanceof Closure) {\n                Closure<?> closure = (Closure<?>) arr[1];\n                Iterator<?> iterator = ((Collection) arr[0]).iterator();\n                List<Object> list = new ArrayList<Object>();\n                while (iterator.hasNext()) {\n                    list.add(curryDelegateAndGetContent(closure, iterator.next()));\n                }\n                val = list;\n            } else {\n                val = Arrays.asList(arr);\n            }\n        }\n        content.put(name, val);\n\n        return val;\n    }",
    "reference": "Intercepts calls for setting a key and value for a JSON object\n\n@param name the key name\n@param args the value associated with the key",
    "generated": "Invoke a method.\n\n@param name the method name\n@param args the arguments to invoke\n@return the method value"
  },
  {
    "code": "public void setVariable(String name, Object value) {\n        if (variables == null)\n            variables = new LinkedHashMap();\n        variables.put(name, value);\n    }",
    "reference": "Sets the value of the given variable\n\n@param name  the name of the variable to set\n@param value the new value for the given variable",
    "generated": "Set a variable.\n\n@param name the name\n@param value the value\n@return the value"
  },
  {
    "code": "public Object getProperty(Object object) {\n        MetaMethod getter = getGetter();\n        if (getter == null) {\n            if (field != null) return field.getProperty(object);\n            //TODO: create a WriteOnlyException class?\n            throw new GroovyRuntimeException(\"Cannot read write-only property: \" + name);\n        }\n        return getter.invoke(object, MetaClassHelper.EMPTY_ARRAY);\n    }",
    "reference": "Get the property of the given object.\n\n@param object which to be got\n@return the property of the given object\n@throws RuntimeException if the property could not be evaluated",
    "generated": "Get the property of the given object.\n\n@param object the object\n@return the property value"
  },
  {
    "code": "public void setProperty(Object object, Object newValue) {\n        MetaMethod setter = getSetter();\n        if (setter == null) {\n            if (field != null && !Modifier.isFinal(field.getModifiers())) {\n                field.setProperty(object, newValue);\n                return;\n            }\n            throw new GroovyRuntimeException(\"Cannot set read-only property: \" + name);\n        }\n        newValue = DefaultTypeTransformation.castToType(newValue, getType());\n        setter.invoke(object, new Object[]{newValue});\n    }",
    "reference": "Set the property on the given object to the new value.\n\n@param object   on which to set the property\n@param newValue the new value of the property\n@throws RuntimeException if the property could not be set",
    "generated": "Set the property of the given object.\n\n@param object object\n@param newValue newValue"
  },
  {
    "code": "public int getModifiers() {\n        MetaMethod getter = getGetter();\n        MetaMethod setter = getSetter();\n        if (setter != null && getter == null) return setter.getModifiers();\n        if (getter != null && setter == null) return getter.getModifiers();\n        int modifiers = getter.getModifiers() | setter.getModifiers();\n        int visibility = 0;\n        if (Modifier.isPublic(modifiers)) visibility = Modifier.PUBLIC;\n        if (Modifier.isProtected(modifiers)) visibility = Modifier.PROTECTED;\n        if (Modifier.isPrivate(modifiers)) visibility = Modifier.PRIVATE;\n        int states = getter.getModifiers() & setter.getModifiers();\n        states &= ~(Modifier.PUBLIC | Modifier.PROTECTED | Modifier.PRIVATE);\n        states |= visibility;\n        return states;\n    }",
    "reference": "Gets the visibility modifiers for the property as defined by the getter and setter methods.\n\n@return the visibility modifer of the getter, the setter, or both depending on which exist",
    "generated": "Get the modifiers of the method.\n\n@param method method"
  },
  {
    "code": "public static boolean isTrait(final ClassNode cNode) {\n        return cNode!=null\n                && ((cNode.isInterface() && !cNode.getAnnotations(TRAIT_CLASSNODE).isEmpty())\n                    || isAnnotatedWithTrait(cNode));\n    }",
    "reference": "Returns true if the specified class node is a trait.\n@param cNode a class node to test\n@return true if the classnode represents a trait",
    "generated": "Check whether a class node is a trait.\n\n@param cNode class node\n@return true if the class node is a trait"
  },
  {
    "code": "public static Method getBridgeMethodTarget(Method someMethod) {\n        TraitBridge annotation = someMethod.getAnnotation(TraitBridge.class);\n        if (annotation==null) {\n            return null;\n        }\n        Class aClass = annotation.traitClass();\n        String desc = annotation.desc();\n        for (Method method : aClass.getDeclaredMethods()) {\n            String methodDescriptor = BytecodeHelper.getMethodDescriptor(method.getReturnType(), method.getParameterTypes());\n            if (desc.equals(methodDescriptor)) {\n                return method;\n            }\n        }\n        return null;\n    }",
    "reference": "Reflection API to find the method corresponding to the default implementation of a trait, given a bridge method.\n@param someMethod a method node\n@return null if it is not a method implemented in a trait. If it is, returns the method from the trait class.",
    "generated": "Returns the target method of the given method.\n\n@param someMethod the method\n@return the target method"
  },
  {
    "code": "private static int findNext(boolean reverse, int pos) {\n        boolean backwards = IS_BACKWARDS_CHECKBOX.isSelected();\n        backwards = backwards ? !reverse : reverse;\n\n        String pattern = (String) FIND_FIELD.getSelectedItem();\n        if (pattern != null && pattern.length() > 0) {\n            try {\n                Document doc = textComponent.getDocument();\n                doc.getText(0, doc.getLength(), SEGMENT);\n            }\n            catch (Exception e) {\n                // should NEVER reach here\n                e.printStackTrace();\n            }\n\n            pos += textComponent.getSelectedText() == null ?\n                    (backwards ? -1 : 1) : 0;\n\n            char first = backwards ?\n                    pattern.charAt(pattern.length() - 1) : pattern.charAt(0);\n            char oppFirst = Character.isUpperCase(first) ?\n                    Character.toLowerCase(first) : Character.toUpperCase(first);\n            int start = pos;\n            boolean wrapped = WRAP_SEARCH_CHECKBOX.isSelected();\n            int end = backwards ? 0 : SEGMENT.getEndIndex();\n            pos += backwards ? -1 : 1;\n\n            int length = textComponent.getDocument().getLength();\n            if (pos > length) {\n                pos = wrapped ? 0 : length;\n            }\n\n            boolean found = false;\n            while (!found && (backwards ? pos > end : pos < end)) {\n                found = !MATCH_CASE_CHECKBOX.isSelected() && SEGMENT.array[pos] == oppFirst;\n                found = found ? found : SEGMENT.array[pos] == first;\n\n                if (found) {\n                    pos += backwards ? -(pattern.length() - 1) : 0;\n                    for (int i = 0; found && i < pattern.length(); i++) {\n                        char c = pattern.charAt(i);\n                        found = SEGMENT.array[pos + i] == c;\n                        if (!MATCH_CASE_CHECKBOX.isSelected() && !found) {\n                            c = Character.isUpperCase(c) ?\n                                    Character.toLowerCase(c) :\n                                    Character.toUpperCase(c);\n                            found = SEGMENT.array[pos + i] == c;\n                        }\n                    }\n                }\n\n                if (!found) {\n                    pos += backwards ? -1 : 1;\n\n                    if (pos == end && wrapped) {\n                        pos = backwards ? SEGMENT.getEndIndex() : 0;\n                        end = start;\n                        wrapped = false;\n                    }\n                }\n            }\n            pos = found ? pos : -1;\n        }\n\n        return pos;\n    }",
    "reference": "Find and select the next searchable matching text.\n\n@param reverse look forwards or backwards\n@param pos     the starting index to start finding from\n@return the location of the next selected, or -1 if not found",
    "generated": "Find the next element of the given element.\n\n@param reverse is the reverse value\n@param pos the position of the element\n@return the next element of the element\n@since 1.0.0"
  },
  {
    "code": "protected static void invalidateSwitchPoints() {\n            if (LOG_ENABLED) {\n                 LOG.info(\"invalidating switch point\");\n            }\n        \tSwitchPoint old = switchPoint;\n            switchPoint = new SwitchPoint();\n            synchronized(IndyInterface.class) { SwitchPoint.invalidateAll(new SwitchPoint[]{old}); }\n        }",
    "reference": "Callback for constant meta class update change",
    "generated": "Invalidate the switch points."
  },
  {
    "code": "public static CallSite bootstrapCurrent(Lookup caller, String name, MethodType type) {\n            return realBootstrap(caller, name, CALL_TYPES.METHOD.ordinal(), type, false, true, false);\n        }",
    "reference": "bootstrap method for method calls with \"this\" as receiver\n@deprecated since Groovy 2.1.0",
    "generated": "Bootstrap the current call site.\n\n@param caller the caller\n@param name the method name\n@param type the method to bootstrap\n@return this call site"
  },
  {
    "code": "private static CallSite realBootstrap(Lookup caller, String name, int callID, MethodType type, boolean safe, boolean thisCall, boolean spreadCall) {\n            // since indy does not give us the runtime types\n            // we produce first a dummy call site, which then changes the target to one,\n            // that does the method selection including the the direct call to the \n            // real method.\n            MutableCallSite mc = new MutableCallSite(type);\n            MethodHandle mh = makeFallBack(mc,caller.lookupClass(),name,callID,type,safe,thisCall,spreadCall);\n            mc.setTarget(mh);\n            return mc;\n        }",
    "reference": "backing bootstrap method with all parameters",
    "generated": "This creates a dummy call site, which then changes the target to one"
  },
  {
    "code": "public void addCell(TableLayoutCell cell) {\n        GridBagConstraints constraints = cell.getConstraints();\n        constraints.insets = new Insets(cellpadding, cellpadding, cellpadding, cellpadding);\n        add(cell.getComponent(), constraints);\n    }",
    "reference": "Adds a new cell to the current grid\n@param cell the td component",
    "generated": "Add a table layout cell.\n\n@param cell table layout cell\n@return table layout cell"
  },
  {
    "code": "public void addClass(ClassNode node) {\n        node = node.redirect();\n        String name = node.getName();\n        ClassNode stored = classes.get(name);\n        if (stored != null && stored != node) {\n            // we have a duplicate class!\n            // One possibility for this is, that we declared a script and a\n            // class in the same file and named the class like the file\n            SourceUnit nodeSource = node.getModule().getContext();\n            SourceUnit storedSource = stored.getModule().getContext();\n            String txt = \"Invalid duplicate class definition of class \" + node.getName() + \" : \";\n            if (nodeSource == storedSource) {\n                // same class in same source\n                txt += \"The source \" + nodeSource.getName() + \" contains at least two definitions of the class \" + node.getName() + \".\\n\";\n                if (node.isScriptBody() || stored.isScriptBody()) {\n                    txt += \"One of the classes is an explicit generated class using the class statement, the other is a class generated from\" +\n                            \" the script body based on the file name. Solutions are to change the file name or to change the class name.\\n\";\n                }\n            } else {\n                txt += \"The sources \" + nodeSource.getName() + \" and \" + storedSource.getName() + \" each contain a class with the name \" + node.getName() + \".\\n\";\n            }\n            nodeSource.getErrorCollector().addErrorAndContinue(\n                    new SyntaxErrorMessage(new SyntaxException(txt, node.getLineNumber(), node.getColumnNumber(), node.getLastLineNumber(), node.getLastColumnNumber()), nodeSource)\n            );\n        }\n        classes.put(name, node);\n\n        if (classesToCompile.containsKey(name)) {\n            ClassNode cn = classesToCompile.get(name);\n            cn.setRedirect(node);\n            classesToCompile.remove(name);\n        }\n    }",
    "reference": "Adds a class to the unit.",
    "generated": "Generate a class node.\n\n@param node the class node\n@param node the class node\n@return the class node"
  },
  {
    "code": "private String getSlashyPath(final String path) {\n        String changedPath = path;\n        if (File.separatorChar != '/')\n            changedPath = changedPath.replace(File.separatorChar, '/');\n\n        return changedPath;\n    }",
    "reference": "This solution is based on an absolute path",
    "generated": "Generate a path to the given path.\n\n@param path the path\n@return the path to the changed path\n@since 1.0"
  },
  {
    "code": "public static void write(Path self, String text, String charset) throws IOException {\n        Writer writer = null;\n        try {\n            writer = new OutputStreamWriter(Files.newOutputStream(self, CREATE, APPEND), Charset.forName(charset));\n            writer.write(text);\n            writer.flush();\n\n            Writer temp = writer;\n            writer = null;\n            temp.close();\n        } finally {\n            closeWithWarning(writer);\n        }\n    }",
    "reference": "Write the text to the Path, using the specified encoding.\n\n@param self    a Path\n@param text    the text to write to the Path\n@param charset the charset used\n@throws java.io.IOException if an IOException occurs.\n@since 2.3.0",
    "generated": "Write a text to the path.\n\n@param self the path\n@param text the text to write\n@param charset the charset\n@return the text to write\n@throws IOException"
  },
  {
    "code": "public static void append(Path self, Object text) throws IOException {\n        Writer writer = null;\n        try {\n            writer = new OutputStreamWriter(Files.newOutputStream(self, CREATE, APPEND), Charset.defaultCharset());\n            InvokerHelper.write(writer, text);\n            writer.flush();\n\n            Writer temp = writer;\n            writer = null;\n            temp.close();\n        } finally {\n            closeWithWarning(writer);\n        }\n    }",
    "reference": "Append the text at the end of the Path.\n\n@param self a Path\n@param text the text to append at the end of the Path\n@throws java.io.IOException if an IOException occurs.\n@since 2.3.0",
    "generated": "Append an object to the path.\n\n@param self the path\n@param text the text to append\n@return the text to append\n@since 1.0.0"
  },
  {
    "code": "public static int count(CharSequence self, CharSequence text) {\n        int answer = 0;\n        for (int idx = 0; true; idx++) {\n            idx = self.toString().indexOf(text.toString(), idx);\n            // break once idx goes to -1 or for case of empty string once\n            // we get to the end to avoid JDK library bug (see GROOVY-5858)\n            if (idx < answer) break;\n            ++answer;\n        }\n        return answer;\n    }",
    "reference": "Count the number of occurrences of a sub CharSequence.\n\n@param self a CharSequence\n@param text a sub CharSequence\n@return the number of occurrences of the given CharSequence inside this CharSequence\n@see #count(String, String)\n@since 1.8.2",
    "generated": "Count the characters in a CharSequence."
  },
  {
    "code": "public static <T extends CharSequence> T eachMatch(T self, CharSequence regex, @ClosureParams(value=FromString.class, options={\"List<String>\",\"String[]\"}) Closure closure) {\n        eachMatch(self.toString(), regex.toString(), closure);\n        return self;\n    }",
    "reference": "Process each regex group matched substring of the given CharSequence. If the closure\nparameter takes one argument, an array with all match groups is passed to it.\nIf the closure takes as many arguments as there are match groups, then each\nparameter will be one match group.\n\n@param self    the source CharSequence\n@param regex   a Regex CharSequence\n@param closure a closure with one parameter or as much parameters as groups\n@return the source CharSequence\n@see #eachMatch(String, String, groovy.lang.Closure)\n@since 1.8.2",
    "generated": "Each match.\n\n@param self the CharSequence\n@param regex the regex\n@param closure the closure to be called"
  },
  {
    "code": "public static String eachMatch(String self, String regex, @ClosureParams(value=FromString.class, options={\"List<String>\",\"String[]\"}) Closure closure) {\n        return eachMatch(self, Pattern.compile(regex), closure);\n    }",
    "reference": "Process each regex group matched substring of the given string. If the closure\nparameter takes one argument, an array with all match groups is passed to it.\nIf the closure takes as many arguments as there are match groups, then each\nparameter will be one match group.\n\n@param self    the source string\n@param regex   a Regex string\n@param closure a closure with one parameter or as much parameters as groups\n@return the source string\n@since 1.6.0",
    "generated": "Each match to the given regex.\n\n@param self the string\n@param regex the regex to match\n@param closure the closure to match\n@return the string"
  },
  {
    "code": "public static String expandLine(CharSequence self, int tabStop) {\n        String s = self.toString();\n        int index;\n        while ((index = s.indexOf('\\t')) != -1) {\n            StringBuilder builder = new StringBuilder(s);\n            int count = tabStop - index % tabStop;\n            builder.deleteCharAt(index);\n            for (int i = 0; i < count; i++) builder.insert(index, \" \");\n            s = builder.toString();\n        }\n        return s;\n    }",
    "reference": "Expands all tabs into spaces. Assumes the CharSequence represents a single line of text.\n\n@param self A line to expand\n@param tabStop The number of spaces a tab represents\n@return The expanded toString() of this CharSequence\n@see #expandLine(String, int)\n@since 1.8.2",
    "generated": "Expand a character sequence.\n\n@param self the character sequence\n@param tabStop the tab stop index"
  },
  {
    "code": "public static String find(CharSequence self, CharSequence regex, @ClosureParams(value=SimpleType.class, options=\"java.lang.String[]\") Closure closure) {\n        return find(self.toString(), Pattern.compile(regex.toString()), closure);\n    }",
    "reference": "Returns the result of calling a closure with the first occurrence of a regular expression found within a CharSequence.\nIf the regex doesn't match, the closure will not be called and find will return null.\n\n@param self    a CharSequence\n@param regex   the capturing regex CharSequence\n@param closure the closure that will be passed the full match, plus each of the capturing groups (if any)\n@return a String containing the result of calling the closure (calling toString() if needed), or null if the regex pattern doesn't match\n@see #find(String, java.util.regex.Pattern, groovy.lang.Closure)\n@since 1.8.2",
    "generated": "Find a CharSequence with the given regex.\n\n@param self CharSequence\n@param regex CharSequence\n@param closure Closure"
  },
  {
    "code": "public static String getAt(CharSequence self, Collection indices) {\n        StringBuilder answer = new StringBuilder();\n        for (Object value : indices) {\n            if (value instanceof Range) {\n                answer.append(getAt(self, (Range) value));\n            } else if (value instanceof Collection) {\n                answer.append(getAt(self, (Collection) value));\n            } else {\n                int idx = DefaultTypeTransformation.intUnbox(value);\n                answer.append(getAt(self, idx));\n            }\n        }\n        return answer.toString();\n    }",
    "reference": "Select a List of characters from a CharSequence using a Collection\nto identify the indices to be selected.\n\n@param self    a CharSequence\n@param indices a Collection of indices\n@return a String consisting of the characters at the given indices\n@since 1.0",
    "generated": "Get the index of the specified indices.\n\n@param self the CharSequence\n@param indices the indices\n@return the index"
  },
  {
    "code": "public static CharSequence getAt(CharSequence text, int index) {\n        index = normaliseIndex(index, text.length());\n        return text.subSequence(index, index + 1);\n    }",
    "reference": "Support the subscript operator for CharSequence.\n\n@param text  a CharSequence\n@param index the index of the Character to get\n@return the Character at the given index\n@since 1.0",
    "generated": "Get the character at the specified index.\n\n@param text text text\n@param index index index"
  },
  {
    "code": "public static String getAt(GString text, int index) {\n        return (String) getAt(text.toString(), index);\n    }",
    "reference": "Support the subscript operator for GString.\n\n@param text  a GString\n@param index the index of the Character to get\n@return the Character at the given index\n@since 2.3.7",
    "generated": "Get the value at the specified index.\n\n@param text the text\n@param index the index of the text\n@return the value"
  },
  {
    "code": "public static CharSequence getAt(CharSequence text, IntRange range) {\n        return getAt(text, (Range) range);\n    }",
    "reference": "Support the range subscript operator for CharSequence with IntRange\n\n@param text  a CharSequence\n@param range an IntRange\n@return the subsequence CharSequence\n@since 1.0",
    "generated": "Get a CharSequence from the specified IntRange.\n\n@param text CharSequence\n@param range IntRange"
  },
  {
    "code": "public static CharSequence getAt(CharSequence text, Range range) {\n        RangeInfo info = subListBorders(text.length(), range);\n        CharSequence sequence = text.subSequence(info.from, info.to);\n        return info.reverse ? reverse(sequence) : sequence;\n    }",
    "reference": "Support the range subscript operator for CharSequence\n\n@param text  a CharSequence\n@param range a Range\n@return the subsequence CharSequence\n@since 1.0",
    "generated": "Get a character from a given range.\n\n@param text CharSequence\n@param range Range"
  },
  {
    "code": "public static String getAt(GString text, Range range) {\n        return getAt(text.toString(), range);\n    }",
    "reference": "Support the range subscript operator for GString\n\n@param text  a GString\n@param range a Range\n@return the String of characters corresponding to the provided range\n@since 2.3.7",
    "generated": "Returns the value of the given text.\n\n@param text the text\n@param range the range\n@return the value of the text\n@since 1.0"
  },
  {
    "code": "public static List getAt(Matcher self, Collection indices) {\n        List result = new ArrayList();\n        for (Object value : indices) {\n            if (value instanceof Range) {\n                result.addAll(getAt(self, (Range) value));\n            } else {\n                int idx = DefaultTypeTransformation.intUnbox(value);\n                result.add(getAt(self, idx));\n            }\n        }\n        return result;\n    }",
    "reference": "Select a List of values from a Matcher using a Collection\nto identify the indices to be selected.\n\n@param self    a Matcher\n@param indices a Collection of indices\n@return a String of the values at the given indices\n@since 1.6.0",
    "generated": "Get a list of indices.\n\n@param indices indices"
  },
  {
    "code": "public static String getAt(String text, int index) {\n        index = normaliseIndex(index, text.length());\n        return text.substring(index, index + 1);\n    }",
    "reference": "Support the subscript operator for String.\n\n@param text  a String\n@param index the index of the Character to get\n@return the Character at the given index\n@since 1.0",
    "generated": "Returns the text at the specified index.\n\n@param text the text\n@param index the index of the text\n@return the text"
  },
  {
    "code": "public static String getAt(String text, IntRange range) {\n        return getAt(text, (Range) range);\n    }",
    "reference": "Support the range subscript operator for String with IntRange\n\n@param text  a String\n@param range an IntRange\n@return the resulting String\n@since 1.0",
    "generated": "Get an integer from a string.\n\n@param text the string\n@param range the integer range\n@return the integer value"
  },
  {
    "code": "public static String getAt(String text, Range range) {\n        RangeInfo info = subListBorders(text.length(), range);\n        String answer = text.substring(info.from, info.to);\n        if (info.reverse) {\n            answer = reverse(answer);\n        }\n        return answer;\n    }",
    "reference": "Support the range subscript operator for String\n\n@param text  a String\n@param range a Range\n@return a substring corresponding to the Range\n@since 1.0",
    "generated": "Get an item from a given range.\n\n@param text a string\n@param range a range\n@return a string"
  },
  {
    "code": "public static int getCount(Matcher matcher) {\n        int counter = 0;\n        matcher.reset();\n        while (matcher.find()) {\n            counter++;\n        }\n        return counter;\n    }",
    "reference": "Find the number of Strings matched to the given Matcher.\n\n@param matcher a Matcher\n@return int  the number of Strings matched to the given matcher.\n@since 1.0",
    "generated": "Returns the count of the matching matcher.\n\n@param matcher Matcher to find the matching matcher\n@return the count of the matching matcher."
  },
  {
    "code": "public static boolean isAllWhitespace(CharSequence self) {\n        String s = self.toString();\n        for (int i = 0; i < s.length(); i++) {\n            if (!Character.isWhitespace(s.charAt(i)))\n                return false;\n        }\n        return true;\n    }",
    "reference": "True if a CharSequence only contains whitespace characters.\n\n@param self The CharSequence to check the characters in\n@return true If all characters are whitespace characters\n@see #isAllWhitespace(String)\n@since 1.8.2",
    "generated": "Check if the character sequence is all whitespace.\n\n@param self CharSequence\n@return true if the character sequence is all whitespace\n@return false if the character sequence is all whitespace\n@since 1.0"
  },
  {
    "code": "public static boolean isBigDecimal(CharSequence self) {\n        try {\n            new BigDecimal(self.toString().trim());\n            return true;\n        } catch (NumberFormatException nfe) {\n            return false;\n        }\n    }",
    "reference": "Determine if a CharSequence can be parsed as a BigDecimal.\n\n@param self a CharSequence\n@return true if the CharSequence can be parsed\n@see #isBigDecimal(String)\n@since 1.8.2",
    "generated": "Check if the CharSequence is a BigDecimal.\n\n@param self CharSequence\n@return true if the CharSequence is a BigDecimal."
  },
  {
    "code": "public static boolean isBigInteger(CharSequence self) {\n        try {\n            new BigInteger(self.toString().trim());\n            return true;\n        } catch (NumberFormatException nfe) {\n            return false;\n        }\n    }",
    "reference": "Determine if a CharSequence can be parsed as a BigInteger.\n\n@param self a CharSequence\n@return true if the CharSequence can be parsed\n@see #isBigInteger(String)\n@since 1.8.2",
    "generated": "Check if the CharSequence is a BigInteger.\n\n@param self CharSequence\n@return true if the CharSequence is a BigInteger."
  },
  {
    "code": "public static boolean isDouble(CharSequence self) {\n        try {\n            Double.valueOf(self.toString().trim());\n            return true;\n        } catch (NumberFormatException nfe) {\n            return false;\n        }\n    }",
    "reference": "Determine if a CharSequence can be parsed as a Double.\n\n@param self a CharSequence\n@return true if the CharSequence can be parsed\n@see #isDouble(String)\n@since 1.8.2",
    "generated": "Check whether the CharSequence is a double.\n\n@param self CharSequence\n@return true if the CharSequence is a double"
  },
  {
    "code": "public static boolean isFloat(CharSequence self) {\n        try {\n            Float.valueOf(self.toString().trim());\n            return true;\n        } catch (NumberFormatException nfe) {\n            return false;\n        }\n    }",
    "reference": "Determine if a CharSequence can be parsed as a Float.\n\n@param self a CharSequence\n@return true if the CharSequence can be parsed\n@see #isFloat(String)\n@since 1.8.2",
    "generated": "Check whether the CharSequence is a float.\n\n@param self CharSequence\n@return true if the CharSequence is a float"
  },
  {
    "code": "public static boolean isInteger(CharSequence self) {\n        try {\n            Integer.valueOf(self.toString().trim());\n            return true;\n        } catch (NumberFormatException nfe) {\n            return false;\n        }\n    }",
    "reference": "Determine if a CharSequence can be parsed as an Integer.\n\n@param self a CharSequence\n@return true if the CharSequence can be parsed\n@see #isInteger(String)\n@since 1.8.2",
    "generated": "Check whether the character sequence is an integer.\n\n@param self CharSequence\n@return true if the character sequence is an integer"
  },
  {
    "code": "public static boolean isLong(CharSequence self) {\n        try {\n            Long.valueOf(self.toString().trim());\n            return true;\n        } catch (NumberFormatException nfe) {\n            return false;\n        }\n    }",
    "reference": "Determine if a CharSequence can be parsed as a Long.\n\n@param self a CharSequence\n@return true if the CharSequence can be parsed\n@see #isLong(String)\n@since 1.8.2",
    "generated": "Check whether the CharSequence is a long value.\n\n@param self CharSequence\n@return true if the CharSequence is a long value"
  },
  {
    "code": "public static StringBuffer leftShift(String self, Object value) {\n        return new StringBuffer(self).append(value);\n    }",
    "reference": "Overloads the left shift operator to provide an easy way to append multiple\nobjects as string representations to a String.\n\n@param self  a String\n@param value an Object\n@return a StringBuffer built from this string\n@since 1.0",
    "generated": "Shift the given value.\n\n@param self the string to shift.\n@param value the value to shift\n@return the string to shift."
  },
  {
    "code": "public static StringBuilder leftShift(StringBuilder self, Object value) {\n        self.append(value);\n        return self;\n    }",
    "reference": "Overloads the left shift operator to provide syntactic sugar for appending to a StringBuilder.\n\n@param self  a StringBuilder\n@param value an Object\n@return the original StringBuilder\n@since 1.8.2",
    "generated": "Shift the given value.\n\n@param value the value to shift."
  },
  {
    "code": "public static String minus(CharSequence self, Object target) {\n        String s = self.toString();\n        String text = DefaultGroovyMethods.toString(target);\n        int index = s.indexOf(text);\n        if (index == -1) return s;\n        int end = index + text.length();\n        if (s.length() > end) {\n            return s.substring(0, index) + s.substring(end);\n        }\n        return s.substring(0, index);\n    }",
    "reference": "Remove a part of a CharSequence by replacing the first occurrence\nof target within self with '' and returns the result.\n\n@param self   a CharSequence\n@param target an object representing the part to remove\n@return a String containing the original minus the part to be removed\n@see #minus(String, Object)\n@since 1.8.2",
    "generated": "Subtract a character sequence.\n\n@param self CharSequence\n@param target CharSequence\n@return CharSequence"
  },
  {
    "code": "public static String minus(CharSequence self, Pattern pattern) {\n        return pattern.matcher(self).replaceFirst(\"\");\n    }",
    "reference": "Remove a part of a CharSequence. This replaces the first occurrence\nof the pattern within self with '' and returns the result.\n\n@param self   a String\n@param pattern a Pattern representing the part to remove\n@return a String minus the part to be removed\n@since 2.2.0",
    "generated": "Subtract a character sequence.\n\n@param self the character sequence\n@param pattern the pattern to match\n@return the character sequence"
  },
  {
    "code": "public static String multiply(CharSequence self, Number factor) {\n        String s = self.toString();\n        int size = factor.intValue();\n        if (size == 0)\n            return \"\";\n        else if (size < 0) {\n            throw new IllegalArgumentException(\"multiply() should be called with a number of 0 or greater not: \" + size);\n        }\n        StringBuilder answer = new StringBuilder(s);\n        for (int i = 1; i < size; i++) {\n            answer.append(s);\n        }\n        return answer.toString();\n    }",
    "reference": "Repeat a CharSequence a certain number of times.\n\n@param self   a CharSequence to be repeated\n@param factor the number of times the CharSequence should be repeated\n@return a String composed of a repetition\n@throws IllegalArgumentException if the number of repetitions is &lt; 0\n@since 1.8.2",
    "generated": "Multiply a character sequence.\n\n@param self the character sequence\n@param factor the factor to multiply."
  },
  {
    "code": "public static String next(CharSequence self) {\n        StringBuilder buffer = new StringBuilder(self);\n        if (buffer.length() == 0) {\n            buffer.append(Character.MIN_VALUE);\n        } else {\n            char last = buffer.charAt(buffer.length() - 1);\n            if (last == Character.MAX_VALUE) {\n                buffer.append(Character.MIN_VALUE);\n            } else {\n                char next = last;\n                next++;\n                buffer.setCharAt(buffer.length() - 1, next);\n            }\n        }\n        return buffer.toString();\n    }",
    "reference": "This method is called by the ++ operator for the class CharSequence.\nIt increments the last character in the given CharSequence. If the last\ncharacter in the CharSequence is Character.MAX_VALUE a Character.MIN_VALUE\nwill be appended. The empty CharSequence is incremented to a string\nconsisting of the character Character.MIN_VALUE.\n\n@param self a CharSequence\n@return a value obtained by incrementing the toString() of the CharSequence\n@since 1.8.2",
    "generated": "Next character.\n\n@param self CharSequence"
  },
  {
    "code": "public static String normalize(final CharSequence self) {\n        final String s = self.toString();\n        int nx = s.indexOf('\\r');\n\n        if (nx < 0) {\n            return s;\n        }\n\n        final int len = s.length();\n        final StringBuilder sb = new StringBuilder(len);\n\n        int i = 0;\n\n        do {\n            sb.append(s, i, nx);\n            sb.append('\\n');\n\n            if ((i = nx + 1) >= len) break;\n\n            if (s.charAt(i) == '\\n') {\n                // skip the LF in CR LF\n                if (++i >= len) break;\n            }\n\n            nx = s.indexOf('\\r', i);\n        } while (nx > 0);\n\n        sb.append(s, i, len);\n\n        return sb.toString();\n    }",
    "reference": "Return a String with linefeeds and carriage returns normalized to linefeeds.\n\n@param self a CharSequence object\n@return the normalized toString() for the CharSequence\n@see #normalize(String)\n@since 1.8.2",
    "generated": "Normalize a CharSequence.\n\n@param self CharSequence to be normalized\n@return the normalized string"
  },
  {
    "code": "public static String plus(CharSequence left, Object value) {\n        return left + DefaultGroovyMethods.toString(value);\n    }",
    "reference": "Appends the String representation of the given operand to this CharSequence.\n\n@param left  a CharSequence\n@param value any Object\n@return the original toString() of the CharSequence with the object appended\n@since 1.8.2",
    "generated": "Add a character sequence to the given value.\n\n@param left the character sequence\n@param value the value to add\n@return the value"
  },
  {
    "code": "public static String plus(Number value, String right) {\n        return DefaultGroovyMethods.toString(value) + right;\n    }",
    "reference": "Appends a String to the string representation of this number.\n\n@param value a Number\n@param right a String\n@return a String\n@since 1.0",
    "generated": "Add a number to the given value.\n\n@param value value to plus\n@param right"
  },
  {
    "code": "public static List<String> readLines(CharSequence self) throws IOException {\n        return IOGroovyMethods.readLines(new StringReader(self.toString()));\n    }",
    "reference": "Return the lines of a CharSequence as a List of String.\n\n@param self a CharSequence object\n@return a list of lines\n@throws java.io.IOException if an error occurs\n@since 1.8.2",
    "generated": "Read a list of lines.\n\n@param self CharSequence\n@return List of lines"
  },
  {
    "code": "public static String replaceAll(final CharSequence self, final CharSequence regex, final CharSequence replacement) {\n        return self.toString().replaceAll(regex.toString(), replacement.toString());\n    }",
    "reference": "Replaces each substring of this CharSequence that matches the given\nregular expression with the given replacement.\n\n@param self        a CharSequence\n@param regex       the capturing regex\n@param replacement the string to be substituted for each match\n@return the toString() of the CharSequence with content replaced\n@throws java.util.regex.PatternSyntaxException if the regular expression's syntax is invalid\n@see String#replaceAll(String, String)\n@since 1.8.2",
    "generated": "Replaces a CharSequence with the given regex.\n\n@param self CharSequence\n@param regex CharSequence\n@param replacement CharSequence\n@return CharSequence"
  },
  {
    "code": "public static String replaceFirst(final CharSequence self, final CharSequence regex, final CharSequence replacement) {\n        return self.toString().replaceFirst(regex.toString(), replacement.toString());\n    }",
    "reference": "Replaces the first substring of this CharSequence that matches the given\nregular expression with the given replacement.\n\n@param self        a CharSequence\n@param regex       the capturing regex\n@param replacement the CharSequence to be substituted for each match\n@return a CharSequence with replaced content\n@throws java.util.regex.PatternSyntaxException if the regular expression's syntax is invalid\n@see String#replaceFirst(String, String)\n@since 1.8.2",
    "generated": "Replace a regex with a replacement.\n\n@param self CharSequence\n@param regex CharSequence\n@param replacement CharSequence\n@return String"
  },
  {
    "code": "public static void setIndex(Matcher matcher, int idx) {\n        int count = getCount(matcher);\n        if (idx < -count || idx >= count) {\n            throw new IndexOutOfBoundsException(\"index is out of range \" + (-count) + \"..\" + (count - 1) + \" (index = \" + idx + \")\");\n        }\n        if (idx == 0) {\n            matcher.reset();\n        } else if (idx > 0) {\n            matcher.reset();\n            for (int i = 0; i < idx; i++) {\n                matcher.find();\n            }\n        } else if (idx < 0) {\n            matcher.reset();\n            idx += getCount(matcher);\n            for (int i = 0; i < idx; i++) {\n                matcher.find();\n            }\n        }\n    }",
    "reference": "Set the position of the given Matcher to the given index.\n\n@param matcher a Matcher\n@param idx     the index number\n@since 1.0",
    "generated": "Set the index of the given matcher.\n\n@param matcher Matcher to be used"
  },
  {
    "code": "public static <T> T splitEachLine(CharSequence self, CharSequence regex, @ClosureParams(value=FromString.class,options=\"List<String>\") Closure<T> closure) throws IOException {\n        return splitEachLine(self, Pattern.compile(regex.toString()), closure);\n    }",
    "reference": "Iterates through the given CharSequence line by line, splitting each line using\nthe given regex delimiter.  The list of tokens for each line is then passed to\nthe given closure.\n\n@param self    a CharSequence\n@param regex   the delimiting regular expression\n@param closure a closure\n@return the last value returned by the closure\n@throws java.io.IOException if an error occurs\n@throws java.util.regex.PatternSyntaxException if the regular expression's syntax is invalid\n@see #splitEachLine(CharSequence, java.util.regex.Pattern, groovy.lang.Closure)\n@since 1.8.2",
    "generated": "Split each line of a CharSequence.\n\n@param self CharSequence\n@param regex CharSequence\n@param closure Closure"
  },
  {
    "code": "public static <T> T splitEachLine(CharSequence self, Pattern pattern, @ClosureParams(value=FromString.class,options=\"List<String>\") Closure<T> closure) throws IOException {\n        final List<String> list = readLines(self);\n        T result = null;\n        for (String line : list) {\n            List vals = Arrays.asList(pattern.split(line));\n            result = closure.call(vals);\n        }\n        return result;\n    }",
    "reference": "Iterates through the given CharSequence line by line, splitting each line using\nthe given separator Pattern.  The list of tokens for each line is then passed to\nthe given closure.\n\n@param self    a CharSequence\n@param pattern the regular expression Pattern for the delimiter\n@param closure a closure\n@return the last value returned by the closure\n@throws java.io.IOException if an error occurs\n@since 1.8.2",
    "generated": "Split each line.\n\n@param self the CharSequence\n@param pattern the pattern\n@param closure the closure to call"
  },
  {
    "code": "public static String takeWhile(GString self, @ClosureParams(value=SimpleType.class, options=\"char\") Closure condition) {\n        return (String) takeWhile(self.toString(), condition);\n    }",
    "reference": "A GString variant of the equivalent GString method.\n\n@param self      the original GString\n@param condition the closure that must evaluate to true to continue taking elements\n@return a prefix of elements in the GString where each\nelement passed to the given closure evaluates to true\n@since 2.3.7",
    "generated": "Take a given Closure.\n\n@param self the string to take.\n@param condition the Closure to take."
  },
  {
    "code": "public static List<String> toList(CharSequence self) {\n        String s = self.toString();\n        int size = s.length();\n        List<String> answer = new ArrayList<String>(size);\n        for (int i = 0; i < size; i++) {\n            answer.add(s.substring(i, i + 1));\n        }\n        return answer;\n    }",
    "reference": "Converts the given CharSequence into a List of Strings of one character.\n\n@param self a CharSequence\n@return a List of characters (a 1-character String)\n@see #toSet(String)\n@since 1.8.2",
    "generated": "Generate a list of strings.\n\n@param self CharSequence\n@return a list of strings"
  },
  {
    "code": "public static String unexpand(CharSequence self, int tabStop) {\n        String s = self.toString();\n        if (s.length() == 0) return s;\n        try {\n            StringBuilder builder = new StringBuilder();\n            for (String line : readLines((CharSequence) s)) {\n                builder.append(unexpandLine(line, tabStop));\n                builder.append(\"\\n\");\n            }\n            // remove the normalized ending line ending if it was not present\n            if (!s.endsWith(\"\\n\")) {\n                builder.deleteCharAt(builder.length() - 1);\n            }\n            return builder.toString();\n        } catch (IOException e) {\n            /* ignore */\n        }\n        return s;\n    }",
    "reference": "Replaces sequences of whitespaces with tabs.\n\n@param self A CharSequence to unexpand\n@param tabStop The number of spaces a tab represents\n@return an unexpanded String\n@since 1.8.2",
    "generated": "Unexpand a character sequence.\n\n@param self the character sequence\n@param tabStop the tab stop"
  },
  {
    "code": "public static String unexpandLine(CharSequence self, int tabStop) {\n        StringBuilder builder = new StringBuilder(self.toString());\n        int index = 0;\n        while (index + tabStop < builder.length()) {\n            // cut original string in tabstop-length pieces\n            String piece = builder.substring(index, index + tabStop);\n            // count trailing whitespace characters\n            int count = 0;\n            while ((count < tabStop) && (Character.isWhitespace(piece.charAt(tabStop - (count + 1)))))\n                count++;\n            // replace if whitespace was found\n            if (count > 0) {\n                piece = piece.substring(0, tabStop - count) + '\\t';\n                builder.replace(index, index + tabStop, piece);\n                index = index + tabStop - (count - 1);\n            } else\n                index = index + tabStop;\n        }\n        return builder.toString();\n    }",
    "reference": "Replaces sequences of whitespaces with tabs within a line.\n\n@param self A line to unexpand\n@param tabStop The number of spaces a tab represents\n@return an unexpanded String\n@since 1.8.2",
    "generated": "Unexpand a character sequence.\n\n@param self the character sequence\n@param tabStop the tab stop"
  },
  {
    "code": "public boolean hasPossibleMethod(String name, Expression arguments) {\n        int count = 0;\n\n        if (arguments instanceof TupleExpression) {\n            TupleExpression tuple = (TupleExpression) arguments;\n            // TODO this won't strictly be true when using list expansion in argument calls\n            count = tuple.getExpressions().size();\n        }\n        ClassNode node = this;\n        do {\n            for (MethodNode method : getMethods(name)) {\n                if (method.getParameters().length == count && !method.isStatic()) {\n                    return true;\n                }\n            }\n            node = node.getSuperClass();\n        }\n        while (node != null);\n        return false;\n    }",
    "reference": "Returns true if the given method has a possibly matching instance method with the given name and arguments.\n\n@param name      the name of the method of interest\n@param arguments the arguments to match against\n@return true if a matching method was found",
    "generated": "Check if a method has a possible method.\n\n@param name the name\n@param arguments the arguments\n@return true if the method has a possible method."
  },
  {
    "code": "private void checkOrMarkPrivateAccess(Expression source, FieldNode fn) {\n        if (fn!=null && Modifier.isPrivate(fn.getModifiers()) &&\n            (fn.getDeclaringClass() != typeCheckingContext.getEnclosingClassNode() || typeCheckingContext.getEnclosingClosure()!=null) &&\n            fn.getDeclaringClass().getModule() == typeCheckingContext.getEnclosingClassNode().getModule()) {\n            addPrivateFieldOrMethodAccess(source, fn.getDeclaringClass(), StaticTypesMarker.PV_FIELDS_ACCESS, fn);\n        }\n    }",
    "reference": "Given a field node, checks if we are calling a private field from an inner class.",
    "generated": "Check if the field is private.\n\n@param source Field node\n@param fn FieldNode"
  },
  {
    "code": "private void checkOrMarkPrivateAccess(Expression source, MethodNode mn) {\n        if (mn==null) {\n            return;\n        }\n        ClassNode declaringClass = mn.getDeclaringClass();\n        ClassNode enclosingClassNode = typeCheckingContext.getEnclosingClassNode();\n        if (declaringClass != enclosingClassNode || typeCheckingContext.getEnclosingClosure() != null) {\n            int mods = mn.getModifiers();\n            boolean sameModule = declaringClass.getModule() == enclosingClassNode.getModule();\n            String packageName = declaringClass.getPackageName();\n            if (packageName==null) {\n                packageName = \"\";\n            }\n            if ((Modifier.isPrivate(mods) && sameModule)\n                    || (Modifier.isProtected(mods) && !packageName.equals(enclosingClassNode.getPackageName()))) {\n                addPrivateFieldOrMethodAccess(source, sameModule? declaringClass : enclosingClassNode, StaticTypesMarker.PV_METHODS_ACCESS, mn);\n            }\n        }\n    }",
    "reference": "Given a method node, checks if we are calling a private method from an inner class.",
    "generated": "Check if the method is marked as private."
  },
  {
    "code": "private boolean ensureValidSetter(final Expression expression, final Expression leftExpression, final Expression rightExpression, final SetterInfo setterInfo) {\n        // for expressions like foo = { ... }\n        // we know that the RHS type is a closure\n        // but we must check if the binary expression is an assignment\n        // because we need to check if a setter uses @DelegatesTo\n        VariableExpression ve = new VariableExpression(\"%\", setterInfo.receiverType);\n        MethodCallExpression call = new MethodCallExpression(\n                ve,\n                setterInfo.name,\n                rightExpression\n        );\n        call.setImplicitThis(false);\n        visitMethodCallExpression(call);\n        MethodNode directSetterCandidate = call.getNodeMetaData(StaticTypesMarker.DIRECT_METHOD_CALL_TARGET);\n        if (directSetterCandidate==null) {\n            // this may happen if there's a setter of type boolean/String/Class, and that we are using the property\n            // notation AND that the RHS is not a boolean/String/Class\n            for (MethodNode setter : setterInfo.setters) {\n                ClassNode type = getWrapper(setter.getParameters()[0].getOriginType());\n                if (Boolean_TYPE.equals(type) || STRING_TYPE.equals(type) || CLASS_Type.equals(type)) {\n                    call = new MethodCallExpression(\n                            ve,\n                            setterInfo.name,\n                            new CastExpression(type,rightExpression)\n                    );\n                    call.setImplicitThis(false);\n                    visitMethodCallExpression(call);\n                    directSetterCandidate = call.getNodeMetaData(StaticTypesMarker.DIRECT_METHOD_CALL_TARGET);\n                    if (directSetterCandidate!=null) {\n                        break;\n                    }\n                }\n            }\n        }\n        if (directSetterCandidate != null) {\n            for (MethodNode setter : setterInfo.setters) {\n                if (setter == directSetterCandidate) {\n                    leftExpression.putNodeMetaData(StaticTypesMarker.DIRECT_METHOD_CALL_TARGET, directSetterCandidate);\n                    storeType(leftExpression, getType(rightExpression));\n                    break;\n                }\n            }\n        } else {\n            ClassNode firstSetterType = setterInfo.setters.iterator().next().getParameters()[0].getOriginType();\n            addAssignmentError(firstSetterType, getType(rightExpression), expression);\n            return true;\n        }\n        return false;\n    }",
    "reference": "Given a binary expression corresponding to an assignment, will check that the type of the RHS matches one\nof the possible setters and if not, throw a type checking error.\n@param expression the assignment expression\n@param leftExpression left expression of the assignment\n@param rightExpression right expression of the assignment\n@param setterInfo possible setters\n@return true if type checking passed",
    "generated": "Check if the RHS type is a closure\n        // because we must check if the binary expression is an assignment\n        // because we need to check if the binary expression is an assignment\n        // because we need to check if the binary expression is an assignment\n        // because we need to check if a setter uses @DelegatesTo\n        VariableExpression ve = new VariableExpression(\"%\", setterInfo.receiverType);"
  },
  {
    "code": "private void addArrayMethods(List<MethodNode> methods, ClassNode receiver, String name, ClassNode[] args) {\n        if (args.length!=1) return;\n        if (!receiver.isArray()) return;\n        if (!isIntCategory(getUnwrapper(args[0]))) return;\n        if (\"getAt\".equals(name)) {\n            MethodNode node = new MethodNode(name, Opcodes.ACC_PUBLIC, receiver.getComponentType(), new Parameter[]{new Parameter(args[0],\"arg\")}, null, null);\n            node.setDeclaringClass(receiver.redirect());\n            methods.add(node);\n        } else if (\"setAt\".equals(name)) {\n            MethodNode node = new MethodNode(name, Opcodes.ACC_PUBLIC, VOID_TYPE, new Parameter[]{new Parameter(args[0],\"arg\")}, null, null);\n            node.setDeclaringClass(receiver.redirect());\n            methods.add(node);\n        }\n    }",
    "reference": "add various getAt and setAt methods for primitive arrays\n@param receiver the receiver class\n@param name  the name of the method\n@param args the argument classes",
    "generated": "Add an array of methods.\n\n@param methods the list of methods to add\n@param receiver the receiver\n@param args the arguments to add\n@return the list of methods"
  },
  {
    "code": "private ClassNode getGenericsResolvedTypeOfFieldOrProperty(AnnotatedNode an, ClassNode type) {\n        if (!type.isUsingGenerics()) return type;\n        Map<String, GenericsType> connections = new HashMap();\n        //TODO: inner classes mean a different this-type. This is ignored here!\n        extractGenericsConnections(connections, typeCheckingContext.getEnclosingClassNode(), an.getDeclaringClass());\n        type= applyGenericsContext(connections, type);\n        return type;\n    }",
    "reference": "resolves a Field or Property node generics by using the current class and\nthe declaring class to extract the right meaning of the generics symbols\n@param an a FieldNode or PropertyNode\n@param type the origin type\n@return the new ClassNode with corrected generics",
    "generated": "Returns the resolved type of field or property."
  },
  {
    "code": "public static byte[] decodeBase64(String value) {\n        int byteShift = 4;\n        int tmp = 0;\n        boolean done = false;\n        final StringBuilder buffer = new StringBuilder();\n\n        for (int i = 0; i != value.length(); i++) {\n            final char c = value.charAt(i);\n            final int sixBit = (c < 123) ? EncodingGroovyMethodsSupport.TRANSLATE_TABLE[c] : 66;\n\n            if (sixBit < 64) {\n                if (done)\n                    throw new RuntimeException(\"= character not at end of base64 value\"); // TODO: change this exception type\n\n                tmp = (tmp << 6) | sixBit;\n\n                if (byteShift-- != 4) {\n                    buffer.append((char) ((tmp >> (byteShift * 2)) & 0XFF));\n                }\n\n            } else if (sixBit == 64) {\n\n                byteShift--;\n                done = true;\n\n            } else if (sixBit == 66) {\n                // RFC 2045 says that I'm allowed to take the presence of\n                // these characters as evidence of data corruption\n                // So I will\n                throw new RuntimeException(\"bad character in base64 value\"); // TODO: change this exception type\n            }\n\n            if (byteShift == 0) byteShift = 4;\n        }\n\n        try {\n            return buffer.toString().getBytes(\"ISO-8859-1\");\n        } catch (UnsupportedEncodingException e) {\n            throw new RuntimeException(\"Base 64 decode produced byte values > 255\"); // TODO: change this exception type\n        }\n    }",
    "reference": "Decode the String from Base64 into a byte array.\n\n@param value the string to be decoded\n@return the decoded bytes as an array\n@since 1.0",
    "generated": "Decode a base64 encoded string.\n\n@param value Base64 encoded string\n@return base64 encoded byte values"
  },
  {
    "code": "protected void runStatements(Reader reader, PrintStream out)\n            throws IOException {\n        log.debug(\"runStatements()\");\n        StringBuilder txt = new StringBuilder();\n        String line = \"\";\n        BufferedReader in = new BufferedReader(reader);\n\n        while ((line = in.readLine()) != null) {\n            line = getProject().replaceProperties(line);\n            if (line.indexOf(\"--\") >= 0) {\n                txt.append(\"\\n\");\n            }\n        }\n        // Catch any statements not followed by ;\n        if (!txt.toString().equals(\"\")) {\n            execGroovy(txt.toString(), out);\n        }\n    }",
    "reference": "Read in lines and execute them.\n\n@param reader the reader from which to get the groovy source to exec\n@param out    the outputstream to use\n@throws java.io.IOException if something goes wrong",
    "generated": "Run the statements.\n\n@param reader the reader\n@param out the output stream\n@return the output stream"
  },
  {
    "code": "public static String toJson(Date date) {\n        if (date == null) {\n            return NULL_VALUE;\n        }\n\n        CharBuf buffer = CharBuf.create(26);\n        writeDate(date, buffer);\n\n        return buffer.toString();\n    }",
    "reference": "Format a date that is parseable from JavaScript, according to ISO-8601.\n\n@param date the date to format to a JSON string\n@return a formatted date in the form of a string",
    "generated": "Convert a date to a JSON string.\n\n@param date Date\n@return a JSON string"
  },
  {
    "code": "public static String toJson(Calendar cal) {\n        if (cal == null) {\n            return NULL_VALUE;\n        }\n\n        CharBuf buffer = CharBuf.create(26);\n        writeDate(cal.getTime(), buffer);\n\n        return buffer.toString();\n    }",
    "reference": "Format a calendar instance that is parseable from JavaScript, according to ISO-8601.\n\n@param cal the calendar to format to a JSON string\n@return a formatted date in the form of a string",
    "generated": "Generate a JSON representation of a calendar.\n\n@param cal Calendar object\n@return JSON representation of the calendar."
  },
  {
    "code": "private static void writeNumber(Class<?> numberClass, Number value, CharBuf buffer) {\n        if (numberClass == Integer.class) {\n            buffer.addInt((Integer) value);\n        } else if (numberClass == Long.class) {\n            buffer.addLong((Long) value);\n        } else if (numberClass == BigInteger.class) {\n            buffer.addBigInteger((BigInteger) value);\n        } else if (numberClass == BigDecimal.class) {\n            buffer.addBigDecimal((BigDecimal) value);\n        } else if (numberClass == Double.class) {\n            Double doubleValue = (Double) value;\n            if (doubleValue.isInfinite()) {\n                throw new JsonException(\"Number \" + value + \" can't be serialized as JSON: infinite are not allowed in JSON.\");\n            }\n            if (doubleValue.isNaN()) {\n                throw new JsonException(\"Number \" + value + \" can't be serialized as JSON: NaN are not allowed in JSON.\");\n            }\n\n            buffer.addDouble(doubleValue);\n        } else if (numberClass == Float.class) {\n            Float floatValue = (Float) value;\n            if (floatValue.isInfinite()) {\n                throw new JsonException(\"Number \" + value + \" can't be serialized as JSON: infinite are not allowed in JSON.\");\n            }\n            if (floatValue.isNaN()) {\n                throw new JsonException(\"Number \" + value + \" can't be serialized as JSON: NaN are not allowed in JSON.\");\n            }\n\n            buffer.addFloat(floatValue);\n        } else if (numberClass == Byte.class) {\n            buffer.addByte((Byte) value);\n        } else if (numberClass == Short.class) {\n            buffer.addShort((Short) value);\n        } else { // Handle other Number implementations\n            buffer.addString(value.toString());\n        }\n    }",
    "reference": "Serializes Number value and writes it into specified buffer.",
    "generated": "Write a Number to the CharBuf.\n\n@param value Number to write\n@param buffer the CharBuf\n@param buffer the CharBuf"
  },
  {
    "code": "private static void writeCharSequence(CharSequence seq, CharBuf buffer) {\n        if (seq.length() > 0) {\n            buffer.addJsonEscapedString(seq.toString());\n        } else {\n            buffer.addChars(EMPTY_STRING_CHARS);\n        }\n    }",
    "reference": "Serializes any char sequence and writes it into specified buffer.",
    "generated": "Write a CharSequence to the given buffer.\n\n@param seq CharBuf buffer"
  },
  {
    "code": "public static Enum castToEnum(Object object, Class<? extends Enum> type) {\n        if (object==null) return null;\n        if (type.isInstance(object)) return (Enum) object;\n        if (object instanceof String || object instanceof GString) {\n            return Enum.valueOf(type, object.toString());\n        }\n        throw new GroovyCastException(object, type);\n    }",
    "reference": "this class requires that the supplied enum is not fitting a\nCollection case for casting",
    "generated": "Convert an object to an enum.\n\n@param object the object\n@param type the type to convert\n@throws GroovyCastException"
  },
  {
    "code": "public void setTargetBytecode(String version) {\n        if (CompilerConfiguration.PRE_JDK5.equals(version) || CompilerConfiguration.POST_JDK5.equals(version)) {\n            this.targetBytecode = version;\n        }\n    }",
    "reference": "Sets the bytecode compatibility mode\n\n@param version the bytecode compatibility mode",
    "generated": "Set the target bytecode.\n\n@param version the version of the target\n@return the target bytecode"
  },
  {
    "code": "public List depthFirst() {\n        List answer = new NodeList();\n        answer.add(this);\n        answer.addAll(depthFirstRest());\n        return answer;\n    }",
    "reference": "Provides a collection of all the nodes in the tree\nusing a depth first traversal.\n\n@return the list of (depth-first) ordered nodes",
    "generated": "Generate a depth first node.\n\n@param depthFirst - depth first node\n@return the depth first node"
  },
  {
    "code": "public static String getGetterName(String propertyName, Class type) {\n        String prefix = type == boolean.class || type == Boolean.class ? \"is\" : \"get\";\n        return prefix + MetaClassHelper.capitalize(propertyName);\n    }",
    "reference": "Gets the name for the getter for this property\n\n@return The name of the property. The name is \"get\"+ the capitalized propertyName\nor, in the case of boolean values, \"is\" + the capitalized propertyName",
    "generated": "Get the property name.\n\n@param propertyName the property name\n@param type the class to be used"
  },
  {
    "code": "public static ProxyMetaClass getInstance(Class theClass) throws IntrospectionException {\n        MetaClassRegistry metaRegistry = GroovySystem.getMetaClassRegistry();\n        MetaClass meta = metaRegistry.getMetaClass(theClass);\n        return new ProxyMetaClass(metaRegistry, theClass, meta);\n    }",
    "reference": "convenience factory method for the most usual case.",
    "generated": "Create a new proxy class.\n\n@param theClass the class\n@return the proxy class"
  },
  {
    "code": "private Expression correctClassClassChain(PropertyExpression pe) {\n        LinkedList<Expression> stack = new LinkedList<Expression>();\n        ClassExpression found = null;\n        for (Expression it = pe; it != null; it = ((PropertyExpression) it).getObjectExpression()) {\n            if (it instanceof ClassExpression) {\n                found = (ClassExpression) it;\n                break;\n            } else if (!(it.getClass() == PropertyExpression.class)) {\n                return pe;\n            }\n            stack.addFirst(it);\n        }\n        if (found == null) return pe;\n\n        if (stack.isEmpty()) return pe;\n        Object stackElement = stack.removeFirst();\n        if (!(stackElement.getClass() == PropertyExpression.class)) return pe;\n        PropertyExpression classPropertyExpression = (PropertyExpression) stackElement;\n        String propertyNamePart = classPropertyExpression.getPropertyAsString();\n        if (propertyNamePart == null || !propertyNamePart.equals(\"class\")) return pe;\n\n        found.setSourcePosition(classPropertyExpression);\n        if (stack.isEmpty()) return found;\n        stackElement = stack.removeFirst();\n        if (!(stackElement.getClass() == PropertyExpression.class)) return pe;\n        PropertyExpression classPropertyExpressionContainer = (PropertyExpression) stackElement;\n\n        classPropertyExpressionContainer.setObjectExpression(found);\n        return pe;\n    }",
    "reference": "and class as property",
    "generated": "Correct the class chain.\n\n@param pe Class expression"
  },
  {
    "code": "public static void closeWithWarning(Closeable c) {\n        if (c != null) {\n            try {\n                c.close();\n            } catch (IOException e) {\n                LOG.warning(\"Caught exception during close(): \" + e);\n            }\n        }\n    }",
    "reference": "Close the Closeable. Logging a warning if any problems occur.\n\n@param c the thing to close",
    "generated": "Close a closeable.\n\n@param c closeable"
  },
  {
    "code": "public V get(K key) {\n        ManagedReference<V> ref = internalMap.get(key);\n        if (ref!=null) return ref.get();\n        return null;\n    }",
    "reference": "Returns the value stored for the given key at the point of call.\n@param key a non null key\n@return the value stored in the map for the given key",
    "generated": "Get the value of the key.\n\n@param key key\n@return the value of the key\n@return the value"
  },
  {
    "code": "public void put(final K key, V value) {\n        ManagedReference<V> ref = new ManagedReference<V>(bundle, value) {\n            @Override\n            public void finalizeReference() {\n                super.finalizeReference();\n                internalMap.remove(key, get());\n            }\n        };\n        internalMap.put(key, ref);\n    }",
    "reference": "Sets a new value for a given key. an older value is overwritten.\n@param key a non null key\n@param value the new value",
    "generated": "Write a reference to a key.\n\n@param key key\n@param value value"
  },
  {
    "code": "public String getSample(int line, int column, Janitor janitor) {\n        String sample = null;\n        String text = source.getLine(line, janitor);\n\n        if (text != null) {\n            if (column > 0) {\n                String marker = Utilities.repeatString(\" \", column - 1) + \"^\";\n\n                if (column > 40) {\n                    int start = column - 30 - 1;\n                    int end = (column + 10 > text.length() ? text.length() : column + 10 - 1);\n                    sample = \"   \" + text.substring(start, end) + Utilities.eol() + \"   \" +\n                            marker.substring(start, marker.length());\n                } else {\n                    sample = \"   \" + text + Utilities.eol() + \"   \" + marker;\n                }\n            } else {\n                sample = text;\n            }\n        }\n\n        return sample;\n    }",
    "reference": "Returns a sampling of the source at the specified line and column,\nof null if it is unavailable.",
    "generated": "Get the sample of a line.\n\n@param line line index\n@param column index of the column\n@param janitor janitor"
  },
  {
    "code": "public static void withInstance(String url, Closure c) throws SQLException {\n        Sql sql = null;\n        try {\n            sql = newInstance(url);\n            c.call(sql);\n        } finally {\n            if (sql != null) sql.close();\n        }\n    }",
    "reference": "Invokes a closure passing it a new Sql instance created from the given JDBC connection URL.\nThe created connection will be closed if required.\n\n@param url a database url of the form\n<code>jdbc:<em>subprotocol</em>:<em>subname</em></code>\n@param c the Closure to call\n@see #newInstance(String)\n@throws SQLException if a database access error occurs",
    "generated": "Create a new instance.\n\n@param url the URL of the instance\n@param c the closure to call\n@return"
  },
  {
    "code": "public synchronized void withTransaction(Closure closure) throws SQLException {\n        boolean savedCacheConnection = cacheConnection;\n        cacheConnection = true;\n        Connection connection = null;\n        boolean savedAutoCommit = true;\n        try {\n            connection = createConnection();\n            savedAutoCommit = connection.getAutoCommit();\n            connection.setAutoCommit(false);\n            callClosurePossiblyWithConnection(closure, connection);\n            connection.commit();\n        } catch (SQLException e) {\n            handleError(connection, e);\n            throw e;\n        } catch (RuntimeException e) {\n            handleError(connection, e);\n            throw e;\n        } catch (Error e) {\n            handleError(connection, e);\n            throw e;\n        } catch (Exception e) {\n            handleError(connection, e);\n            throw new SQLException(\"Unexpected exception during transaction\", e);\n        } finally {\n            if (connection != null) {\n                try {\n                    connection.setAutoCommit(savedAutoCommit);\n                }\n                catch (SQLException e) {\n                    LOG.finest(\"Caught exception resetting auto commit: \" + e.getMessage() + \" - continuing\");\n                }\n            }\n            cacheConnection = false;\n            closeResources(connection, null);\n            cacheConnection = savedCacheConnection;\n            if (dataSource != null && !cacheConnection) {\n                useConnection = null;\n            }\n        }\n    }",
    "reference": "Performs the closure within a transaction using a cached connection.\nIf the closure takes a single argument, it will be called\nwith the connection, otherwise it will be called with no arguments.\n\n@param closure the given closure\n@throws SQLException if a database error occurs",
    "generated": "Call the closure with the transaction.\n\n@param closure closure to call"
  },
  {
    "code": "public static Object invoke(Object object, String methodName, Object[] parameters) {\n        try {\n            Class[] classTypes = new Class[parameters.length];\n            for (int i = 0; i < classTypes.length; i++) {\n                classTypes[i] = parameters[i].getClass();\n            }\n            Method method = object.getClass().getMethod(methodName, classTypes);\n            return method.invoke(object, parameters);\n        } catch (Throwable t) {\n            return InvokerHelper.invokeMethod(object, methodName,  parameters);\n        }\n    }",
    "reference": "Invoke a method through reflection.\nFalls through to using the Invoker to call the method in case the reflection call fails..\n\n@param object the object on which to invoke a method\n@param methodName the name of the method to invoke\n@param parameters the parameters of the method call\n@return the result of the method call",
    "generated": "Invoke a method with the given parameters.\n\n@param object the object\n@param methodName the method name\n@param parameters the parameters\n@param parameters the parameters\n@return the object"
  },
  {
    "code": "public static String[] tokenizeUnquoted(String s) {\r\n        List tokens = new LinkedList();\r\n        int first = 0;\r\n        while (first < s.length()) {\r\n            first = skipWhitespace(s, first);\r\n            int last = scanToken(s, first);\r\n            if (first < last) {\r\n                tokens.add(s.substring(first, last));\r\n            }\r\n            first = last;\r\n        }\r\n        return (String[])tokens.toArray(new String[tokens.size()]);\r\n    }",
    "reference": "This method tokenizes a string by space characters,\nbut ignores spaces in quoted parts,that are parts in\n'' or \"\". The method does allows the usage of \"\" in ''\nand '' in \"\". The space character between tokens is not\nreturned.\n\n@param s the string to tokenize\n@return the tokens",
    "generated": "Parse a string from a quoted string."
  },
  {
    "code": "@SuppressWarnings(\"unchecked\")\n    private void addPrivateFieldsAccessors(ClassNode node) {\n        Set<ASTNode> accessedFields = (Set<ASTNode>) node.getNodeMetaData(StaticTypesMarker.PV_FIELDS_ACCESS);\n        if (accessedFields==null) return;\n        Map<String, MethodNode> privateConstantAccessors = (Map<String, MethodNode>) node.getNodeMetaData(PRIVATE_FIELDS_ACCESSORS);\n        if (privateConstantAccessors!=null) {\n            // already added\n            return;\n        }\n        int acc = -1;\n        privateConstantAccessors = new HashMap<String, MethodNode>();\n        final int access = Opcodes.ACC_STATIC | Opcodes.ACC_PUBLIC | Opcodes.ACC_SYNTHETIC;\n        for (FieldNode fieldNode : node.getFields()) {\n            if (accessedFields.contains(fieldNode)) {\n\n                acc++;\n                Parameter param = new Parameter(node.getPlainNodeReference(), \"$that\");\n                Expression receiver = fieldNode.isStatic()?new ClassExpression(node):new VariableExpression(param);\n                Statement stmt = new ExpressionStatement(new PropertyExpression(\n                        receiver,\n                        fieldNode.getName()\n                ));\n                MethodNode accessor = node.addMethod(\"pfaccess$\"+acc, access, fieldNode.getOriginType(), new Parameter[]{param}, ClassNode.EMPTY_ARRAY, stmt);\n                privateConstantAccessors.put(fieldNode.getName(), accessor);\n            }\n        }\n        node.setNodeMetaData(PRIVATE_FIELDS_ACCESSORS, privateConstantAccessors);\n    }",
    "reference": "Adds special accessors for private constants so that inner classes can retrieve them.",
    "generated": "Add the private fields accessors to the given class node.\n\n@param node the class node\n@param node the class node\n@return the class node"
  },
  {
    "code": "public static boolean isPostJDK5(String bytecodeVersion) {\n        return JDK5.equals(bytecodeVersion)\n            || JDK6.equals(bytecodeVersion)\n            || JDK7.equals(bytecodeVersion)\n            || JDK8.equals(bytecodeVersion);\n    }",
    "reference": "Checks if the specified bytecode version string represents a JDK 1.5+ compatible\nbytecode version.\n@param bytecodeVersion the bytecode version string (1.4, 1.5, 1.6, 1.7 or 1.8)\n@return true if the bytecode version is JDK 1.5+",
    "generated": "Returns true if the given bytecode version is POST JDK5."
  },
  {
    "code": "public void setTargetDirectory(String directory) {\n        if (directory != null && directory.length() > 0) {\n            this.targetDirectory = new File(directory);\n        } else {\n            this.targetDirectory = null;\n        }\n    }",
    "reference": "Sets the target directory.",
    "generated": "Set the target directory.\n\n@param directory the directory to be used"
  },
  {
    "code": "protected synchronized Class loadClass(final String name, boolean resolve) throws ClassNotFoundException {\n        Class c = this.findLoadedClass(name);\n        if (c != null) return c;\n        c = (Class) customClasses.get(name);\n        if (c != null) return c;\n\n        try {\n            c = oldFindClass(name);\n        } catch (ClassNotFoundException cnfe) {\n            // IGNORE\n        }\n        if (c == null) c = super.loadClass(name, resolve);\n\n        if (resolve) resolveClass(c);\n\n        return c;\n    }",
    "reference": "loads a class using the name of the class",
    "generated": "Load a class.\n\n@param name the class name\n@param resolve boolean"
  },
  {
    "code": "public NodeList getAt(String name) {\n        NodeList answer = new NodeList();\n        for (Object child : this) {\n            if (child instanceof Node) {\n                Node childNode = (Node) child;\n                Object temp = childNode.get(name);\n                if (temp instanceof Collection) {\n                    answer.addAll((Collection) temp);\n                } else {\n                    answer.add(temp);\n                }\n            }\n        }\n        return answer;\n    }",
    "reference": "Provides lookup of elements by non-namespaced name.\n\n@param name the name or shortcut key for nodes of interest\n@return the nodes of interest which match name",
    "generated": "Returns the list of nodes at the given name.\n\n@param name the name\n@return the list of nodes"
  },
  {
    "code": "public String text() {\n        String previousText = null;\n        StringBuilder buffer = null;\n        for (Object child : this) {\n            String text = null;\n            if (child instanceof String) {\n                text = (String) child;\n            } else if (child instanceof Node) {\n                text = ((Node) child).text();\n            }\n            if (text != null) {\n                if (previousText == null) {\n                    previousText = text;\n                } else {\n                    if (buffer == null) {\n                        buffer = new StringBuilder();\n                        buffer.append(previousText);\n                    }\n                    buffer.append(text);\n                }\n            }\n        }\n        if (buffer != null) {\n            return buffer.toString();\n        }\n        if (previousText != null) {\n            return previousText;\n        }\n        return \"\";\n    }",
    "reference": "Returns the text value of all of the elements in the collection.\n\n@return the text value of all the elements in the collection or null",
    "generated": "Generate a documentation string for this object.\n\n@param child object\n@return text"
  },
  {
    "code": "private boolean isAllNumeric(TokenStream stream) {\n    List<Token> tokens = ((NattyTokenSource) stream.getTokenSource()).getTokens();\n    for(Token token:tokens) {\n      try {\n        Integer.parseInt(token.getText());\n      } catch(NumberFormatException e) {\n        return false;\n      }\n    }\n    return true;\n  }",
    "reference": "Determines if a token stream contains only numeric tokens\n@param stream\n@return true if all tokens in the given stream can be parsed as an integer",
    "generated": "Check if the stream is all numeric.\n\n@param stream stream token stream\n@return true if the stream is all numeric"
  },
  {
    "code": "private List<TokenStream> collectTokenStreams(TokenStream stream) {\n    \n    // walk through the token stream and build a collection \n    // of sub token streams that represent possible date locations\n    List<Token> currentGroup = null;\n    List<List<Token>> groups = new ArrayList<List<Token>>();\n    Token currentToken;\n    int currentTokenType;\n    StringBuilder tokenString = new StringBuilder();\n    while((currentToken = stream.getTokenSource().nextToken()).getType() != DateLexer.EOF) {\n      currentTokenType = currentToken.getType();\n      tokenString.append(DateParser.tokenNames[currentTokenType]).append(\" \");\n\n      // we're currently NOT collecting for a possible date group\n      if(currentGroup == null) {\n        // skip over white space and known tokens that cannot be the start of a date\n        if(currentTokenType != DateLexer.WHITE_SPACE &&\n            DateParser.FOLLOW_empty_in_parse186.member(currentTokenType)) {\n\n          currentGroup = new ArrayList<Token>();\n          currentGroup.add(currentToken);\n        }\n      }\n\n      // we're currently collecting\n      else {\n        // preserve white space\n        if(currentTokenType == DateLexer.WHITE_SPACE) {\n          currentGroup.add(currentToken);\n        }\n\n        else {\n          // if this is an unknown token, we'll close out the current group\n          if(currentTokenType == DateLexer.UNKNOWN) {\n            addGroup(currentGroup, groups);\n            currentGroup = null;\n          }\n          // otherwise, the token is known and we're currently collecting for\n          // a group, so we'll add it to the current group\n          else {\n            currentGroup.add(currentToken);\n          }\n        }\n      }\n    }\n\n    if(currentGroup != null) {\n      addGroup(currentGroup, groups);\n    }\n    \n    _logger.info(\"STREAM: \" + tokenString.toString());\n    List<TokenStream> streams = new ArrayList<TokenStream>();\n    for(List<Token> group:groups) {\n      if(!group.isEmpty()) {\n        StringBuilder builder = new StringBuilder();\n        builder.append(\"GROUP: \");\n        for (Token token : group) {\n          builder.append(DateParser.tokenNames[token.getType()]).append(\" \");\n        }\n        _logger.info(builder.toString());\n\n        streams.add(new CommonTokenStream(new NattyTokenSource(group)));\n      }\n    }\n\n    return streams;\n  }",
    "reference": "Scans the given token global token stream for a list of sub-token\nstreams representing those portions of the global stream that\nmay contain date time information\n\n@param stream\n@return",
    "generated": "Walk through the token stream and build a collection \n    of sub token streams that represent possible date locations\n    List<Token>"
  },
  {
    "code": "private void addGroup(List<Token> group, List<List<Token>> groups) {\n\n    if(group.isEmpty()) return;\n\n    // remove trailing tokens that should be ignored\n    while(!group.isEmpty() && IGNORED_TRAILING_TOKENS.contains(\n        group.get(group.size() - 1).getType())) {\n      group.remove(group.size() - 1);\n    }\n\n    // if the group still has some tokens left, we'll add it to our list of groups\n    if(!group.isEmpty()) {\n      groups.add(group);\n    }\n  }",
    "reference": "Cleans up the given group and adds it to the list of groups if still valid\n@param group\n@param groups",
    "generated": "Add a group to our list of groups\n   "
  },
  {
    "code": "public void seekToDayOfWeek(String direction, String seekType, String seekAmount, String dayOfWeek) {\n    int dayOfWeekInt = Integer.parseInt(dayOfWeek);\n    int seekAmountInt = Integer.parseInt(seekAmount);\n    assert(direction.equals(DIR_LEFT) || direction.equals(DIR_RIGHT));\n    assert(seekType.equals(SEEK_BY_DAY) || seekType.equals(SEEK_BY_WEEK));\n    assert(dayOfWeekInt >= 1 && dayOfWeekInt <= 7);\n    \n    markDateInvocation();\n    \n    int sign = direction.equals(DIR_RIGHT) ? 1 : -1;\n    if(seekType.equals(SEEK_BY_WEEK)) {\n      // set our calendar to this weeks requested day of the week,\n      // then add or subtract the week(s)\n      _calendar.set(Calendar.DAY_OF_WEEK, dayOfWeekInt);\n      _calendar.add(Calendar.DAY_OF_YEAR, seekAmountInt * 7 * sign);\n    }\n    \n    else if(seekType.equals(SEEK_BY_DAY)) {\n      // find the closest day\n      do {\n        _calendar.add(Calendar.DAY_OF_YEAR, sign);\n      } while(_calendar.get(Calendar.DAY_OF_WEEK) != dayOfWeekInt);\n      \n      // now add/subtract any additional days\n      if(seekAmountInt > 0) {\n        _calendar.add(Calendar.WEEK_OF_YEAR, (seekAmountInt - 1) * sign);\n      }\n    }\n  }",
    "reference": "seeks to a specified day of the week in the past or future.\n\n@param direction the direction to seek: two possibilities\n'<' go backward\n'>' go forward\n\n@param seekType the type of seek to perform (by_day or by_week)\nby_day means we seek to the very next occurrence of the given day\nby_week means we seek to the first occurrence of the given day week in the\nnext (or previous,) week (or multiple of next or previous week depending\non the seek amount.)\n\n@param seekAmount the amount to seek.  Must be guaranteed to parse as an integer\n\n@param dayOfWeek the day of the week to seek to, represented as an integer from\n1 to 7 (1 being Sunday, 7 being Saturday.) Must be guaranteed to parse as an Integer",
    "generated": "Set our calendar to this weeks requested day of the week,"
  },
  {
    "code": "public void seekToDayOfMonth(String dayOfMonth) {\n    int dayOfMonthInt = Integer.parseInt(dayOfMonth);\n    assert(dayOfMonthInt >= 1 && dayOfMonthInt <= 31);\n    \n    markDateInvocation();\n    \n    dayOfMonthInt = Math.min(dayOfMonthInt, _calendar.getActualMaximum(Calendar.DAY_OF_MONTH));\n    _calendar.set(Calendar.DAY_OF_MONTH, dayOfMonthInt);\n  }",
    "reference": "Seeks to the given day within the current month\n@param dayOfMonth the day of the month to seek to, represented as an integer\nfrom 1 to 31. Must be guaranteed to parse as an Integer.  If this day is\nbeyond the last day of the current month, the actual last day of the month\nwill be used.",
    "generated": "Seek to the day of the month."
  },
  {
    "code": "public void seekToDayOfYear(String dayOfYear) {\n    int dayOfYearInt = Integer.parseInt(dayOfYear);\n    assert(dayOfYearInt >= 1 && dayOfYearInt <= 366);\n    \n    markDateInvocation();\n    \n    dayOfYearInt = Math.min(dayOfYearInt, _calendar.getActualMaximum(Calendar.DAY_OF_YEAR));\n    _calendar.set(Calendar.DAY_OF_YEAR, dayOfYearInt);\n  }",
    "reference": "Seeks to the given day within the current year\n@param dayOfYear the day of the year to seek to, represented as an integer\nfrom 1 to 366. Must be guaranteed to parse as an Integer.  If this day is\nbeyond the last day of the current year, the actual last day of the year\nwill be used.",
    "generated": "Seek to the given dayOfYear."
  },
  {
    "code": "public void seekToMonth(String direction, String seekAmount, String month) {\n    int seekAmountInt = Integer.parseInt(seekAmount);\n    int monthInt = Integer.parseInt(month);\n    assert(direction.equals(DIR_LEFT) || direction.equals(DIR_RIGHT));\n    assert(monthInt >= 1 && monthInt <= 12);\n    \n    markDateInvocation();\n    \n    // set the day to the first of month. This step is necessary because if we seek to the \n    // current day of a month whose number of days is less than the current day, we will \n    // pushed into the next month.\n    _calendar.set(Calendar.DAY_OF_MONTH, 1);\n    \n    // seek to the appropriate year\n    if(seekAmountInt > 0) {\n      int currentMonth = _calendar.get(Calendar.MONTH) + 1;\n      int sign = direction.equals(DIR_RIGHT) ? 1 : -1;\n      int numYearsToShift = seekAmountInt +\n              (currentMonth == monthInt ? 0 : (currentMonth < monthInt ? sign > 0 ? -1 : 0 : sign > 0 ? 0 : -1));\n\n      _calendar.add(Calendar.YEAR, (numYearsToShift * sign));\n    }\n    \n    // now set the month\n    _calendar.set(Calendar.MONTH, monthInt - 1);\n  }",
    "reference": "seeks to a particular month\n\n@param direction the direction to seek: two possibilities\n'<' go backward\n'>' go forward\n\n@param seekAmount the amount to seek.  Must be guaranteed to parse as an integer\n\n@param month the month to seek to.  Must be guaranteed to parse as an integer\nbetween 1 and 12",
    "generated": "Seek to the specified month. This step is necessary because if we seek to the \n    current day of a month whose number of days is less than the current day, we will seek to the next month."
  },
  {
    "code": "public void setExplicitTime(String hours, String minutes, String seconds, String amPm, String zoneString) {\n    int hoursInt = Integer.parseInt(hours);\n    int minutesInt = minutes != null ? Integer.parseInt(minutes) : 0;\n    assert(amPm == null || amPm.equals(AM) || amPm.equals(PM));\n    assert(hoursInt >= 0);\n    assert(minutesInt >= 0 && minutesInt < 60); \n    \n    markTimeInvocation(amPm);\n    \n    // reset milliseconds to 0\n    _calendar.set(Calendar.MILLISECOND, 0);\n    \n    // if no explicit zone is given, we use our own\n    TimeZone zone = null;\n    if(zoneString != null) {\n      if(zoneString.startsWith(PLUS) || zoneString.startsWith(MINUS)) {\n        zoneString = GMT + zoneString;\n      }\n      zone = TimeZone.getTimeZone(zoneString);\n    }\n    \n    _calendar.setTimeZone(zone != null ? zone : _defaultTimeZone);\n    \n    _calendar.set(Calendar.HOUR_OF_DAY, hoursInt);\n    // hours greater than 12 are in 24-hour time\n    if(hoursInt <= 12) {\n      int amPmInt = amPm == null ? \n        (hoursInt >= 12 ? Calendar.PM : Calendar.AM) :\n        amPm.equals(PM) ? Calendar.PM : Calendar.AM;\n      _calendar.set(Calendar.AM_PM, amPmInt);\n      \n      // calendar is whacky at 12 o'clock (must use 0)\n      if(hoursInt == 12) hoursInt = 0;\n      _calendar.set(Calendar.HOUR, hoursInt);\n    }\n    \n    if(seconds != null) {\n      int secondsInt = Integer.parseInt(seconds);\n      assert(secondsInt >= 0 && secondsInt < 60); \n      _calendar.set(Calendar.SECOND, secondsInt);\n    }\n    else {\n      _calendar.set(Calendar.SECOND, 0);\n    }\n    \n    _calendar.set(Calendar.MINUTE, minutesInt);\n  }",
    "reference": "Sets the the time of day\n\n@param hours the hours to set.  Must be guaranteed to parse as an\ninteger between 0 and 23\n\n@param minutes the minutes to set.  Must be guaranteed to parse as\nan integer between 0 and 59\n\n@param seconds the optional seconds to set.  Must be guaranteed to parse as\nan integer between 0 and 59\n\n@param amPm the meridian indicator to use.  Must be either 'am' or 'pm'\n\n@param zoneString the time zone to use in one of two formats:\n- zoneinfo format (America/New_York, America/Los_Angeles, etc)\n- GMT offset (+05:00, -0500, +5, etc)",
    "generated": "Set the explicit time.\n\n@param hours minutes, minutes, amPm, zoneString"
  },
  {
    "code": "public void seekToHoliday(String holidayString, String direction, String seekAmount) {\n    Holiday holiday = Holiday.valueOf(holidayString);\n    assert(holiday != null);\n    \n    seekToIcsEvent(HOLIDAY_ICS_FILE, holiday.getSummary(), direction, seekAmount);\n  }",
    "reference": "Seeks forward or backwards to a particular holiday based on the current date\n\n@param holidayString The holiday to seek to\n@param direction     The direction to seek\n@param seekAmount    The number of years to seek",
    "generated": "Seek to Holiday."
  },
  {
    "code": "public void seekToHolidayYear(String holidayString, String yearString) {\n    Holiday holiday = Holiday.valueOf(holidayString);\n    assert(holiday != null);\n    \n    seekToIcsEventYear(HOLIDAY_ICS_FILE, yearString, holiday.getSummary());\n  }",
    "reference": "Seeks to the given holiday within the given year\n\n@param holidayString\n@param yearString",
    "generated": "Seek to Holiday year."
  },
  {
    "code": "public void seekToSeason(String seasonString, String direction, String seekAmount) {\n    Season season = Season.valueOf(seasonString);\n    assert(season!= null);\n    \n    seekToIcsEvent(SEASON_ICS_FILE, season.getSummary(), direction, seekAmount);\n  }",
    "reference": "Seeks forward or backwards to a particular season based on the current date\n\n@param seasonString The season to seek to\n@param direction     The direction to seek\n@param seekAmount    The number of years to seek",
    "generated": "Seek to a season."
  },
  {
    "code": "public void seekToSeasonYear(String seasonString, String yearString) {\n    Season season = Season.valueOf(seasonString);\n    assert(season != null);\n    \n    seekToIcsEventYear(SEASON_ICS_FILE, yearString, season.getSummary());\n  }",
    "reference": "Seeks to the given season within the given year\n\n@param seasonString\n@param yearString",
    "generated": "Seek to the season year."
  },
  {
    "code": "private void resetCalendar() {\n    _calendar = getCalendar();\n    if (_defaultTimeZone != null) {\n      _calendar.setTimeZone(_defaultTimeZone);\n    }\n    _currentYear = _calendar.get(Calendar.YEAR);\n  }",
    "reference": "Resets the calendar",
    "generated": "Reset the calendar."
  },
  {
    "code": "private Date seasonalDateFromIcs(String icsFileName, String eventSummary, int year) {\n    Map<Integer, Date> dates = getDatesFromIcs(icsFileName, eventSummary, year, year);\n    return dates.get(year - (eventSummary.equals(Holiday.NEW_YEARS_EVE.getSummary()) ? 1 : 0));\n  }",
    "reference": "Finds and returns the date for the given event summary and year within the given ics file,\nor null if not present.",
    "generated": "Generate a seasonal date from Ics file.\n\n@param icsFileName Ics file\n@param eventSummary Ics file\n@param year year"
  },
  {
    "code": "private void markDateInvocation() {\n\n    _updatePreviousDates = !_dateGivenInGroup;\n    _dateGivenInGroup = true;\n    _dateGroup.setDateInferred(false);\n\n    if(_firstDateInvocationInGroup) {\n      // if a time has been given within the current date group, \n      // we capture the current time before resetting the calendar\n      if(_timeGivenInGroup) {\n        int hours = _calendar.get(Calendar.HOUR_OF_DAY);\n        int minutes = _calendar.get(Calendar.MINUTE);\n        int seconds = _calendar.get(Calendar.SECOND);\n        resetCalendar();\n        _calendar.set(Calendar.HOUR_OF_DAY, hours);\n        _calendar.set(Calendar.MINUTE, minutes);\n        _calendar.set(Calendar.SECOND, seconds);\n      }\n      else {\n        resetCalendar();\n      }\n      _firstDateInvocationInGroup = false;\n    }\n  }",
    "reference": "ensures that the first invocation of a date seeking\nrule is captured",
    "generated": "If a date has been given within the current date group, then reset the calendar"
  },
  {
    "code": "private Calendar cleanHistCalendar(Calendar cal) {\n        cal.set(Calendar.MILLISECOND, 0);\n        cal.set(Calendar.SECOND, 0);\n        cal.set(Calendar.MINUTE, 0);\n        cal.set(Calendar.HOUR, 0);\n        return cal;\n    }",
    "reference": "Put everything smaller than days at 0\n@param cal calendar to be cleaned",
    "generated": "Clean the HistCalendar.\n\n@param cal the calendar to be cleaned."
  },
  {
    "code": "public static Calendar parseDividendDate(String date) {\n        if (!Utils.isParseable(date)) {\n            return null;\n        }\n        date = date.trim();\n        SimpleDateFormat format = new SimpleDateFormat(Utils.getDividendDateFormat(date), Locale.US);\n        format.setTimeZone(TimeZone.getTimeZone(YahooFinance.TIMEZONE));\n        try {\n            Calendar today = Calendar.getInstance(TimeZone.getTimeZone(YahooFinance.TIMEZONE));\n            Calendar parsedDate = Calendar.getInstance(TimeZone.getTimeZone(YahooFinance.TIMEZONE));\n            parsedDate.setTime(format.parse(date));\n\n            if (parsedDate.get(Calendar.YEAR) == 1970) {\n                // Not really clear which year the dividend date is... making a reasonable guess.\n                int monthDiff = parsedDate.get(Calendar.MONTH) - today.get(Calendar.MONTH);\n                int year = today.get(Calendar.YEAR);\n                if (monthDiff > 6) {\n                    year -= 1;\n                } else if (monthDiff < -6) {\n                    year += 1;\n                }\n                parsedDate.set(Calendar.YEAR, year);\n            }\n\n            return parsedDate;\n        } catch (ParseException ex) {\n            log.warn(\"Failed to parse dividend date: \" + date);\n            log.debug(\"Failed to parse dividend date: \" + date, ex);\n            return null;\n        }\n    }",
    "reference": "Used to parse the dividend dates. Returns null if the date cannot be\nparsed.\n\n@param date String received that represents the date\n@return Calendar object representing the parsed date",
    "generated": "Parse a dividend date.\n\n@param date the date to parse\n@return Calendar object"
  },
  {
    "code": "public static TimeZone get(String suffix) {\n        if(SUFFIX_TIMEZONES.containsKey(suffix)) {\n            return SUFFIX_TIMEZONES.get(suffix);\n        }\n        log.warn(\"Cannot find time zone for exchange suffix: '{}'. Using default: America/New_York\", suffix);\n        return SUFFIX_TIMEZONES.get(\"\");\n    }",
    "reference": "Get the time zone for a specific exchange suffix\n\n@param suffix suffix for the exchange in YahooFinance\n@return time zone of the exchange",
    "generated": "Get the time zone for exchange suffix."
  },
  {
    "code": "public static TimeZone getStockTimeZone(String symbol) {\n        // First check if it's a known stock index\n        if(INDEX_TIMEZONES.containsKey(symbol)) {\n            return INDEX_TIMEZONES.get(symbol);\n        }\n        \n        if(!symbol.contains(\".\")) {\n            return ExchangeTimeZone.get(\"\");\n        }\n        String[] split = symbol.split(\"\\\\.\");\n        return ExchangeTimeZone.get(split[split.length - 1]);\n    }",
    "reference": "Get the time zone for a specific stock or index.\nFor stocks, the exchange suffix is extracted from the stock symbol to retrieve the time zone.\n\n@param symbol stock symbol in YahooFinance\n@return time zone of the exchange on which this stock is traded",
    "generated": "Check if it's a known stock index\n       "
  },
  {
    "code": "@Override protected Class getPrototypeClass(Video content) {\n    Class prototypeClass;\n    if (content.isFavorite()) {\n      prototypeClass = FavoriteVideoRenderer.class;\n    } else if (content.isLive()) {\n      prototypeClass = LiveVideoRenderer.class;\n    } else {\n      prototypeClass = LikeVideoRenderer.class;\n    }\n    return prototypeClass;\n  }",
    "reference": "Method to declare Video-VideoRenderer mapping.\nFavorite videos will be rendered using FavoriteVideoRenderer.\nLive videos will be rendered using LiveVideoRenderer.\nLiked videos will be rendered using LikeVideoRenderer.\n\n@param content used to map object-renderers.\n@return VideoRenderer subtype class.",
    "generated": "Get the prototype of the video.\n\n@param content Video instance"
  },
  {
    "code": "@Override public int getItemViewType(int position) {\n    T content = getItem(position);\n    return rendererBuilder.getItemViewType(content);\n  }",
    "reference": "Indicate to the RecyclerView the type of Renderer used to one position using a numeric value.\n\n@param position to analyze.\n@return the id associated to the Renderer used to render the content given a position.",
    "generated": "Get the item view type.\n\n@param position the position of the item\n@return item view type"
  },
  {
    "code": "@Override public RendererViewHolder onCreateViewHolder(ViewGroup viewGroup, int viewType) {\n    rendererBuilder.withParent(viewGroup);\n    rendererBuilder.withLayoutInflater(LayoutInflater.from(viewGroup.getContext()));\n    rendererBuilder.withViewType(viewType);\n    RendererViewHolder viewHolder = rendererBuilder.buildRendererViewHolder();\n    if (viewHolder == null) {\n      throw new NullRendererBuiltException(\"RendererBuilder have to return a not null viewHolder\");\n    }\n    return viewHolder;\n  }",
    "reference": "One of the two main methods in this class. Creates a RendererViewHolder instance with a\nRenderer inside ready to be used. The RendererBuilder to create a RendererViewHolder using the\ninformation given as parameter.\n\n@param viewGroup used to create the ViewHolder.\n@param viewType associated to the renderer.\n@return ViewHolder extension with the Renderer it has to use inside.",
    "generated": "Create a RendererViewHolder.\n\n@param viewGroup View group\n@param viewType View type"
  },
  {
    "code": "@Override public void onBindViewHolder(RendererViewHolder viewHolder, int position) {\n    T content = getItem(position);\n    Renderer<T> renderer = viewHolder.getRenderer();\n    if (renderer == null) {\n      throw new NullRendererBuiltException(\"RendererBuilder have to return a not null renderer\");\n    }\n    renderer.setContent(content);\n    updateRendererExtraValues(content, renderer, position);\n    renderer.render();\n  }",
    "reference": "Given a RendererViewHolder passed as argument and a position renders the view using the\nRenderer previously stored into the RendererViewHolder.\n\n@param viewHolder with a Renderer class inside.\n@param position to render.",
    "generated": "Bind the view holder to the given position.\n\n@param viewHolder ViewHolder"
  },
  {
    "code": "public void diffUpdate(List<T> newList) {\n    if (getCollection().size() == 0) {\n      addAll(newList);\n      notifyDataSetChanged();\n    } else {\n      DiffCallback diffCallback = new DiffCallback(collection, newList);\n      DiffUtil.DiffResult diffResult = DiffUtil.calculateDiff(diffCallback);\n      clear();\n      addAll(newList);\n      diffResult.dispatchUpdatesTo(this);\n    }\n  }",
    "reference": "Provides a ready to use diff update for our adapter based on the implementation of the\nstandard equals method from Object.\n\n@param newList to refresh our content",
    "generated": "Update a list of objects.\n\n@param newList list of objects\n@return"
  },
  {
    "code": "public RendererBuilder<T> withPrototype(Renderer<? extends T> renderer) {\n    if (renderer == null) {\n      throw new NeedsPrototypesException(\n          \"RendererBuilder can't use a null Renderer<T> instance as prototype\");\n    }\n    this.prototypes.add(renderer);\n    return this;\n  }",
    "reference": "Add a Renderer instance as prototype.\n\n@param renderer to use as prototype.\n@return the current RendererBuilder instance.",
    "generated": "Add a renderer to the prototype.\n\n@param renderer renderer to be used"
  },
  {
    "code": "public <G extends T> RendererBuilder<T> bind(Class<G> clazz, Renderer<? extends G> prototype) {\n    if (clazz == null || prototype == null) {\n      throw new IllegalArgumentException(\n          \"The binding RecyclerView binding can't be configured using null instances\");\n    }\n    prototypes.add(prototype);\n    binding.put(clazz, prototype.getClass());\n    return this;\n  }",
    "reference": "Given a class configures the binding between a class and a Renderer class.\n\n@param clazz to bind.\n@param prototype used as Renderer.\n@return the current RendererBuilder instance.",
    "generated": "Bind a renderer to the given class.\n\n@param clazz the class to bind\n@param prototype the renderer to bind\n@return this"
  },
  {
    "code": "int getItemViewType(T content) {\n    Class prototypeClass = getPrototypeClass(content);\n    validatePrototypeClass(prototypeClass);\n    return getItemViewType(prototypeClass);\n  }",
    "reference": "Return the item view type used by the adapter to implement recycle mechanism.\n\n@param content to be rendered.\n@return an integer that represents the renderer inside the adapter.",
    "generated": "Get the item view type.\n\n@param content the content\n@return the item view type"
  },
  {
    "code": "protected Renderer build() {\n    validateAttributes();\n\n    Renderer renderer;\n    if (isRecyclable(convertView, content)) {\n      renderer = recycle(convertView, content);\n    } else {\n      renderer = createRenderer(content, parent);\n    }\n    return renderer;\n  }",
    "reference": "Main method of this class related to ListView widget. This method is the responsible of\nrecycle or create a new Renderer instance with all the needed information to implement the\nrendering. This method will validate all the attributes passed in the builder constructor and\nwill check if can recycle or has to create a new Renderer instance.\n\nThis method is used with ListView because the view recycling mechanism is implemented in this\nclass. RecyclerView widget will use buildRendererViewHolder method.\n\n@return ready to use Renderer instance.",
    "generated": "Create a new renderer.\n\n@param content content content\n@return the rendered renderer"
  },
  {
    "code": "protected RendererViewHolder buildRendererViewHolder() {\n    validateAttributesToCreateANewRendererViewHolder();\n\n    Renderer renderer = getPrototypeByIndex(viewType).copy();\n    renderer.onCreate(null, layoutInflater, parent);\n    return new RendererViewHolder(renderer);\n  }",
    "reference": "Main method of this class related to RecyclerView widget. This method is the responsible of\ncreate a new Renderer instance with all the needed information to implement the rendering.\nThis method will validate all the attributes passed in the builder constructor and will create\na RendererViewHolder instance.\n\nThis method is used with RecyclerView because the view recycling mechanism is implemented out\nof this class and we only have to return new RendererViewHolder instances.\n\n@return ready to use RendererViewHolder instance.",
    "generated": "Create a new RendererViewHolder.\n\n@param viewType view type\n@param layoutInflater LayoutInflater parent"
  },
  {
    "code": "private Renderer recycle(View convertView, T content) {\n    Renderer renderer = (Renderer) convertView.getTag();\n    renderer.onRecycle(content);\n    return renderer;\n  }",
    "reference": "Recycles the Renderer getting it from the tag associated to the renderer root view. This view\nis not used with RecyclerView widget.\n\n@param convertView that contains the tag.\n@param content to be updated in the recycled renderer.\n@return a recycled renderer.",
    "generated": "Recycle the given content.\n\n@param convertView view\n@param content content"
  },
  {
    "code": "private Renderer createRenderer(T content, ViewGroup parent) {\n    int prototypeIndex = getPrototypeIndex(content);\n    Renderer renderer = getPrototypeByIndex(prototypeIndex).copy();\n    renderer.onCreate(content, layoutInflater, parent);\n    return renderer;\n  }",
    "reference": "Create a Renderer getting a copy from the prototypes collection.\n\n@param content to render.\n@param parent used to inflate the view.\n@return a new renderer.",
    "generated": "Create a renderer.\n\n@param content the content\n@param parent the parent component\n@return the renderer"
  },
  {
    "code": "private Renderer getPrototypeByIndex(final int prototypeIndex) {\n    Renderer prototypeSelected = null;\n    int i = 0;\n    for (Renderer prototype : prototypes) {\n      if (i == prototypeIndex) {\n        prototypeSelected = prototype;\n      }\n      i++;\n    }\n    return prototypeSelected;\n  }",
    "reference": "Search one prototype using the prototype index which is equals to the view type. This method\nhas to be implemented because prototypes member is declared with Collection and that interface\ndoesn't allow the client code to get one element by index.\n\n@param prototypeIndex used to search.\n@return prototype renderer.",
    "generated": "Get the prototype by its index.\n\n@param prototypeIndex index"
  },
  {
    "code": "private boolean isRecyclable(View convertView, T content) {\n    boolean isRecyclable = false;\n    if (convertView != null && convertView.getTag() != null) {\n      Class prototypeClass = getPrototypeClass(content);\n      validatePrototypeClass(prototypeClass);\n      isRecyclable = prototypeClass.equals(convertView.getTag().getClass());\n    }\n    return isRecyclable;\n  }",
    "reference": "Check if one Renderer is recyclable getting it from the convertView's tag and checking the\nclass used.\n\n@param convertView to get the renderer if is not null.\n@param content used to get the prototype class.\n@return true if the renderer is recyclable.",
    "generated": "Check if the content is a isRecyclable view.\n\n@param convertView view\n@param content content\n@return true if the content is isRecyclable"
  },
  {
    "code": "private int getItemViewType(Class prototypeClass) {\n    int itemViewType = -1;\n    for (Renderer renderer : prototypes) {\n      if (renderer.getClass().equals(prototypeClass)) {\n        itemViewType = getPrototypeIndex(renderer);\n        break;\n      }\n    }\n    if (itemViewType == -1) {\n      throw new PrototypeNotFoundException(\n          \"Review your RendererBuilder implementation, you are returning one\"\n              + \" prototype class not found in prototypes collection\");\n    }\n    return itemViewType;\n  }",
    "reference": "Return the Renderer class associated to the prototype.\n\n@param prototypeClass used to search the renderer in the prototypes collection.\n@return the prototype index associated to the prototypeClass.",
    "generated": "Returns the item view type of the given prototype class.\n\n@param prototypeClass prototype class"
  },
  {
    "code": "private int getPrototypeIndex(Renderer renderer) {\n    int index = 0;\n    for (Renderer prototype : prototypes) {\n      if (prototype.getClass().equals(renderer.getClass())) {\n        break;\n      }\n      index++;\n    }\n    return index;\n  }",
    "reference": "Return the index associated to the Renderer.\n\n@param renderer used to search in the prototypes collection.\n@return the prototype index associated to the renderer passed as argument.",
    "generated": "Get the prototype index of the renderer.\n\n@param renderer renderer"
  },
  {
    "code": "private void validateAttributes() {\n    if (content == null) {\n      throw new NullContentException(\"RendererBuilder needs content to create Renderer instances\");\n    }\n\n    if (parent == null) {\n      throw new NullParentException(\"RendererBuilder needs a parent to inflate Renderer instances\");\n    }\n\n    if (layoutInflater == null) {\n      throw new NullLayoutInflaterException(\n          \"RendererBuilder needs a LayoutInflater to inflate Renderer instances\");\n    }\n  }",
    "reference": "Throws one RendererException if the content parent or layoutInflater are null.",
    "generated": "Validate the attributes of the RendererBuilder."
  },
  {
    "code": "private void validateAttributesToCreateANewRendererViewHolder() {\n    if (viewType == null) {\n      throw new NullContentException(\n          \"RendererBuilder needs a view type to create a RendererViewHolder\");\n    }\n    if (layoutInflater == null) {\n      throw new NullLayoutInflaterException(\n          \"RendererBuilder needs a LayoutInflater to create a RendererViewHolder\");\n    }\n    if (parent == null) {\n      throw new NullParentException(\n          \"RendererBuilder needs a parent to create a RendererViewHolder\");\n    }\n  }",
    "reference": "Throws one RendererException if the viewType, layoutInflater or parent are null.",
    "generated": "Validate the attributes to create an existing RendererViewHolder."
  },
  {
    "code": "protected Class getPrototypeClass(T content) {\n    if (prototypes.size() == 1) {\n      return prototypes.get(0).getClass();\n    } else {\n      return binding.get(content.getClass());\n    }\n  }",
    "reference": "Method to be implemented by the RendererBuilder subtypes. In this method the library user will\ndefine the mapping between content and renderer class.\n\n@param content used to map object to Renderers.\n@return the class associated to the renderer.",
    "generated": "Get the superclass class of the given content.\n\n@param content the content\n@return the superclass class"
  },
  {
    "code": "@Override protected View inflate(LayoutInflater inflater, ViewGroup parent) {\n    View inflatedView = inflater.inflate(R.layout.video_renderer, parent, false);\n        /*\n         * You don't have to use ButterKnife library to implement the mapping between your layout\n         * and your widgets you can implement setUpView and hookListener methods declared in\n         * Renderer<T> class.\n         */\n    ButterKnife.bind(this, inflatedView);\n    return inflatedView;\n  }",
    "reference": "Inflate the main layout used to render videos in the list view.\n\n@param inflater LayoutInflater service to inflate.\n@param parent ViewGroup used to inflate xml.\n@return view inflated.",
    "generated": "Inflate the video renderer.\n\n@param inflater LayoutInflater parent"
  },
  {
    "code": "@Override public void render() {\n    Video video = getContent();\n    renderThumbnail(video);\n    renderTitle(video);\n    renderMarker(video);\n    renderLabel();\n  }",
    "reference": "Main render algorithm based on render the video thumbnail, render the title, render the marker\nand the label.",
    "generated": "Render the video content.\n\n@param content the video content\n@param marker the marker\n@param label the marker\n@return"
  },
  {
    "code": "private void renderThumbnail(Video video) {\n    Picasso.with(getContext()).cancelRequest(thumbnail);\n    Picasso.with(getContext())\n        .load(video.getThumbnail())\n        .placeholder(R.drawable.placeholder)\n        .into(thumbnail);\n  }",
    "reference": "Use picasso to render the video thumbnail into the thumbnail widget using a temporal\nplaceholder.\n\n@param video to get the rendered thumbnail.",
    "generated": "Render the thumbnail of a video.\n\n@param video video the video\n@param video the video to render"
  },
  {
    "code": "public void onCreate(T content, LayoutInflater layoutInflater, ViewGroup parent) {\n    this.content = content;\n    this.rootView = inflate(layoutInflater, parent);\n    if (rootView == null) {\n      throw new NotInflateViewException(\n          \"Renderer instances have to return a not null view in inflateView method\");\n    }\n    this.rootView.setTag(this);\n    setUpView(rootView);\n    hookListeners(rootView);\n  }",
    "reference": "Method called when the renderer is going to be created. This method has the responsibility of\ninflate the xml layout using the layoutInflater and the parent ViewGroup, set itself to the\ntag and call setUpView and hookListeners methods.\n\n@param content to render. If you are using Renderers with RecyclerView widget the content will\nbe null in this method.\n@param layoutInflater used to inflate the view.\n@param parent used to inflate the view.",
    "generated": "Create a renderer instance.\n\n@param content LayoutInflater layoutInflater"
  },
  {
    "code": "Renderer copy() {\n    Renderer copy = null;\n    try {\n      copy = (Renderer) this.clone();\n    } catch (CloneNotSupportedException e) {\n      Log.e(\"Renderer\", \"All your renderers should be clonables.\");\n    }\n    return copy;\n  }",
    "reference": "Create a clone of the Renderer. This method is the base of the prototype mechanism implemented\nto avoid create new objects from RendererBuilder. Pay an special attention implementing clone\nmethod in Renderer subtypes.\n\n@return a copy of the current renderer.",
    "generated": "Copy a renderer.\n\n@param this renderer instance"
  },
  {
    "code": "@Override public View getView(int position, View convertView, ViewGroup parent) {\n    T content = getItem(position);\n    rendererBuilder.withContent(content);\n    rendererBuilder.withConvertView(convertView);\n    rendererBuilder.withParent(parent);\n    rendererBuilder.withLayoutInflater(LayoutInflater.from(parent.getContext()));\n    Renderer<T> renderer = rendererBuilder.build();\n    if (renderer == null) {\n      throw new NullRendererBuiltException(\"RendererBuilder have to return a not null Renderer\");\n    }\n    updateRendererExtraValues(content, renderer, position);\n    renderer.render();\n    return renderer.getRootView();\n  }",
    "reference": "Main method of RendererAdapter. This method has the responsibility of update the\nRendererBuilder values and create or recycle a new Renderer. Once the renderer has been\nobtained the RendereBuilder will call the render method in the renderer and will return the\nRenderer root view to the ListView.\n\nIf rRendererBuilder returns a null Renderer this method will throw a\nNullRendererBuiltException.\n\n@param position to render.\n@param convertView to use to recycle.\n@param parent used to inflate views.\n@return view rendered.",
    "generated": "Returns the view of the given position.\n\n@param position the position of the view\n@param convertView"
  },
  {
    "code": "@Override public Object instantiateItem(ViewGroup parent, int position) {\n        T content = getItem(position);\n        rendererBuilder.withContent(content);\n        rendererBuilder.withParent(parent);\n        rendererBuilder.withLayoutInflater(LayoutInflater.from(parent.getContext()));\n        Renderer<T> renderer = rendererBuilder.build();\n        if (renderer == null) {\n            throw new NullRendererBuiltException(\"RendererBuilder have to return a not null Renderer\");\n        }\n        updateRendererExtraValues(content, renderer, position);\n        renderer.render();\n        View view = renderer.getRootView();\n        parent.addView(view);\n        return view;\n    }",
    "reference": "Main method of VPRendererAdapter. This method has the responsibility of update the\nRendererBuilder values and create or recycle a new Renderer. Once the renderer has been\nobtained the RendereBuilder will call the render method in the renderer and will return the\nRenderer root view to the ViewPager.\n\nIf RendererBuilder returns a null Renderer this method will throw a\nNullRendererBuiltException.\n\n@param parent The containing View in which the page will be shown.\n@param position to render.\n@return view rendered.",
    "generated": "Instantiate an item.\n\n@param parent view group\n@param position position"
  },
  {
    "code": "public VideoCollection generate(final int videoCount) {\n    List<Video> videos = new LinkedList<Video>();\n    for (int i = 0; i < videoCount; i++) {\n      Video video = generateRandomVideo();\n      videos.add(video);\n    }\n    return new VideoCollection(videos);\n  }",
    "reference": "Generate a VideoCollection with random data obtained form VIDEO_INFO map. You don't need o\ncreate your own AdapteeCollections. Review ListAdapteeCollection if needed.\n\n@param videoCount size of the collection.\n@return VideoCollection generated.",
    "generated": "Generate a video collection.\n\n@param videoCount Video count"
  },
  {
    "code": "private void initializeVideoInfo() {\n    VIDEO_INFO.put(\"The Big Bang Theory\", \"http://thetvdb.com/banners/_cache/posters/80379-9.jpg\");\n    VIDEO_INFO.put(\"Breaking Bad\", \"http://thetvdb.com/banners/_cache/posters/81189-22.jpg\");\n    VIDEO_INFO.put(\"Arrow\", \"http://thetvdb.com/banners/_cache/posters/257655-15.jpg\");\n    VIDEO_INFO.put(\"Game of Thrones\", \"http://thetvdb.com/banners/_cache/posters/121361-26.jpg\");\n    VIDEO_INFO.put(\"Lost\", \"http://thetvdb.com/banners/_cache/posters/73739-2.jpg\");\n    VIDEO_INFO.put(\"How I met your mother\",\n        \"http://thetvdb.com/banners/_cache/posters/75760-29.jpg\");\n    VIDEO_INFO.put(\"Dexter\", \"http://thetvdb.com/banners/_cache/posters/79349-24.jpg\");\n    VIDEO_INFO.put(\"Sleepy Hollow\", \"http://thetvdb.com/banners/_cache/posters/269578-5.jpg\");\n    VIDEO_INFO.put(\"The Vampire Diaries\", \"http://thetvdb.com/banners/_cache/posters/95491-27.jpg\");\n    VIDEO_INFO.put(\"Friends\", \"http://thetvdb.com/banners/_cache/posters/79168-4.jpg\");\n    VIDEO_INFO.put(\"New Girl\", \"http://thetvdb.com/banners/_cache/posters/248682-9.jpg\");\n    VIDEO_INFO.put(\"The Mentalist\", \"http://thetvdb.com/banners/_cache/posters/82459-1.jpg\");\n    VIDEO_INFO.put(\"Sons of Anarchy\", \"http://thetvdb.com/banners/_cache/posters/82696-1.jpg\");\n  }",
    "reference": "Initialize VIDEO_INFO data.",
    "generated": "Initialize the video information."
  },
  {
    "code": "private Video generateRandomVideo() {\n    Video video = new Video();\n    configureFavoriteStatus(video);\n    configureLikeStatus(video);\n    configureLiveStatus(video);\n    configureTitleAndThumbnail(video);\n    return video;\n  }",
    "reference": "Create a random video.\n\n@return random video.",
    "generated": "Generate a random Video."
  },
  {
    "code": "protected final ByteBuffer parseContent(ByteBuffer in) throws BaseExceptions.ParserException {\n        if (contentComplete()) {\n            throw new BaseExceptions.InvalidState(\"content already complete: \" + _endOfContent);\n        } else {\n            switch (_endOfContent) {\n            case UNKNOWN_CONTENT:\n                // This makes sense only for response parsing. Requests must always have\n                // either Content-Length or Transfer-Encoding\n\n                _endOfContent = EndOfContent.EOF_CONTENT;\n                _contentLength = Long.MAX_VALUE;    // Its up to the user to limit a body size\n                return parseContent(in);\n\n            case CONTENT_LENGTH:\n            case EOF_CONTENT:\n                return nonChunkedContent(in);\n\n            case CHUNKED_CONTENT:\n                return chunkedContent(in);\n\n            default:\n                throw new BaseExceptions.InvalidState(\"not implemented: \" + _endOfContent);\n            }\n        }\n    }",
    "reference": "Parses the buffer into body content\n@param in ByteBuffer to parse\n@return a ByteBuffer with the parsed body content. Buffer in may not be depleted. If more data is\nneeded, null is returned. In the case of content complete, an empty ByteBuffer is returned.\n@throws BaseExceptions.ParserException",
    "generated": "Parse a ByteBuffer."
  },
  {
    "code": "final protected void putChar(char c) {\n        final int clen = _internalBuffer.length;\n        if (clen == _bufferPosition) {\n            final char[] next = new char[2 * clen + 1];\n\n            System.arraycopy(_internalBuffer, 0, next, 0, _bufferPosition);\n            _internalBuffer = next;\n        }\n\n        _internalBuffer[_bufferPosition++] = c;\n    }",
    "reference": "Store the char in the internal buffer",
    "generated": "Write a character to the buffer.\n\n@param c character to write\n@return"
  },
  {
    "code": "final protected String getTrimmedString() throws BadMessage {\n        if (_bufferPosition == 0) return \"\";\n\n        int start = 0;\n        boolean quoted = false;\n        // Look for start\n        while (start < _bufferPosition) {\n            final char ch = _internalBuffer[start];\n            if (ch == '\"') {\n                quoted = true;\n                break;\n            }\n            else if (ch != HttpTokens.SPACE && ch != HttpTokens.TAB) {\n                break;\n            }\n            start++;\n        }\n\n        int end = _bufferPosition;  // Position is of next write\n\n        // Look for end\n        while(end > start) {\n            final char ch = _internalBuffer[end - 1];\n\n            if (quoted) {\n                if (ch == '\"') break;\n                else if (ch != HttpTokens.SPACE && ch != HttpTokens.TAB) {\n                    throw new BadMessage(\"String might not quoted correctly: '\" + getString() + \"'\");\n                }\n            }\n            else if (ch != HttpTokens.SPACE && ch != HttpTokens.TAB) break;\n            end--;\n        }\n\n        String str = new String(_internalBuffer, start, end - start);\n\n        return str;\n    }",
    "reference": "Returns the string in the buffer minus an leading or trailing whitespace or quotes",
    "generated": "Generate a trimmed string.\n\n@param _bufferPosition - Position is of next write\n\n       "
  },
  {
    "code": "final protected char next(final ByteBuffer buffer, boolean allow8859) throws BaseExceptions.BadMessage {\n\n        if (!buffer.hasRemaining()) return HttpTokens.EMPTY_BUFF;\n\n        if (_segmentByteLimit <= _segmentBytePosition) {\n            shutdownParser();\n            throw new BaseExceptions.BadMessage(\"Request length limit exceeded: \" + _segmentByteLimit);\n        }\n\n        final byte b = buffer.get();\n        _segmentBytePosition++;\n\n        // If we ended on a CR, make sure we are\n        if (_cr) {\n            if (b != HttpTokens.LF) {\n                throw new BadCharacter(\"Invalid sequence: LF didn't follow CR: \" + b);\n            }\n            _cr = false;\n            return (char)b;  // must be LF\n        }\n\n        // Make sure its a valid character\n        if (b < HttpTokens.SPACE) {\n            if (b == HttpTokens.CR) {   // Set the flag to check for _cr and just run again\n                _cr = true;\n                return next(buffer, allow8859);\n            }\n            else if (b == HttpTokens.TAB || allow8859 && b < 0) {\n                return (char)(b & 0xff);\n            }\n            else if (b == HttpTokens.LF) {\n                return (char)b; // A backend should accept a bare linefeed. http://tools.ietf.org/html/rfc2616#section-19.3\n            }\n            else if (isLenient()) {\n                return HttpTokens.REPLACEMENT;\n            }\n            else {\n                shutdownParser();\n                throw new BadCharacter(\"Invalid char: '\" + (char)(b & 0xff) + \"', 0x\" + Integer.toHexString(b));\n            }\n        }\n\n        // valid ascii char\n        return (char)b;\n    }",
    "reference": "Removes CRs but returns LFs",
    "generated": "Next character from a ByteBuffer.\n\n@param buffer the buffer to next character\n@param allow8859 flag to check for a valid character\n@return the next character"
  },
  {
    "code": "protected int mapGanttBarHeight(int height)\n   {\n      switch (height)\n      {\n         case 0:\n         {\n            height = 6;\n            break;\n         }\n\n         case 1:\n         {\n            height = 8;\n            break;\n         }\n\n         case 2:\n         {\n            height = 10;\n            break;\n         }\n\n         case 3:\n         {\n            height = 12;\n            break;\n         }\n\n         case 4:\n         {\n            height = 14;\n            break;\n         }\n\n         case 5:\n         {\n            height = 18;\n            break;\n         }\n\n         case 6:\n         {\n            height = 24;\n            break;\n         }\n      }\n\n      return (height);\n   }",
    "reference": "This method maps the encoded height of a Gantt bar to\nthe height in pixels.\n\n@param height encoded height\n@return height in pixels",
    "generated": "Map the GanttBar height.\n\n@param height the height of the GanttBar\n@return the height of the GanttBar"
  },
  {
    "code": "protected TableFontStyle getColumnFontStyle(byte[] data, int offset, Map<Integer, FontBase> fontBases)\n   {\n      int uniqueID = MPPUtility.getInt(data, offset);\n      FieldType fieldType = MPPTaskField.getInstance(MPPUtility.getShort(data, offset + 4));\n      Integer index = Integer.valueOf(MPPUtility.getByte(data, offset + 8));\n      int style = MPPUtility.getByte(data, offset + 9);\n      ColorType color = ColorType.getInstance(MPPUtility.getByte(data, offset + 10));\n      int change = MPPUtility.getByte(data, offset + 12);\n\n      FontBase fontBase = fontBases.get(index);\n\n      boolean bold = ((style & 0x01) != 0);\n      boolean italic = ((style & 0x02) != 0);\n      boolean underline = ((style & 0x04) != 0);\n\n      boolean boldChanged = ((change & 0x01) != 0);\n      boolean underlineChanged = ((change & 0x02) != 0);\n      boolean italicChanged = ((change & 0x04) != 0);\n      boolean colorChanged = ((change & 0x08) != 0);\n      boolean fontChanged = ((change & 0x10) != 0);\n      boolean backgroundColorChanged = (uniqueID == -1);\n      boolean backgroundPatternChanged = (uniqueID == -1);\n\n      return (new TableFontStyle(uniqueID, fieldType, fontBase, italic, bold, underline, false, color.getColor(), Color.BLACK, BackgroundPattern.TRANSPARENT, italicChanged, boldChanged, underlineChanged, false, colorChanged, fontChanged, backgroundColorChanged, backgroundPatternChanged));\n   }",
    "reference": "Retrieve column font details from a block of property data.\n\n@param data property data\n@param offset offset into property data\n@param fontBases map of font bases\n@return ColumnFontStyle instance",
    "generated": "Generate a table font style for a given byte array.\n\n@param data byte array of data\n@param offset the offset of the column\n@return table font style"
  },
  {
    "code": "private InputStream prepareInputStream(InputStream stream) throws IOException\n   {\n      InputStream result;\n      BufferedInputStream bis = new BufferedInputStream(stream);\n      readHeaderProperties(bis);\n      if (isCompressed())\n      {\n         result = new InflaterInputStream(bis);\n      }\n      else\n      {\n         result = bis;\n      }\n      return result;\n   }",
    "reference": "If the file is compressed, handle this so that the stream is ready to read.\n\n@param stream input stream\n@return uncompressed input stream",
    "generated": "Prepare a stream.\n\n@param stream InputStream\n@return InputStream"
  },
  {
    "code": "private String readHeaderString(BufferedInputStream stream) throws IOException\n   {\n      int bufferSize = 100;\n      stream.mark(bufferSize);\n      byte[] buffer = new byte[bufferSize];\n      stream.read(buffer);\n      Charset charset = CharsetHelper.UTF8;\n      String header = new String(buffer, charset);\n      int prefixIndex = header.indexOf(\"PPX!!!!|\");\n      int suffixIndex = header.indexOf(\"|!!!!XPP\");\n\n      if (prefixIndex != 0 || suffixIndex == -1)\n      {\n         throw new IOException(\"File format not recognised\");\n      }\n\n      int skip = suffixIndex + 9;\n      stream.reset();\n      stream.skip(skip);\n\n      return header.substring(prefixIndex + 8, suffixIndex);\n   }",
    "reference": "Read the header from the Phoenix file.\n\n@param stream input stream\n@return raw header data",
    "generated": "Read the header from a buffered stream.\n\n@param stream InputStream\n@return header string"
  },
  {
    "code": "private void readHeaderProperties(BufferedInputStream stream) throws IOException\n   {\n      String header = readHeaderString(stream);\n      for (String property : header.split(\"\\\\|\"))\n      {\n         String[] expression = property.split(\"=\");\n         m_properties.put(expression[0], expression[1]);\n      }\n   }",
    "reference": "Read properties from the raw header data.\n\n@param stream input stream",
    "generated": "Read the header properties."
  },
  {
    "code": "@Override public void process(MPPReader reader, ProjectFile file, DirectoryEntry root) throws MPXJException, IOException\n   {\n      try\n      {\n         populateMemberData(reader, file, root);\n         processProjectProperties();\n\n         if (!reader.getReadPropertiesOnly())\n         {\n            processCalendarData();\n            processResourceData();\n            processTaskData();\n            processConstraintData();\n            processAssignmentData();\n\n            if (reader.getReadPresentationData())\n            {\n               processViewPropertyData();\n               processViewData();\n               processTableData();\n            }\n         }\n      }\n\n      finally\n      {\n         clearMemberData();\n      }\n   }",
    "reference": "This method is used to process an MPP8 file. This is the file format\nused by Project 98.\n\n@param reader parent file reader\n@param file Parent MPX file\n@param root Root of the POI file system.\n@throws MPXJException\n@throws IOException",
    "generated": "Process a project file.\n\n@param reader the MPXReader instance\n@param file the project file\n@param root the directory entry\n@return void"
  },
  {
    "code": "private void updateBaseCalendarNames(List<Pair<ProjectCalendar, Integer>> baseCalendars)\n   {\n      for (Pair<ProjectCalendar, Integer> pair : baseCalendars)\n      {\n         ProjectCalendar cal = pair.getFirst();\n         Integer baseCalendarID = pair.getSecond();\n         ProjectCalendar baseCal = m_calendarMap.get(baseCalendarID);\n         if (baseCal != null)\n         {\n            cal.setParent(baseCal);\n         }\n      }\n   }",
    "reference": "The way calendars are stored in an MPP8 file means that there\ncan be forward references between the base calendar unique ID for a\nderived calendar, and the base calendar itself. To get around this,\nwe initially populate the base calendar name attribute with the\nbase calendar unique ID, and now in this method we can convert those\nID values into the correct names.\n\n@param baseCalendars list of calendars and base calendar IDs",
    "generated": "Update the base calendar names.\n\n@param baseCalendars list of base calendars"
  },
  {
    "code": "private void setTaskNotes(Task task, byte[] data, ExtendedData taskExtData, FixDeferFix taskVarData)\n   {\n      String notes = taskExtData.getString(TASK_NOTES);\n      if (notes == null && data.length == 366)\n      {\n         byte[] offsetData = taskVarData.getByteArray(getOffset(data, 362));\n         if (offsetData != null && offsetData.length >= 12)\n         {\n            notes = taskVarData.getString(getOffset(offsetData, 8));\n            \n            // We do pick up some random stuff with this approach, and \n            // we don't know enough about the file format to know when to ignore it\n            // so we'll use a heuristic here to ignore anything that\n            // doesn't look like RTF.\n            if (notes != null && notes.indexOf('{') == -1)\n            {\n               notes = null;\n            }\n         }\n      }\n      \n      if (notes != null)\n      {\n         if (m_reader.getPreserveNoteFormatting() == false)\n         {\n            notes = RtfHelper.strip(notes);\n         }\n\n         task.setNotes(notes);\n      }\n   }",
    "reference": "There appear to be two ways of representing task notes in an MPP8 file.\nThis method tries to determine which has been used.\n\n@param task task\n@param data task data\n@param taskExtData extended task data\n@param taskVarData task var data",
    "generated": "Set the task notes."
  },
  {
    "code": "private void processHyperlinkData(Task task, byte[] data)\n   {\n      if (data != null)\n      {\n         int offset = 12;\n         String hyperlink;\n         String address;\n         String subaddress;\n\n         offset += 12;\n         hyperlink = MPPUtility.getUnicodeString(data, offset);\n         offset += ((hyperlink.length() + 1) * 2);\n\n         offset += 12;\n         address = MPPUtility.getUnicodeString(data, offset);\n         offset += ((address.length() + 1) * 2);\n\n         offset += 12;\n         subaddress = MPPUtility.getUnicodeString(data, offset);\n\n         task.setHyperlink(hyperlink);\n         task.setHyperlinkAddress(address);\n         task.setHyperlinkSubAddress(subaddress);\n      }\n   }",
    "reference": "This method is used to extract the task hyperlink attributes\nfrom a block of data and call the appropriate modifier methods\nto configure the specified task object.\n\n@param task task instance\n@param data hyperlink data block",
    "generated": "Process the hyperlink data."
  },
  {
    "code": "public void process(String driverClass, String connectionString, String projectID, String outputFile) throws Exception\n   {\n      System.out.println(\"Reading Primavera database started.\");\n\n      Class.forName(driverClass);\n      Properties props = new Properties();\n\n      //\n      // This is not a very robust way to detect that we're working with SQLlite...\n      // If you are trying to grab data from\n      // a standalone P6 using SQLite, the SQLite JDBC driver needs this property\n      // in order to correctly parse timestamps.\n      //\n      if (driverClass.equals(\"org.sqlite.JDBC\"))\n      {\n         props.setProperty(\"date_string_format\", \"yyyy-MM-dd HH:mm:ss\");\n      }\n\n      Connection c = DriverManager.getConnection(connectionString, props);\n      PrimaveraDatabaseReader reader = new PrimaveraDatabaseReader();\n      reader.setConnection(c);\n\n      processProject(reader, Integer.parseInt(projectID), outputFile);\n   }",
    "reference": "Extract Primavera project data and export in another format.\n\n@param driverClass JDBC driver class name\n@param connectionString JDBC connection string\n@param projectID project ID\n@param outputFile output file\n@throws Exception",
    "generated": "Reads Primavera database from a standalone P6 using SQLite, the SQLite JDBC driver needs this property\n     "
  },
  {
    "code": "private void processProject(PrimaveraDatabaseReader reader, int projectID, String outputFile) throws Exception\n   {\n      long start = System.currentTimeMillis();\n      reader.setProjectID(projectID);\n      ProjectFile projectFile = reader.read();\n      long elapsed = System.currentTimeMillis() - start;\n      System.out.println(\"Reading database completed in \" + elapsed + \"ms.\");\n\n      System.out.println(\"Writing output file started.\");\n      start = System.currentTimeMillis();\n      ProjectWriter writer = ProjectWriterUtility.getProjectWriter(outputFile);\n      writer.write(projectFile, outputFile);\n      elapsed = System.currentTimeMillis() - start;\n      System.out.println(\"Writing output completed in \" + elapsed + \"ms.\");\n   }",
    "reference": "Process a single project.\n\n@param reader Primavera reader\n@param projectID required project ID\n@param outputFile output file name",
    "generated": "Process a project.\n\n@param reader PrimaveraDatabaseReader reader\n@param outputFile"
  },
  {
    "code": "public static ResourceField getInstance(int value)\n   {\n      ResourceField result = null;\n\n      if (value >= 0 && value < FIELD_ARRAY.length)\n      {\n         result = FIELD_ARRAY[value];\n      }\n      else\n      {\n         if ((value & 0x8000) != 0)\n         {\n            int baseValue = ResourceField.ENTERPRISE_CUSTOM_FIELD1.getValue();\n            int id = baseValue + (value & 0xFFF);\n            result = ResourceField.getInstance(id);\n         }\n      }\n\n      return (result);\n   }",
    "reference": "Retrieve an instance of the ResourceField class based on the data read from an\nMS Project file.\n\n@param value value from an MS Project file\n@return ResourceField instance",
    "generated": "Get a ResourceField instance.\n\n@param value value"
  },
  {
    "code": "public void update(Record record, boolean isText) throws MPXJException\n   {\n      int length = record.getLength();\n\n      for (int i = 0; i < length; i++)\n      {\n         if (isText == true)\n         {\n            add(getTaskCode(record.getString(i)));\n         }\n         else\n         {\n            add(record.getInteger(i).intValue());\n         }\n      }\n   }",
    "reference": "This method populates the task model from data read from an MPX file.\n\n@param record data read from an MPX file\n@param isText flag indicating whether the textual or numeric data is being supplied",
    "generated": "Update a record.\n\n@param record the record to update\n@param isText"
  },
  {
    "code": "private void add(int field)\n   {\n      if (field < m_flags.length)\n      {\n         if (m_flags[field] == false)\n         {\n            m_flags[field] = true;\n            m_fields[m_count] = field;\n            ++m_count;\n         }\n      }\n   }",
    "reference": "This method is called from the task class each time an attribute\nis added, ensuring that all of the attributes present in each task\nrecord are present in the resource model.\n\n@param field field identifier",
    "generated": "Add a field to the list of fields."
  },
  {
    "code": "@SuppressWarnings(\"unchecked\") private boolean isFieldPopulated(Task task, TaskField field)\n   {\n      boolean result = false;\n      if (field != null)\n      {\n         Object value = task.getCachedValue(field);\n         switch (field)\n         {\n            case PREDECESSORS:\n            case SUCCESSORS:\n            {\n               result = value != null && !((List<Relation>) value).isEmpty();\n               break;\n            }\n\n            default:\n            {\n               result = value != null;\n               break;\n            }\n         }\n      }\n      return result;\n   }",
    "reference": "Determine if a task field contains data.\n\n@param task task instance\n@param field target field\n@return true if the field contains data",
    "generated": "Check if the field is populated.\n\n@param task Task\n@param field field"
  },
  {
    "code": "private String getTaskField(int key)\n   {\n      String result = null;\n\n      if ((key > 0) && (key < m_taskNames.length))\n      {\n         result = m_taskNames[key];\n      }\n\n      return (result);\n   }",
    "reference": "Returns Task field name of supplied code no.\n\n@param key - the code no of required Task field\n@return - field name",
    "generated": "Generate a task field.\n\n@param key the key to generate\n@return the task field"
  },
  {
    "code": "private int getTaskCode(String field) throws MPXJException\n   {\n      Integer result = m_taskNumbers.get(field.trim());\n\n      if (result == null)\n      {\n         throw new MPXJException(MPXJException.INVALID_TASK_FIELD_NAME + \" \" + field);\n      }\n\n      return (result.intValue());\n   }",
    "reference": "Returns code number of Task field supplied.\n\n@param field - name\n@return - code no",
    "generated": "Get the task code for a given field.\n\n@param field field name"
  },
  {
    "code": "public static String parseString(String value)\n   {\n      if (value != null)\n      {\n         // Strip angle brackets if present\n         if (!value.isEmpty() && value.charAt(0) == '<')\n         {\n            value = value.substring(1, value.length() - 1);\n         }\n\n         // Strip quotes if present\n         if (!value.isEmpty() && value.charAt(0) == '\"')\n         {\n            value = value.substring(1, value.length() - 1);\n         }\n      }\n      return value;\n   }",
    "reference": "Parse a string.\n\n@param value string representation\n@return String value",
    "generated": "Parse a value.\n\n@param value value value\n@return value"
  },
  {
    "code": "public static Number parseDouble(String value) throws ParseException\n   {\n\n      Number result = null;\n      value = parseString(value);\n\n      // If we still have a value\n      if (value != null && !value.isEmpty() && !value.equals(\"-1 -1\"))\n      {\n         int index = value.indexOf(\"E+\");\n         if (index != -1)\n         {\n            value = value.substring(0, index) + 'E' + value.substring(index + 2, value.length());\n         }\n\n         if (value.indexOf('E') != -1)\n         {\n            result = DOUBLE_FORMAT.get().parse(value);\n         }\n         else\n         {\n            result = Double.valueOf(value);\n         }\n      }\n\n      return result;\n   }",
    "reference": "Parse the string representation of a double.\n\n@param value string representation\n@return Java representation\n@throws ParseException",
    "generated": "Parse a double value\n@param value the string to parse\n@return the double value"
  },
  {
    "code": "public static Boolean parseBoolean(String value) throws ParseException\n   {\n      Boolean result = null;\n      Integer number = parseInteger(value);\n      if (number != null)\n      {\n         result = number.intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;\n      }\n\n      return result;\n   }",
    "reference": "Parse a string representation of a Boolean value.\n\n@param value string representation\n@return Boolean value",
    "generated": "Parse a boolean value.\n\n@param value"
  },
  {
    "code": "public static Integer parseInteger(String value) throws ParseException\n   {\n      Integer result = null;\n\n      if (value.length() > 0 && value.indexOf(' ') == -1)\n      {\n         if (value.indexOf('.') == -1)\n         {\n            result = Integer.valueOf(value);\n         }\n         else\n         {\n            Number n = DatatypeConverter.parseDouble(value);\n            result = Integer.valueOf(n.intValue());\n         }\n      }\n\n      return result;\n   }",
    "reference": "Parse a string representation of an Integer value.\n\n@param value string representation\n@return Integer value",
    "generated": "Parse an integer value.\n\n@param value the string to parse\n@return the integer value"
  },
  {
    "code": "public static Date parseEpochTimestamp(String value)\n   {\n      Date result = null;\n\n      if (value.length() > 0)\n      {\n         if (!value.equals(\"-1 -1\"))\n         {\n            Calendar cal = DateHelper.popCalendar(JAVA_EPOCH);\n\n            int index = value.indexOf(' ');\n            if (index == -1)\n            {\n               if (value.length() < 6)\n               {\n                  value = \"000000\" + value;\n                  value = value.substring(value.length() - 6);\n               }\n\n               int hours = Integer.parseInt(value.substring(0, 2));\n               int minutes = Integer.parseInt(value.substring(2, 4));\n               int seconds = Integer.parseInt(value.substring(4));\n\n               cal.set(Calendar.HOUR, hours);\n               cal.set(Calendar.MINUTE, minutes);\n               cal.set(Calendar.SECOND, seconds);\n            }\n            else\n            {\n               long astaDays = Long.parseLong(value.substring(0, index));\n               int astaSeconds = Integer.parseInt(value.substring(index + 1));\n\n               cal.add(Calendar.DAY_OF_YEAR, (int) (astaDays - ASTA_EPOCH));\n               cal.set(Calendar.MILLISECOND, 0);\n               cal.set(Calendar.SECOND, 0);\n               cal.set(Calendar.HOUR, 0);\n               cal.add(Calendar.SECOND, astaSeconds);\n            }\n\n            result = cal.getTime();\n            DateHelper.pushCalendar(cal);\n         }\n      }\n\n      return result;\n   }",
    "reference": "Parse the string representation of a timestamp.\n\n@param value string representation\n@return Java representation",
    "generated": "Parse the epoch timestamp."
  },
  {
    "code": "protected void processProjectListItem(Map<Integer, String> result, Row row)\n   {\n      Integer id = row.getInteger(\"PROJ_ID\");\n      String name = row.getString(\"PROJ_NAME\");\n      result.put(id, name);\n   }",
    "reference": "Retrieve the details of a single project from the database.\n\n@param result Map instance containing the results\n@param row result set row read from the database",
    "generated": "Process the project list item.\n\n@param result map row\n@param row"
  },
  {
    "code": "protected void processCalendarData(ProjectCalendar calendar, Row row)\n   {\n      int dayIndex = row.getInt(\"CD_DAY_OR_EXCEPTION\");\n      if (dayIndex == 0)\n      {\n         processCalendarException(calendar, row);\n      }\n      else\n      {\n         processCalendarHours(calendar, row, dayIndex);\n      }\n   }",
    "reference": "Read calendar hours and exception data.\n\n@param calendar parent calendar\n@param row calendar hours and exception data",
    "generated": "Process calendar data.\n\n@param calendar ProjectCalendar calendar\n@param row Row"
  },
  {
    "code": "private void processCalendarException(ProjectCalendar calendar, Row row)\n   {\n      Date fromDate = row.getDate(\"CD_FROM_DATE\");\n      Date toDate = row.getDate(\"CD_TO_DATE\");\n      boolean working = row.getInt(\"CD_WORKING\") != 0;\n      ProjectCalendarException exception = calendar.addCalendarException(fromDate, toDate);\n      if (working)\n      {\n         exception.addRange(new DateRange(row.getDate(\"CD_FROM_TIME1\"), row.getDate(\"CD_TO_TIME1\")));\n         exception.addRange(new DateRange(row.getDate(\"CD_FROM_TIME2\"), row.getDate(\"CD_TO_TIME2\")));\n         exception.addRange(new DateRange(row.getDate(\"CD_FROM_TIME3\"), row.getDate(\"CD_TO_TIME3\")));\n         exception.addRange(new DateRange(row.getDate(\"CD_FROM_TIME4\"), row.getDate(\"CD_TO_TIME4\")));\n         exception.addRange(new DateRange(row.getDate(\"CD_FROM_TIME5\"), row.getDate(\"CD_TO_TIME5\")));\n      }\n   }",
    "reference": "Process a calendar exception.\n\n@param calendar parent calendar\n@param row calendar exception data",
    "generated": "Process the calendar exception.\n\n@param calendar the project calendar\n@param row the row\n@param row the row\n@param row the row\n@return the project calendar"
  },
  {
    "code": "private void processCalendarHours(ProjectCalendar calendar, Row row, int dayIndex)\n   {\n      Day day = Day.getInstance(dayIndex);\n      boolean working = row.getInt(\"CD_WORKING\") != 0;\n      calendar.setWorkingDay(day, working);\n      if (working == true)\n      {\n         ProjectCalendarHours hours = calendar.addCalendarHours(day);\n\n         Date start = row.getDate(\"CD_FROM_TIME1\");\n         Date end = row.getDate(\"CD_TO_TIME1\");\n         if (start != null && end != null)\n         {\n            hours.addRange(new DateRange(start, end));\n         }\n\n         start = row.getDate(\"CD_FROM_TIME2\");\n         end = row.getDate(\"CD_TO_TIME2\");\n         if (start != null && end != null)\n         {\n            hours.addRange(new DateRange(start, end));\n         }\n\n         start = row.getDate(\"CD_FROM_TIME3\");\n         end = row.getDate(\"CD_TO_TIME3\");\n         if (start != null && end != null)\n         {\n            hours.addRange(new DateRange(start, end));\n         }\n\n         start = row.getDate(\"CD_FROM_TIME4\");\n         end = row.getDate(\"CD_TO_TIME4\");\n         if (start != null && end != null)\n         {\n            hours.addRange(new DateRange(start, end));\n         }\n\n         start = row.getDate(\"CD_FROM_TIME5\");\n         end = row.getDate(\"CD_TO_TIME5\");\n         if (start != null && end != null)\n         {\n            hours.addRange(new DateRange(start, end));\n         }\n      }\n   }",
    "reference": "Process calendar hours.\n\n@param calendar parent calendar\n@param row calendar hours data\n@param dayIndex day index",
    "generated": "Process the calendar hours."
  },
  {
    "code": "protected void processResourceBaseline(Row row)\n   {\n      Integer id = row.getInteger(\"RES_UID\");\n      Resource resource = m_project.getResourceByUniqueID(id);\n      if (resource != null)\n      {\n         int index = row.getInt(\"RB_BASE_NUM\");\n\n         resource.setBaselineWork(index, row.getDuration(\"RB_BASE_WORK\"));\n         resource.setBaselineCost(index, row.getCurrency(\"RB_BASE_COST\"));\n      }\n   }",
    "reference": "Read resource baseline values.\n\n@param row result set row",
    "generated": "Process the resource baseline."
  },
  {
    "code": "protected void processDurationField(Row row)\n   {\n      processField(row, \"DUR_FIELD_ID\", \"DUR_REF_UID\", MPDUtility.getAdjustedDuration(m_project, row.getInt(\"DUR_VALUE\"), MPDUtility.getDurationTimeUnits(row.getInt(\"DUR_FMT\"))));\n   }",
    "reference": "Read a single duration field extended attribute.\n\n@param row field data",
    "generated": "Process the duration field."
  },
  {
    "code": "protected void processOutlineCodeField(Integer entityID, Row row)\n   {\n      processField(row, \"OC_FIELD_ID\", entityID, row.getString(\"OC_NAME\"));\n   }",
    "reference": "Read a single outline code field extended attribute.\n\n@param entityID parent entity\n@param row field data",
    "generated": "Process the outline code field."
  },
  {
    "code": "protected void processTaskBaseline(Row row)\n   {\n      Integer id = row.getInteger(\"TASK_UID\");\n      Task task = m_project.getTaskByUniqueID(id);\n      if (task != null)\n      {\n         int index = row.getInt(\"TB_BASE_NUM\");\n\n         task.setBaselineDuration(index, MPDUtility.getAdjustedDuration(m_project, row.getInt(\"TB_BASE_DUR\"), MPDUtility.getDurationTimeUnits(row.getInt(\"TB_BASE_DUR_FMT\"))));\n         task.setBaselineStart(index, row.getDate(\"TB_BASE_START\"));\n         task.setBaselineFinish(index, row.getDate(\"TB_BASE_FINISH\"));\n         task.setBaselineWork(index, row.getDuration(\"TB_BASE_WORK\"));\n         task.setBaselineCost(index, row.getCurrency(\"TB_BASE_COST\"));\n      }\n   }",
    "reference": "Read task baseline values.\n\n@param row result set row",
    "generated": "Process a task baseline.\n\n@param row the row to process\n@return the task baseline"
  },
  {
    "code": "protected void processLink(Row row)\n   {\n      Task predecessorTask = m_project.getTaskByUniqueID(row.getInteger(\"LINK_PRED_UID\"));\n      Task successorTask = m_project.getTaskByUniqueID(row.getInteger(\"LINK_SUCC_UID\"));\n      if (predecessorTask != null && successorTask != null)\n      {\n         RelationType type = RelationType.getInstance(row.getInt(\"LINK_TYPE\"));\n         TimeUnit durationUnits = MPDUtility.getDurationTimeUnits(row.getInt(\"LINK_LAG_FMT\"));\n         Duration duration = MPDUtility.getDuration(row.getDouble(\"LINK_LAG\").doubleValue(), durationUnits);\n         Relation relation = successorTask.addPredecessor(predecessorTask, type, duration);\n         relation.setUniqueID(row.getInteger(\"LINK_UID\"));\n         m_eventManager.fireRelationReadEvent(relation);\n      }\n   }",
    "reference": "Process a relationship between two tasks.\n\n@param row relationship data",
    "generated": "Process a link.\n\n@param row the row to process\n@return"
  },
  {
    "code": "protected void processAssignmentBaseline(Row row)\n   {\n      Integer id = row.getInteger(\"ASSN_UID\");\n      ResourceAssignment assignment = m_assignmentMap.get(id);\n      if (assignment != null)\n      {\n         int index = row.getInt(\"AB_BASE_NUM\");\n\n         assignment.setBaselineStart(index, row.getDate(\"AB_BASE_START\"));\n         assignment.setBaselineFinish(index, row.getDate(\"AB_BASE_FINISH\"));\n         assignment.setBaselineWork(index, row.getDuration(\"AB_BASE_WORK\"));\n         assignment.setBaselineCost(index, row.getCurrency(\"AB_BASE_COST\"));\n      }\n   }",
    "reference": "Read resource assignment baseline values.\n\n@param row result set row",
    "generated": "Process the baseline of the given row.\n\n@param row the row to process"
  },
  {
    "code": "protected void postProcessing()\n   {\n      //\n      // Update the internal structure. We'll take this opportunity to\n      // generate outline numbers for the tasks as they don't appear to\n      // be present in the MPP file.\n      //\n      ProjectConfig config = m_project.getProjectConfig();\n      config.setAutoWBS(m_autoWBS);\n      config.setAutoOutlineNumber(true);\n      m_project.updateStructure();\n      config.setAutoOutlineNumber(false);\n\n      //\n      // Perform post-processing to set the summary flag\n      //\n      for (Task task : m_project.getTasks())\n      {\n         task.setSummary(task.hasChildTasks());\n      }\n\n      //\n      // Ensure that the unique ID counters are correct\n      //\n      config.updateUniqueCounters();\n   }",
    "reference": "Carry out any post-processing required to tidy up\nthe data read from the database.",
    "generated": "Update the internal structure. We'll take this opportunity to\n      // generate outline numbers for the tasks as they don't appear to\n      // be present in the MPP file."
  },
  {
    "code": "private Integer getNullOnValue(Integer value, int nullValue)\n   {\n      return (NumberHelper.getInt(value) == nullValue ? null : value);\n   }",
    "reference": "This method returns the value it is passed, or null if the value\nmatches the nullValue argument.\n\n@param value value under test\n@param nullValue return null if value under test matches this value\n@return value or null",
    "generated": "Get the null value.\n\n@param value"
  },
  {
    "code": "public void process(DirectoryEntry projectDir, ProjectFile file, DocumentInputStreamFactory inputStreamFactory) throws IOException\n   {\n      DirectoryEntry consDir;\n      try\n      {\n         consDir = (DirectoryEntry) projectDir.getEntry(\"TBkndCons\");\n      }\n\n      catch (FileNotFoundException ex)\n      {\n         consDir = null;\n      }\n\n      if (consDir != null)\n      {\n         FixedMeta consFixedMeta = new FixedMeta(new DocumentInputStream(((DocumentEntry) consDir.getEntry(\"FixedMeta\"))), 10);\n         FixedData consFixedData = new FixedData(consFixedMeta, 20, inputStreamFactory.getInstance(consDir, \"FixedData\"));\n         //         FixedMeta consFixed2Meta = new FixedMeta(new DocumentInputStream(((DocumentEntry) consDir.getEntry(\"Fixed2Meta\"))), 9);\n         //         FixedData consFixed2Data = new FixedData(consFixed2Meta, 48, getEncryptableInputStream(consDir, \"Fixed2Data\"));\n\n         int count = consFixedMeta.getAdjustedItemCount();\n         int lastConstraintID = -1;\n\n         ProjectProperties properties = file.getProjectProperties();\n         EventManager eventManager = file.getEventManager();\n\n         boolean project15 = NumberHelper.getInt(properties.getMppFileType()) == 14 && NumberHelper.getInt(properties.getApplicationVersion()) > ApplicationVersion.PROJECT_2010;\n         int durationUnitsOffset = project15 ? 18 : 14;\n         int durationOffset = project15 ? 14 : 16;\n\n         for (int loop = 0; loop < count; loop++)\n         {\n            byte[] metaData = consFixedMeta.getByteArrayValue(loop);\n\n            //\n            // SourceForge bug 2209477: we were reading an int here, but\n            // it looks like the deleted flag is just a short.\n            //\n            if (MPPUtility.getShort(metaData, 0) != 0)\n            {\n               continue;\n            }\n\n            int index = consFixedData.getIndexFromOffset(MPPUtility.getInt(metaData, 4));\n            if (index == -1)\n            {\n               continue;\n            }\n\n            //\n            // Do we have enough data?\n            //\n            byte[] data = consFixedData.getByteArrayValue(index);\n            if (data.length < 14)\n            {\n               continue;\n            }\n\n            int constraintID = MPPUtility.getInt(data, 0);\n            if (constraintID <= lastConstraintID)\n            {\n               continue;\n            }\n\n            lastConstraintID = constraintID;\n            int taskID1 = MPPUtility.getInt(data, 4);\n            int taskID2 = MPPUtility.getInt(data, 8);\n\n            if (taskID1 == taskID2)\n            {\n               continue;\n            }\n\n            // byte[] metaData2 = consFixed2Meta.getByteArrayValue(loop);\n            // int index2 = consFixed2Data.getIndexFromOffset(MPPUtility.getInt(metaData2, 4));\n            // byte[] data2 = consFixed2Data.getByteArrayValue(index2);\n\n            Task task1 = file.getTaskByUniqueID(Integer.valueOf(taskID1));\n            Task task2 = file.getTaskByUniqueID(Integer.valueOf(taskID2));\n            if (task1 != null && task2 != null)\n            {\n               RelationType type = RelationType.getInstance(MPPUtility.getShort(data, 12));\n               TimeUnit durationUnits = MPPUtility.getDurationTimeUnits(MPPUtility.getShort(data, durationUnitsOffset));\n               Duration lag = MPPUtility.getAdjustedDuration(properties, MPPUtility.getInt(data, durationOffset), durationUnits);\n               Relation relation = task2.addPredecessor(task1, type, lag);\n               relation.setUniqueID(Integer.valueOf(constraintID));\n               eventManager.fireRelationReadEvent(relation);\n            }\n         }\n      }\n   }",
    "reference": "Main entry point when called to process constraint data.\n\n@param projectDir project directory\n@param file parent project file\n@param inputStreamFactory factory to create input stream",
    "generated": "Process a project file.\n\n@param projectDir project directory\n@param file project file\n@param inputStreamFactory"
  },
  {
    "code": "private Object getColumnValue(String table, String column, String data, int type, boolean epochDateFormat) throws MPXJException\n   {\n      try\n      {\n         Object value = null;\n\n         switch (type)\n         {\n            case Types.BIT:\n            {\n               value = DatatypeConverter.parseBoolean(data);\n               break;\n            }\n\n            case Types.VARCHAR:\n            case Types.LONGVARCHAR:\n            {\n               value = DatatypeConverter.parseString(data);\n               break;\n            }\n\n            case Types.TIME:\n            {\n               value = DatatypeConverter.parseBasicTime(data);\n               break;\n            }\n\n            case Types.TIMESTAMP:\n            {\n               if (epochDateFormat)\n               {\n                  value = DatatypeConverter.parseEpochTimestamp(data);\n               }\n               else\n               {\n                  value = DatatypeConverter.parseBasicTimestamp(data);\n               }\n               break;\n            }\n\n            case Types.DOUBLE:\n            {\n               value = DatatypeConverter.parseDouble(data);\n               break;\n            }\n\n            case Types.INTEGER:\n            {\n               value = DatatypeConverter.parseInteger(data);\n               break;\n            }\n\n            default:\n            {\n               throw new IllegalArgumentException(\"Unsupported SQL type: \" + type);\n            }\n         }\n\n         return value;\n      }\n\n      catch (Exception ex)\n      {\n         throw new MPXJException(\"Failed to parse \" + table + \".\" + column + \" (data=\" + data + \", type=\" + type + \")\", ex);\n      }\n   }",
    "reference": "Maps the text representation of column data to Java types.\n\n@param table table name\n@param column column name\n@param data text representation of column data\n@param type column data type\n@param epochDateFormat true if date is represented as an offset from an epoch\n@return Java representation of column data\n@throws MPXJException",
    "generated": "Parse a column value.\n\n@param table column name to parse\n@param data data to parse"
  },
  {
    "code": "public List<Callouts.Callout> getCallout()\n   {\n      if (callout == null)\n      {\n         callout = new ArrayList<Callouts.Callout>();\n      }\n      return this.callout;\n   }",
    "reference": "Gets the value of the callout property.\n\n<p>\nThis accessor method returns a reference to the live list,\nnot a snapshot. Therefore any modification you make to the\nreturned list will be present inside the JAXB object.\nThis is why there is not a <CODE>set</CODE> method for the callout property.\n\n<p>\nFor example, to add a new item, do as follows:\n<pre>\ngetCallout().add(newItem);\n</pre>\n\n\n<p>\nObjects of the following type(s) are allowed in the list\n{@link Callouts.Callout }",
    "generated": "Generate a list of Callouts.Callout"
  },
  {
    "code": "public static final Integer parseMinutesFromHours(String value)\n   {\n      Integer result = null;\n      if (value != null)\n      {\n         result = Integer.valueOf(Integer.parseInt(value) * 60);\n      }\n      return result;\n   }",
    "reference": "Parse a duration in minutes form a number of hours.\n\n@param value String representation\n@return Integer instance",
    "generated": "Parse the minutes from hours.\n\n@param value the value to parse\n@return the minutes value"
  },
  {
    "code": "public static final CurrencySymbolPosition parseCurrencySymbolPosition(String value)\n   {\n      CurrencySymbolPosition result = MAP_TO_CURRENCY_SYMBOL_POSITION.get(value);\n      result = result == null ? CurrencySymbolPosition.BEFORE_WITH_SPACE : result;\n      return result;\n   }",
    "reference": "Parse a currency symbol position from a string representation.\n\n@param value String representation\n@return CurrencySymbolPosition instance",
    "generated": "Parse the currency symbol position.\n\n@param value Currency symbol position"
  },
  {
    "code": "public static final Date parseDate(String value)\n   {\n      Date result = null;\n\n      try\n      {\n         if (value != null && !value.isEmpty())\n         {\n            result = DATE_FORMAT.get().parse(value);\n         }\n      }\n      catch (ParseException ex)\n      {\n         // Ignore\n      }\n\n      return result;\n   }",
    "reference": "Parse a date value.\n\n@param value String representation\n@return Date instance",
    "generated": "Parse a date.\n\n@param value Date value\n@return Date"
  },
  {
    "code": "public static final Date parseDateTime(String value)\n   {\n      Date result = null;\n\n      try\n      {\n         if (value != null && !value.isEmpty())\n         {\n            result = DATE_TIME_FORMAT.get().parse(value);\n         }\n      }\n      catch (ParseException ex)\n      {\n         // Ignore\n      }\n\n      return result;\n   }",
    "reference": "Parse a date time value.\n\n@param value String representation\n@return Date instance",
    "generated": "Parse a date value.\n\n@param value the date value\n@return Date"
  },
  {
    "code": "public static final int getInt(byte[] data, int offset)\n   {\n      int result = 0;\n      int i = offset;\n      for (int shiftBy = 0; shiftBy < 32; shiftBy += 8)\n      {\n         result |= ((data[i] & 0xff)) << shiftBy;\n         ++i;\n      }\n      return result;\n   }",
    "reference": "Read a four byte integer.\n\n@param data byte array\n@param offset offset into array\n@return integer value",
    "generated": "Get an integer from a byte array.\n\n@param data byte array"
  },
  {
    "code": "public static final int getShort(byte[] data, int offset)\n   {\n      int result = 0;\n      int i = offset;\n      for (int shiftBy = 0; shiftBy < 16; shiftBy += 8)\n      {\n         result |= ((data[i] & 0xff)) << shiftBy;\n         ++i;\n      }\n      return result;\n   }",
    "reference": "Read a two byte integer.\n\n@param data byte array\n@param offset offset into array\n@return integer value",
    "generated": "Get the short value from a byte array."
  },
  {
    "code": "public static final String getString(byte[] data, int offset)\n   {\n      return getString(data, offset, data.length - offset);\n   }",
    "reference": "Retrieve a string value.\n\n@param data byte array\n@param offset offset into byte array\n@return string value",
    "generated": "Generate a string from a byte array.\n\n@param data byte array\n@param offset offset"
  },
  {
    "code": "public static final Date getFinishDate(byte[] data, int offset)\n   {\n      Date result;\n      long days = getShort(data, offset);\n\n      if (days == 0x8000)\n      {\n         result = null;\n      }\n      else\n      {\n         result = DateHelper.getDateFromLong(EPOCH + ((days - 1) * DateHelper.MS_PER_DAY));\n      }\n\n      return (result);\n   }",
    "reference": "Retrieve a finish date.\n\n@param data byte array\n@param offset offset into byte array\n@return finish date",
    "generated": "Get the Finish date.\n\n@param data data data\n@param offset offset"
  },
  {
    "code": "public void setDefaultCalendarName(String calendarName)\n   {\n      if (calendarName == null || calendarName.length() == 0)\n      {\n         calendarName = DEFAULT_CALENDAR_NAME;\n      }\n\n      set(ProjectField.DEFAULT_CALENDAR_NAME, calendarName);\n   }",
    "reference": "Sets the Calendar used. 'Standard' if no value is set.\n\n@param calendarName Calendar name",
    "generated": "Set the default calendar name."
  },
  {
    "code": "public Date getStartDate()\n   {\n      Date result = (Date) getCachedValue(ProjectField.START_DATE);\n      if (result == null)\n      {\n         result = getParentFile().getStartDate();\n      }\n      return (result);\n   }",
    "reference": "Retrieves the project start date. If an explicit start date has not been\nset, this method calculates the start date by looking for\nthe earliest task start date.\n\n@return project start date",
    "generated": "Get the start date.\n\n@return start date"
  },
  {
    "code": "public Date getFinishDate()\n   {\n      Date result = (Date) getCachedValue(ProjectField.FINISH_DATE);\n      if (result == null)\n      {\n         result = getParentFile().getFinishDate();\n      }\n      return (result);\n   }",
    "reference": "Retrieves the project finish date. If an explicit finish date has not been\nset, this method calculates the finish date by looking for\nthe latest task finish date.\n\n@return Finish Date",
    "generated": "Get the finish date.\n\n@return the finish date"
  },
  {
    "code": "public void setCurrencySymbol(String symbol)\n   {\n      if (symbol == null)\n      {\n         symbol = DEFAULT_CURRENCY_SYMBOL;\n      }\n\n      set(ProjectField.CURRENCY_SYMBOL, symbol);\n   }",
    "reference": "Sets currency symbol.\n\n@param symbol currency symbol",
    "generated": "Set the currency symbol."
  },
  {
    "code": "public void setSymbolPosition(CurrencySymbolPosition posn)\n   {\n      if (posn == null)\n      {\n         posn = DEFAULT_CURRENCY_SYMBOL_POSITION;\n      }\n      set(ProjectField.CURRENCY_SYMBOL_POSITION, posn);\n   }",
    "reference": "Sets the position of the currency symbol.\n\n@param posn currency symbol position.",
    "generated": "Set the currency symbol position."
  },
  {
    "code": "public void setCurrencyDigits(Integer currDigs)\n   {\n      if (currDigs == null)\n      {\n         currDigs = DEFAULT_CURRENCY_DIGITS;\n      }\n      set(ProjectField.CURRENCY_DIGITS, currDigs);\n   }",
    "reference": "Sets no of currency digits.\n\n@param currDigs Available values, 0,1,2",
    "generated": "Set the currency digits."
  },
  {
    "code": "public Number getMinutesPerMonth()\n   {\n      return Integer.valueOf(NumberHelper.getInt(getMinutesPerDay()) * NumberHelper.getInt(getDaysPerMonth()));\n   }",
    "reference": "Retrieve the default number of minutes per month.\n\n@return minutes per month",
    "generated": "Get the minutes per month.\n\n@param minutesPerDay minutes per day"
  },
  {
    "code": "public Number getMinutesPerYear()\n   {\n      return Integer.valueOf(NumberHelper.getInt(getMinutesPerDay()) * NumberHelper.getInt(getDaysPerMonth()) * 12);\n   }",
    "reference": "Retrieve the default number of minutes per year.\n\n@return minutes per year",
    "generated": "Returns the minutes per year."
  },
  {
    "code": "@SuppressWarnings(\"unchecked\") public Map<String, Object> getCustomProperties()\n   {\n      return (Map<String, Object>) getCachedValue(ProjectField.CUSTOM_PROPERTIES);\n   }",
    "reference": "Retrieve a map of custom document properties.\n\n@return the Document Summary Information Map",
    "generated": "Returns the cached value for the project field.\n\n@param cachedValue cached value"
  },
  {
    "code": "private char getCachedCharValue(FieldType field, char defaultValue)\n   {\n      Character c = (Character) getCachedValue(field);\n      return c == null ? defaultValue : c.charValue();\n   }",
    "reference": "Handles retrieval of primitive char type.\n\n@param field required field\n@param defaultValue default value if field is missing\n@return char value",
    "generated": "Get the cached character value.\n\n@param field field\n@param defaultValue"
  },
  {
    "code": "private void set(FieldType field, boolean value)\n   {\n      set(field, (value ? Boolean.TRUE : Boolean.FALSE));\n   }",
    "reference": "This method inserts a name value pair into internal storage.\n\n@param field task field\n@param value attribute value",
    "generated": "Set the value of a field.\n\n@param field field to set\n@param value"
  },
  {
    "code": "private List<Row> getRows(String sql) throws SQLException\n   {\n      allocateConnection();\n\n      try\n      {\n         List<Row> result = new LinkedList<Row>();\n\n         m_ps = m_connection.prepareStatement(sql);\n         m_rs = m_ps.executeQuery();\n         populateMetaData();\n         while (m_rs.next())\n         {\n            result.add(new MpdResultSetRow(m_rs, m_meta));\n         }\n\n         return (result);\n      }\n\n      finally\n      {\n         releaseConnection();\n      }\n   }",
    "reference": "Retrieve a number of rows matching the supplied query.\n\n@param sql query statement\n@return result set\n@throws SQLException",
    "generated": "Returns a list of rows.\n\n@param sql the SQL statement.\n@return a list of rows"
  },
  {
    "code": "private void releaseConnection()\n   {\n      if (m_rs != null)\n      {\n         try\n         {\n            m_rs.close();\n         }\n\n         catch (SQLException ex)\n         {\n            // silently ignore errors on close\n         }\n\n         m_rs = null;\n      }\n\n      if (m_ps != null)\n      {\n         try\n         {\n            m_ps.close();\n         }\n\n         catch (SQLException ex)\n         {\n            // silently ignore errors on close\n         }\n\n         m_ps = null;\n      }\n   }",
    "reference": "Releases a database connection, and cleans up any resources\nassociated with that connection.",
    "generated": "Release the connection."
  },
  {
    "code": "private void populateMetaData() throws SQLException\n   {\n      m_meta.clear();\n\n      ResultSetMetaData meta = m_rs.getMetaData();\n      int columnCount = meta.getColumnCount() + 1;\n      for (int loop = 1; loop < columnCount; loop++)\n      {\n         String name = meta.getColumnName(loop);\n         Integer type = Integer.valueOf(meta.getColumnType(loop));\n         m_meta.put(name, type);\n      }\n   }",
    "reference": "Retrieves basic meta data from the result set.\n\n@throws SQLException",
    "generated": "Populate the meta data."
  },
  {
    "code": "public void setSchema(String schema)\n   {\n      if (schema.charAt(schema.length() - 1) != '.')\n      {\n         schema = schema + '.';\n      }\n      m_schema = schema;\n   }",
    "reference": "Set the name of the schema containing the schedule tables.\n\n@param schema schema name.",
    "generated": "Set the schema.\n\n@param schema"
  },
  {
    "code": "public ActivityCodeValue addValue(Integer uniqueID, String name, String description)\n   {\n      ActivityCodeValue value = new ActivityCodeValue(this, uniqueID, name, description);\n      m_values.add(value);\n      return value;\n   }",
    "reference": "Add a value to this activity code.\n\n@param uniqueID value unique ID\n@param name value name\n@param description value description\n@return ActivityCodeValue instance",
    "generated": "Create a new ActivityCodeValue.\n\n@param uniqueID the unique ID\n@param name the name\n@param description the description of the value\n@return the value"
  },
  {
    "code": "public static final long getLong(byte[] data, int offset)\n   {\n      long result = 0;\n      int i = offset;\n      for (int shiftBy = 0; shiftBy < 64; shiftBy += 8)\n      {\n         result |= ((long) (data[i] & 0xff)) << shiftBy;\n         ++i;\n      }\n      return result;\n   }",
    "reference": "Read a long int from a byte array.\n\n@param data byte array\n@param offset start offset\n@return long value",
    "generated": "Get a long from a byte array.\n\n@param data data byte array\n@param offset offset"
  },
  {
    "code": "public static final int getInt(InputStream is) throws IOException\n   {\n      byte[] data = new byte[4];\n      is.read(data);\n      return getInt(data, 0);\n   }",
    "reference": "Read an int from an input stream.\n\n@param is input stream\n@return int value",
    "generated": "Read an integer from the input stream.\n\n@param is the input stream\n@return integer value"
  },
  {
    "code": "public static final int getShort(InputStream is) throws IOException\n   {\n      byte[] data = new byte[2];\n      is.read(data);\n      return getShort(data, 0);\n   }",
    "reference": "Read a short int from an input stream.\n\n@param is input stream\n@return int value",
    "generated": "Read the short value.\n\n@param is the input stream\n@return short value"
  },
  {
    "code": "public static final long getLong(InputStream is) throws IOException\n   {\n      byte[] data = new byte[8];\n      is.read(data);\n      return getLong(data, 0);\n   }",
    "reference": "Read a long int from an input stream.\n\n@param is input stream\n@return long value",
    "generated": "Read a long from the input stream.\n\n@param is InputStream\n@return Long value"
  },
  {
    "code": "public static final String getString(InputStream is) throws IOException\n   {\n      int type = is.read();\n      if (type != 1)\n      {\n         throw new IllegalArgumentException(\"Unexpected string format\");\n      }\n\n      Charset charset = CharsetHelper.UTF8;\n      \n      int length = is.read();\n      if (length == 0xFF)\n      {\n         length = getShort(is);\n         if (length == 0xFFFE)\n         {\n            charset = CharsetHelper.UTF16LE;\n            length = (is.read() * 2);\n         }\n      }\n\n      String result;\n      if (length == 0)\n      {\n         result = null;\n      }\n      else\n      {\n         byte[] stringData = new byte[length];         \n         is.read(stringData);\n         result = new String(stringData, charset);\n      }\n      return result;\n   }",
    "reference": "Read a Synchro string from an input stream.\n\n@param is input stream\n@return String instance",
    "generated": "Generate a string from the input stream.\n\n@param is InputStream\n@return string"
  },
  {
    "code": "public static final UUID getUUID(InputStream is) throws IOException\n   {\n      byte[] data = new byte[16];\n      is.read(data);\n\n      long long1 = 0;\n      long1 |= ((long) (data[3] & 0xFF)) << 56;\n      long1 |= ((long) (data[2] & 0xFF)) << 48;\n      long1 |= ((long) (data[1] & 0xFF)) << 40;\n      long1 |= ((long) (data[0] & 0xFF)) << 32;\n      long1 |= ((long) (data[5] & 0xFF)) << 24;\n      long1 |= ((long) (data[4] & 0xFF)) << 16;\n      long1 |= ((long) (data[7] & 0xFF)) << 8;\n      long1 |= ((long) (data[6] & 0xFF)) << 0;\n\n      long long2 = 0;\n      long2 |= ((long) (data[8] & 0xFF)) << 56;\n      long2 |= ((long) (data[9] & 0xFF)) << 48;\n      long2 |= ((long) (data[10] & 0xFF)) << 40;\n      long2 |= ((long) (data[11] & 0xFF)) << 32;\n      long2 |= ((long) (data[12] & 0xFF)) << 24;\n      long2 |= ((long) (data[13] & 0xFF)) << 16;\n      long2 |= ((long) (data[14] & 0xFF)) << 8;\n      long2 |= ((long) (data[15] & 0xFF)) << 0;\n\n      return new UUID(long1, long2);\n   }",
    "reference": "Retrieve a UUID from an input stream.\n\n@param is input stream\n@return UUID instance",
    "generated": "Read the UUID from the input stream.\n\n@param is InputStream\n@return UUID"
  },
  {
    "code": "public static final Date getDate(InputStream is) throws IOException\n   {\n      long timeInSeconds = getInt(is);\n      if (timeInSeconds == 0x93406FFF)\n      {\n         return null;\n      }\n      timeInSeconds -= 3600;\n      timeInSeconds *= 1000;\n      return DateHelper.getDateFromLong(timeInSeconds);\n   }",
    "reference": "Read a Synchro date from an input stream.\n\n@param is input stream\n@return Date instance",
    "generated": "Get the date from the input stream.\n\n@param is InputStream\n@return Date"
  },
  {
    "code": "public static final Date getTime(InputStream is) throws IOException\n   {\n      int timeValue = getInt(is);\n      timeValue -= 86400;\n      timeValue /= 60;\n      return DateHelper.getTimeFromMinutesPastMidnight(Integer.valueOf(timeValue));\n   }",
    "reference": "Read a Synchro time from an input stream.\n\n@param is input stream\n@return Date instance",
    "generated": "Get the time from the input stream.\n\n@param is the input stream\n@return Date"
  },
  {
    "code": "public static final Duration getDuration(InputStream is) throws IOException\n   {\n      double durationInSeconds = getInt(is);\n      durationInSeconds /= (60 * 60);\n      return Duration.getInstance(durationInSeconds, TimeUnit.HOURS);\n   }",
    "reference": "Retrieve a Synchro Duration from an input stream.\n\n@param is input stream\n@return Duration instance",
    "generated": "Get the duration from the input stream.\n\n@param is input stream\n@return Duration instance"
  },
  {
    "code": "public static final Double getDouble(InputStream is) throws IOException\n   {\n      double result = Double.longBitsToDouble(getLong(is));\n      if (Double.isNaN(result))\n      {\n         result = 0;\n      }\n      return Double.valueOf(result);\n   }",
    "reference": "Retrieve a Double from an input stream.\n\n@param is input stream\n@return Double instance",
    "generated": "Get a double from the input stream.\n\n@param is InputStream\n@return Double.value"
  },
  {
    "code": "public void process()\n   {\n      if (m_data != null)\n      {\n         int index = 0;\n         int offset = 0;\n         // First the length (repeated twice)\n         int length = MPPUtility.getInt(m_data, offset);\n         offset += 8;\n         // Then the number of custom columns\n         int numberOfAliases = MPPUtility.getInt(m_data, offset);\n         offset += 4;\n\n         // Then the aliases themselves\n         while (index < numberOfAliases && offset < length)\n         {\n            // Each item consists of the Field ID (2 bytes), 40 0B marker (2 bytes), and the\n            // offset to the string (4 bytes)\n\n            // Get the Field ID\n            int fieldID = MPPUtility.getInt(m_data, offset);\n            offset += 4;\n            // Get the alias offset (offset + 4 for some reason).\n            int aliasOffset = MPPUtility.getInt(m_data, offset) + 4;\n            offset += 4;\n            // Read the alias itself\n            if (aliasOffset < m_data.length)\n            {\n               String alias = MPPUtility.getUnicodeString(m_data, aliasOffset);\n               m_fields.getCustomField(FieldTypeHelper.getInstance(fieldID)).setAlias(alias);\n            }\n            index++;\n         }\n      }\n   }",
    "reference": "Process field aliases.",
    "generated": "Process the data.\n\n@param m_data data data\n@param index the index of the data\n@param offset the offset to the string"
  },
  {
    "code": "public List<ProjectListType.Project> getProject()\n   {\n      if (project == null)\n      {\n         project = new ArrayList<ProjectListType.Project>();\n      }\n      return this.project;\n   }",
    "reference": "Gets the value of the project property.\n\n<p>\nThis accessor method returns a reference to the live list,\nnot a snapshot. Therefore any modification you make to the\nreturned list will be present inside the JAXB object.\nThis is why there is not a <CODE>set</CODE> method for the project property.\n\n<p>\nFor example, to add a new item, do as follows:\n<pre>\ngetProject().add(newItem);\n</pre>\n\n\n<p>\nObjects of the following type(s) are allowed in the list\n{@link ProjectListType.Project }",
    "generated": "Generate a list of ProjectListType.Project objects.\n\n@param project project"
  },
  {
    "code": "public static final String printExtendedAttributeCurrency(Number value)\n   {\n      return (value == null ? null : NUMBER_FORMAT.get().format(value.doubleValue() * 100));\n   }",
    "reference": "Print an extended attribute currency value.\n\n@param value currency value\n@return string representation",
    "generated": "Print the extended attribute currency."
  },
  {
    "code": "public static final Number parseExtendedAttributeCurrency(String value)\n   {\n      Number result;\n\n      if (value == null)\n      {\n         result = null;\n      }\n      else\n      {\n         result = NumberHelper.getDouble(Double.parseDouble(correctNumberFormat(value)) / 100);\n      }\n      return result;\n   }",
    "reference": "Parse an extended attribute currency value.\n\n@param value string representation\n@return currency value",
    "generated": "Parse the extended attribute currency value.\n\n@param value Extended attribute currency value\n@return Extended attribute currency value"
  },
  {
    "code": "public static final Boolean parseExtendedAttributeBoolean(String value)\n   {\n      return ((value.equals(\"1\") ? Boolean.TRUE : Boolean.FALSE));\n   }",
    "reference": "Parse an extended attribute boolean value.\n\n@param value string representation\n@return boolean value",
    "generated": "Parse an extended attribute boolean value.\n\n@param value attribute value\n@return Boolean value"
  },
  {
    "code": "public static final String printExtendedAttributeDate(Date value)\n   {\n      return (value == null ? null : DATE_FORMAT.get().format(value));\n   }",
    "reference": "Print an extended attribute date value.\n\n@param value date value\n@return string representation",
    "generated": "Print the extended attribute date."
  },
  {
    "code": "public static final Date parseExtendedAttributeDate(String value)\n   {\n      Date result = null;\n\n      if (value != null)\n      {\n         try\n         {\n            result = DATE_FORMAT.get().parse(value);\n         }\n\n         catch (ParseException ex)\n         {\n            // ignore exceptions\n         }\n      }\n\n      return (result);\n   }",
    "reference": "Parse an extended attribute date value.\n\n@param value string representation\n@return date value",
    "generated": "Parse an extended attribute date.\n\n@param value attribute value\n@return Date"
  },
  {
    "code": "public static final String printExtendedAttribute(MSPDIWriter writer, Object value, DataType type)\n   {\n      String result;\n\n      if (type == DataType.DATE)\n      {\n         result = printExtendedAttributeDate((Date) value);\n      }\n      else\n      {\n         if (value instanceof Boolean)\n         {\n            result = printExtendedAttributeBoolean((Boolean) value);\n         }\n         else\n         {\n            if (value instanceof Duration)\n            {\n               result = printDuration(writer, (Duration) value);\n            }\n            else\n            {\n               if (type == DataType.CURRENCY)\n               {\n                  result = printExtendedAttributeCurrency((Number) value);\n               }\n               else\n               {\n                  if (value instanceof Number)\n                  {\n                     result = printExtendedAttributeNumber((Number) value);\n                  }\n                  else\n                  {\n                     result = value.toString();\n                  }\n               }\n            }\n         }\n      }\n\n      return (result);\n   }",
    "reference": "Print an extended attribute value.\n\n@param writer parent MSPDIWriter instance\n@param value attribute value\n@param type type of the value being passed\n@return string representation",
    "generated": "Print an extended attribute value.\n\n@param writer the MSPDIWriter instance\n@param value the attribute value\n@return the formatted value"
  },
  {
    "code": "public static final void parseExtendedAttribute(ProjectFile file, FieldContainer mpx, String value, FieldType mpxFieldID, TimeUnit durationFormat)\n   {\n      if (mpxFieldID != null)\n      {\n         switch (mpxFieldID.getDataType())\n         {\n            case STRING:\n            {\n               mpx.set(mpxFieldID, value);\n               break;\n            }\n\n            case DATE:\n            {\n               mpx.set(mpxFieldID, parseExtendedAttributeDate(value));\n               break;\n            }\n\n            case CURRENCY:\n            {\n               mpx.set(mpxFieldID, parseExtendedAttributeCurrency(value));\n               break;\n            }\n\n            case BOOLEAN:\n            {\n               mpx.set(mpxFieldID, parseExtendedAttributeBoolean(value));\n               break;\n            }\n\n            case NUMERIC:\n            {\n               mpx.set(mpxFieldID, parseExtendedAttributeNumber(value));\n               break;\n            }\n\n            case DURATION:\n            {\n               mpx.set(mpxFieldID, parseDuration(file, durationFormat, value));\n               break;\n            }\n\n            default:\n            {\n               break;\n            }\n         }\n      }\n   }",
    "reference": "Parse an extended attribute value.\n\n@param file parent file\n@param mpx parent entity\n@param value string value\n@param mpxFieldID field ID\n@param durationFormat duration format associated with the extended attribute",
    "generated": "Parse an extended attribute value.\n\n@param mpx the project file\n@param durationFormat the duration format\n@param value the extended attribute value\n@param mpxFieldID the field to parse\n@param durationFormat the duration format\n@return null"
  },
  {
    "code": "public static final String printCurrencySymbolPosition(CurrencySymbolPosition value)\n   {\n      String result;\n\n      switch (value)\n      {\n         default:\n         case BEFORE:\n         {\n            result = \"0\";\n            break;\n         }\n\n         case AFTER:\n         {\n            result = \"1\";\n            break;\n         }\n\n         case BEFORE_WITH_SPACE:\n         {\n            result = \"2\";\n            break;\n         }\n\n         case AFTER_WITH_SPACE:\n         {\n            result = \"3\";\n            break;\n         }\n      }\n\n      return (result);\n   }",
    "reference": "Prints a currency symbol position value.\n\n@param value CurrencySymbolPosition instance\n@return currency symbol position",
    "generated": "Print currency symbol position.\n\n@param value Currency symbol position"
  },
  {
    "code": "public static final CurrencySymbolPosition parseCurrencySymbolPosition(String value)\n   {\n      CurrencySymbolPosition result = CurrencySymbolPosition.BEFORE;\n\n      switch (NumberHelper.getInt(value))\n      {\n         case 0:\n         {\n            result = CurrencySymbolPosition.BEFORE;\n            break;\n         }\n\n         case 1:\n         {\n            result = CurrencySymbolPosition.AFTER;\n            break;\n         }\n\n         case 2:\n         {\n            result = CurrencySymbolPosition.BEFORE_WITH_SPACE;\n            break;\n         }\n\n         case 3:\n         {\n            result = CurrencySymbolPosition.AFTER_WITH_SPACE;\n            break;\n         }\n      }\n\n      return (result);\n   }",
    "reference": "Parse a currency symbol position value.\n\n@param value currency symbol position\n@return CurrencySymbolPosition instance",
    "generated": "Parse the currency symbol position.\n\n@param value Currency symbol value"
  },
  {
    "code": "public static final String printAccrueType(AccrueType value)\n   {\n      return (Integer.toString(value == null ? AccrueType.PRORATED.getValue() : value.getValue()));\n   }",
    "reference": "Print an accrue type.\n\n@param value AccrueType instance\n@return accrue type value",
    "generated": "Print a Accrue type.\n\n@param value AccrueType value"
  },
  {
    "code": "public static final String printResourceType(ResourceType value)\n   {\n      return (Integer.toString(value == null ? ResourceType.WORK.getValue() : value.getValue()));\n   }",
    "reference": "Print a resource type.\n\n@param value ResourceType instance\n@return resource type value",
    "generated": "Print a ResourceType value.\n\n@param value ResourceType value"
  },
  {
    "code": "public static final String printWorkGroup(WorkGroup value)\n   {\n      return (Integer.toString(value == null ? WorkGroup.DEFAULT.getValue() : value.getValue()));\n   }",
    "reference": "Print a work group.\n\n@param value WorkGroup instance\n@return work group value",
    "generated": "Print a work group.\n\n@param value WorkGroup value"
  },
  {
    "code": "public static final String printWorkContour(WorkContour value)\n   {\n      return (Integer.toString(value == null ? WorkContour.FLAT.getValue() : value.getValue()));\n   }",
    "reference": "Print a work contour.\n\n@param value WorkContour instance\n@return work contour value",
    "generated": "Print the WorkContour value.\n\n@param value WorkContour"
  },
  {
    "code": "public static final String printBookingType(BookingType value)\n   {\n      return (Integer.toString(value == null ? BookingType.COMMITTED.getValue() : value.getValue()));\n   }",
    "reference": "Print a booking type.\n\n@param value BookingType instance\n@return booking type value",
    "generated": "Print a booking type.\n\n@param value BookingType"
  },
  {
    "code": "public static final String printTaskType(TaskType value)\n   {\n      return (Integer.toString(value == null ? TaskType.FIXED_UNITS.getValue() : value.getValue()));\n   }",
    "reference": "Print a task type.\n\n@param value TaskType instance\n@return task type value",
    "generated": "Print the task type."
  },
  {
    "code": "public static final BigInteger printEarnedValueMethod(EarnedValueMethod value)\n   {\n      return (value == null ? BigInteger.valueOf(EarnedValueMethod.PERCENT_COMPLETE.getValue()) : BigInteger.valueOf(value.getValue()));\n   }",
    "reference": "Print an earned value method.\n\n@param value EarnedValueMethod instance\n@return earned value method value",
    "generated": "Print the Earned value method.\n\n@param value EarnedValueMethod"
  },
  {
    "code": "public static final BigDecimal printUnits(Number value)\n   {\n      return (value == null ? BIGDECIMAL_ONE : new BigDecimal(value.doubleValue() / 100));\n   }",
    "reference": "Print units.\n\n@param value units value\n@return units value",
    "generated": "Print a number of units."
  },
  {
    "code": "public static final Number parseUnits(Number value)\n   {\n      return (value == null ? null : NumberHelper.getDouble(value.doubleValue() * 100));\n   }",
    "reference": "Parse units.\n\n@param value units value\n@return units value",
    "generated": "Parse units.\n\n@param value the unit value\n@return the unit value"
  },
  {
    "code": "public static final BigInteger printTimeUnit(TimeUnit value)\n   {\n      return (BigInteger.valueOf(value == null ? TimeUnit.DAYS.getValue() + 1 : value.getValue() + 1));\n   }",
    "reference": "Print time unit.\n\n@param value TimeUnit instance\n@return time unit value",
    "generated": "Print a time unit.\n\n@param value TimeUnit value"
  },
  {
    "code": "public static final TimeUnit parseWorkUnits(BigInteger value)\n   {\n      TimeUnit result = TimeUnit.HOURS;\n\n      if (value != null)\n      {\n         switch (value.intValue())\n         {\n            case 1:\n            {\n               result = TimeUnit.MINUTES;\n               break;\n            }\n\n            case 3:\n            {\n               result = TimeUnit.DAYS;\n               break;\n            }\n\n            case 4:\n            {\n               result = TimeUnit.WEEKS;\n               break;\n            }\n\n            case 5:\n            {\n               result = TimeUnit.MONTHS;\n               break;\n            }\n\n            case 7:\n            {\n               result = TimeUnit.YEARS;\n               break;\n            }\n\n            default:\n            case 2:\n            {\n               result = TimeUnit.HOURS;\n               break;\n            }\n         }\n      }\n\n      return (result);\n   }",
    "reference": "Parse work units.\n\n@param value work units value\n@return TimeUnit instance",
    "generated": "Parse the work units.\n\n@param value the value to parse"
  },
  {
    "code": "public static final BigInteger printWorkUnits(TimeUnit value)\n   {\n      int result;\n\n      if (value == null)\n      {\n         value = TimeUnit.HOURS;\n      }\n\n      switch (value)\n      {\n         case MINUTES:\n         {\n            result = 1;\n            break;\n         }\n\n         case DAYS:\n         {\n            result = 3;\n            break;\n         }\n\n         case WEEKS:\n         {\n            result = 4;\n            break;\n         }\n\n         case MONTHS:\n         {\n            result = 5;\n            break;\n         }\n\n         case YEARS:\n         {\n            result = 7;\n            break;\n         }\n\n         default:\n         case HOURS:\n         {\n            result = 2;\n            break;\n         }\n      }\n\n      return (BigInteger.valueOf(result));\n   }",
    "reference": "Print work units.\n\n@param value TimeUnit instance\n@return work units value",
    "generated": "Print work units.\n\n@param value TimeUnit"
  },
  {
    "code": "public static final Double parseCurrency(Number value)\n   {\n      return (value == null ? null : NumberHelper.getDouble(value.doubleValue() / 100));\n   }",
    "reference": "Parse currency.\n\n@param value currency value\n@return currency value",
    "generated": "Parse a currency value.\n\n@param value Number to parse\n@return null"
  },
  {
    "code": "public static final BigDecimal printCurrency(Number value)\n   {\n      return (value == null || value.doubleValue() == 0 ? null : new BigDecimal(value.doubleValue() * 100));\n   }",
    "reference": "Print currency.\n\n@param value currency value\n@return currency value",
    "generated": "Print a currency value.\n\n@param value BigDecimal"
  },
  {
    "code": "public static final TimeUnit parseDurationTimeUnits(BigInteger value, TimeUnit defaultValue)\n   {\n      TimeUnit result = defaultValue;\n\n      if (value != null)\n      {\n         switch (value.intValue())\n         {\n            case 3:\n            case 35:\n            {\n               result = TimeUnit.MINUTES;\n               break;\n            }\n\n            case 4:\n            case 36:\n            {\n               result = TimeUnit.ELAPSED_MINUTES;\n               break;\n            }\n\n            case 5:\n            case 37:\n            {\n               result = TimeUnit.HOURS;\n               break;\n            }\n\n            case 6:\n            case 38:\n            {\n               result = TimeUnit.ELAPSED_HOURS;\n               break;\n            }\n\n            case 7:\n            case 39:\n            case 53:\n            {\n               result = TimeUnit.DAYS;\n               break;\n            }\n\n            case 8:\n            case 40:\n            {\n               result = TimeUnit.ELAPSED_DAYS;\n               break;\n            }\n\n            case 9:\n            case 41:\n            {\n               result = TimeUnit.WEEKS;\n               break;\n            }\n\n            case 10:\n            case 42:\n            {\n               result = TimeUnit.ELAPSED_WEEKS;\n               break;\n            }\n\n            case 11:\n            case 43:\n            {\n               result = TimeUnit.MONTHS;\n               break;\n            }\n\n            case 12:\n            case 44:\n            {\n               result = TimeUnit.ELAPSED_MONTHS;\n               break;\n            }\n\n            case 19:\n            case 51:\n            {\n               result = TimeUnit.PERCENT;\n               break;\n            }\n\n            case 20:\n            case 52:\n            {\n               result = TimeUnit.ELAPSED_PERCENT;\n               break;\n            }\n\n            default:\n            {\n               result = PARENT_FILE.get().getProjectProperties().getDefaultDurationUnits();\n               break;\n            }\n         }\n      }\n\n      return (result);\n   }",
    "reference": "Parse duration time units.\n\nNote that we don't differentiate between confirmed and unconfirmed\ndurations. Unrecognised duration types are default the supplied default value.\n\n@param value BigInteger value\n@param defaultValue if value is null, use this value as the result\n@return Duration units",
    "generated": "Parse a duration time unit value.\n\n@param value the duration unit value\n@return the duration unit value"
  },
  {
    "code": "public static final Priority parsePriority(BigInteger priority)\n   {\n      return (priority == null ? null : Priority.getInstance(priority.intValue()));\n   }",
    "reference": "Parse priority.\n\n\n@param priority priority value\n@return Priority instance",
    "generated": "Parse a priority object.\n\n@param priority the priority to parse\n@return Priority object"
  },
  {
    "code": "public static final BigInteger printPriority(Priority priority)\n   {\n      int result = Priority.MEDIUM;\n\n      if (priority != null)\n      {\n         result = priority.getValue();\n      }\n\n      return (BigInteger.valueOf(result));\n   }",
    "reference": "Print priority.\n\n@param priority Priority instance\n@return priority value",
    "generated": "Print a priority value.\n\n@param priority Priority object\n@return priority value"
  },
  {
    "code": "public static final Duration parseDurationInThousanthsOfMinutes(ProjectProperties properties, Number value, TimeUnit targetTimeUnit)\n   {\n      return parseDurationInFractionsOfMinutes(properties, value, targetTimeUnit, 1000);\n   }",
    "reference": "Parse duration represented in thousandths of minutes.\n\n@param properties project properties\n@param value duration value\n@param targetTimeUnit required output time units\n@return Duration instance",
    "generated": "Parse a duration in thousanths of minutes."
  },
  {
    "code": "public static final BigDecimal printDurationInDecimalThousandthsOfMinutes(Duration duration)\n   {\n      BigDecimal result = null;\n      if (duration != null && duration.getDuration() != 0)\n      {\n         result = BigDecimal.valueOf(printDurationFractionsOfMinutes(duration, 1000));\n      }\n      return result;\n   }",
    "reference": "Print duration in thousandths of minutes.\n\n@param duration Duration instance\n@return duration in thousandths of minutes",
    "generated": "Print duration in decimal thousandths of minutes.\n\n@param duration duration"
  },
  {
    "code": "public static final BigInteger printDurationInIntegerTenthsOfMinutes(Duration duration)\n   {\n      BigInteger result = null;\n\n      if (duration != null && duration.getDuration() != 0)\n      {\n         result = BigInteger.valueOf((long) printDurationFractionsOfMinutes(duration, 10));\n      }\n\n      return result;\n   }",
    "reference": "Print duration in tenths of minutes.\n\n@param duration Duration instance\n@return duration in tenths of minutes",
    "generated": "Print duration in integer tenths of minutes.\n\n@param duration duration"
  },
  {
    "code": "public static final UUID parseUUID(String value)\n   {\n      return value == null || value.isEmpty() ? null : UUID.fromString(value);\n   }",
    "reference": "Convert the MSPDI representation of a UUID into a Java UUID instance.\n\n@param value MSPDI UUID\n@return Java UUID instance",
    "generated": "Parse a UUID value.\n\n@param value the UUID value\n@return UUID.fromString(value)"
  },
  {
    "code": "private static final Duration parseDurationInFractionsOfMinutes(ProjectProperties properties, Number value, TimeUnit targetTimeUnit, int factor)\n   {\n      Duration result = null;\n\n      if (value != null)\n      {\n         result = Duration.getInstance(value.intValue() / factor, TimeUnit.MINUTES);\n         if (targetTimeUnit != result.getUnits())\n         {\n            result = result.convertUnits(targetTimeUnit, properties);\n         }\n      }\n\n      return (result);\n   }",
    "reference": "Parse duration represented as an arbitrary fraction of minutes.\n\n@param properties project properties\n@param value duration value\n@param targetTimeUnit required output time units\n@param factor required fraction of a minute\n@return Duration instance",
    "generated": "Parse a duration in fractions of minutes."
  },
  {
    "code": "private static final double printDurationFractionsOfMinutes(Duration duration, int factor)\n   {\n      double result = 0;\n\n      if (duration != null)\n      {\n         result = duration.getDuration();\n\n         switch (duration.getUnits())\n         {\n            case HOURS:\n            case ELAPSED_HOURS:\n            {\n               result *= 60;\n               break;\n            }\n\n            case DAYS:\n            {\n               result *= (60 * 8);\n               break;\n            }\n\n            case ELAPSED_DAYS:\n            {\n               result *= (60 * 24);\n               break;\n            }\n\n            case WEEKS:\n            {\n               result *= (60 * 8 * 5);\n               break;\n            }\n\n            case ELAPSED_WEEKS:\n            {\n               result *= (60 * 24 * 7);\n               break;\n            }\n\n            case MONTHS:\n            {\n               result *= (60 * 8 * 5 * 4);\n               break;\n            }\n\n            case ELAPSED_MONTHS:\n            {\n               result *= (60 * 24 * 30);\n               break;\n            }\n\n            case YEARS:\n            {\n               result *= (60 * 8 * 5 * 52);\n               break;\n            }\n\n            case ELAPSED_YEARS:\n            {\n               result *= (60 * 24 * 365);\n               break;\n            }\n\n            default:\n            {\n               break;\n            }\n         }\n      }\n\n      result *= factor;\n\n      return (result);\n   }",
    "reference": "Print a duration represented by an arbitrary fraction of minutes.\n\n@param duration Duration instance\n@param factor required factor\n@return duration represented as an arbitrary fraction of minutes",
    "generated": "Print duration fractions of minutes."
  },
  {
    "code": "public static final BigDecimal printRate(Rate rate)\n   {\n      BigDecimal result = null;\n      if (rate != null && rate.getAmount() != 0)\n      {\n         result = new BigDecimal(rate.getAmount());\n      }\n      return result;\n   }",
    "reference": "Print rate.\n\n@param rate Rate instance\n@return rate value",
    "generated": "Print a rate."
  },
  {
    "code": "public static final Rate parseRate(BigDecimal value)\n   {\n      Rate result = null;\n\n      if (value != null)\n      {\n         result = new Rate(value, TimeUnit.HOURS);\n      }\n\n      return (result);\n   }",
    "reference": "Parse rate.\n\n@param value rate value\n@return Rate instance",
    "generated": "Parse a Rate object.\n\n@param value BigDecimal value\n@return Rate object"
  },
  {
    "code": "public static final BigInteger printDay(Day day)\n   {\n      return (day == null ? null : BigInteger.valueOf(day.getValue() - 1));\n   }",
    "reference": "Print a day.\n\n@param day Day instance\n@return day value",
    "generated": "Print a day."
  },
  {
    "code": "public static final BigInteger printConstraintType(ConstraintType value)\n   {\n      return (value == null ? null : BigInteger.valueOf(value.getValue()));\n   }",
    "reference": "Print a constraint type.\n\n@param value ConstraintType instance\n@return constraint type value",
    "generated": "Print a constraint type.\n\n@param value constraint type"
  },
  {
    "code": "public static final String printTaskUID(Integer value)\n   {\n      ProjectFile file = PARENT_FILE.get();\n      if (file != null)\n      {\n         file.getEventManager().fireTaskWrittenEvent(file.getTaskByUniqueID(value));\n      }\n      return (value.toString());\n   }",
    "reference": "Print a task UID.\n\n@param value task UID\n@return task UID string",
    "generated": "Print a task UID.\n\n@param value task UID"
  },
  {
    "code": "public static final String printResourceUID(Integer value)\n   {\n      ProjectFile file = PARENT_FILE.get();\n      if (file != null)\n      {\n         file.getEventManager().fireResourceWrittenEvent(file.getResourceByUniqueID(value));\n      }\n      return (value.toString());\n   }",
    "reference": "Print a resource UID.\n\n@param value resource UID value\n@return resource UID string",
    "generated": "Print a resource UID.\n\n@param value the UID value\n@return the UID value"
  },
  {
    "code": "public static final Boolean parseBoolean(String value)\n   {\n      return (value == null || value.charAt(0) != '1' ? Boolean.FALSE : Boolean.TRUE);\n   }",
    "reference": "Parse a boolean.\n\n@param value boolean\n@return Boolean value",
    "generated": "Parse a boolean value.\n\n@param value"
  },
  {
    "code": "public static final String printDateTime(Date value)\n   {\n      return (value == null ? null : DATE_FORMAT.get().format(value));\n   }",
    "reference": "Print a date time value.\n\n@param value date time value\n@return string representation",
    "generated": "Print a DateTime object.\n\n@param value DateTime object\n@return DateTime object"
  },
  {
    "code": "private static final String correctNumberFormat(String value)\n   {\n      String result;\n      int index = value.indexOf(',');\n      if (index == -1)\n      {\n         result = value;\n      }\n      else\n      {\n         char[] chars = value.toCharArray();\n         chars[index] = '.';\n         result = new String(chars);\n      }\n      return result;\n   }",
    "reference": "Detect numbers using comma as a decimal separator and replace with period.\n\n@param value original numeric value\n@return corrected numeric value",
    "generated": "Generate a correct number format.\n\n@param value"
  },
  {
    "code": "public static final ProjectFile setProjectNameAndRead(File directory) throws MPXJException\n   {\n      List<String> projects = listProjectNames(directory);\n\n      if (!projects.isEmpty())\n      {\n         P3DatabaseReader reader = new P3DatabaseReader();\n         reader.setProjectName(projects.get(0));\n         return reader.read(directory);\n      }\n\n      return null;\n   }",
    "reference": "Convenience method which locates the first P3 database in a directory\nand opens it.\n\n@param directory directory containing a P3 database\n@return ProjectFile instance",
    "generated": "Read a project file.\n\n@param directory Project file\n@return project file"
  },
  {
    "code": "public static final List<String> listProjectNames(File directory)\n   {\n      List<String> result = new ArrayList<String>();\n\n      File[] files = directory.listFiles(new FilenameFilter()\n      {\n         @Override public boolean accept(File dir, String name)\n         {\n            return name.toUpperCase().endsWith(\"STR.P3\");\n         }\n      });\n\n      if (files != null)\n      {\n         for (File file : files)\n         {\n            String fileName = file.getName();\n            String prefix = fileName.substring(0, fileName.length() - 6);\n            result.add(prefix);\n         }\n      }\n\n      Collections.sort(result);\n\n      return result;\n   }",
    "reference": "Retrieve a list of the available P3 project names from a directory.\n\n@param directory directory containing P3 files\n@return list of project names",
    "generated": "List the project names.\n\n@param directory File\n@return list of project names"
  },
  {
    "code": "private void readProjectHeader()\n   {\n      Table table = m_tables.get(\"DIR\");\n      MapRow row = table.find(\"\");\n      if (row != null)\n      {\n         setFields(PROJECT_FIELDS, row, m_projectFile.getProjectProperties());\n         m_wbsFormat = new P3WbsFormat(row);\n      }\n   }",
    "reference": "Read general project properties.",
    "generated": "Read the project header."
  },
  {
    "code": "private void readWBS()\n   {\n      Map<Integer, List<MapRow>> levelMap = new HashMap<Integer, List<MapRow>>();\n      for (MapRow row : m_tables.get(\"STR\"))\n      {\n         Integer level = row.getInteger(\"LEVEL_NUMBER\");\n         List<MapRow> items = levelMap.get(level);\n         if (items == null)\n         {\n            items = new ArrayList<MapRow>();\n            levelMap.put(level, items);\n         }\n         items.add(row);\n      }\n\n      int level = 1;\n      while (true)\n      {\n         List<MapRow> items = levelMap.get(Integer.valueOf(level++));\n         if (items == null)\n         {\n            break;\n         }\n\n         for (MapRow row : items)\n         {\n            m_wbsFormat.parseRawValue(row.getString(\"CODE_VALUE\"));\n            String parentWbsValue = m_wbsFormat.getFormattedParentValue();\n            String wbsValue = m_wbsFormat.getFormattedValue();\n            row.setObject(\"WBS\", wbsValue);\n            row.setObject(\"PARENT_WBS\", parentWbsValue);\n         }\n\n         final AlphanumComparator comparator = new AlphanumComparator();\n         Collections.sort(items, new Comparator<MapRow>()\n         {\n            @Override public int compare(MapRow o1, MapRow o2)\n            {\n               return comparator.compare(o1.getString(\"WBS\"), o2.getString(\"WBS\"));\n            }\n         });\n\n         for (MapRow row : items)\n         {\n            String wbs = row.getString(\"WBS\");\n            if (wbs != null && !wbs.isEmpty())\n            {\n               ChildTaskContainer parent = m_wbsMap.get(row.getString(\"PARENT_WBS\"));\n               if (parent == null)\n               {\n                  parent = m_projectFile;\n               }\n\n               Task task = parent.addTask();\n               String name = row.getString(\"CODE_TITLE\");\n               if (name == null || name.isEmpty())\n               {\n                  name = wbs;\n               }\n               task.setName(name);\n               task.setWBS(wbs);\n               task.setSummary(true);\n               m_wbsMap.put(wbs, task);\n            }\n         }\n      }\n   }",
    "reference": "Read tasks representing the WBS.",
    "generated": "Read the WBS data.\n\n@param m_tables the table to read\n@param m_tables the table to read\n@param m_tables the table to read\n@param m_tables the table to read\n@param m_wbsFormat the wbsFormat\n@param m_wbsFormat the wbsFormat\n@param m_wbsFormat the wbsFormat\n@param m_wbsFormat the wbsFormat\n@return"
  },
  {
    "code": "private void readRelationships()\n   {\n      for (MapRow row : m_tables.get(\"REL\"))\n      {\n         Task predecessor = m_activityMap.get(row.getString(\"PREDECESSOR_ACTIVITY_ID\"));\n         Task successor = m_activityMap.get(row.getString(\"SUCCESSOR_ACTIVITY_ID\"));\n         if (predecessor != null && successor != null)\n         {\n            Duration lag = row.getDuration(\"LAG_VALUE\");\n            RelationType type = row.getRelationType(\"LAG_TYPE\");\n\n            successor.addPredecessor(predecessor, type, lag);\n         }\n      }\n   }",
    "reference": "Read task relationships.",
    "generated": "Read the relationships from the table table."
  },
  {
    "code": "private void setFields(Map<String, FieldType> map, MapRow row, FieldContainer container)\n   {\n      if (row != null)\n      {\n         for (Map.Entry<String, FieldType> entry : map.entrySet())\n         {\n            container.set(entry.getValue(), row.getObject(entry.getKey()));\n         }\n      }\n   }",
    "reference": "Set the value of one or more fields based on the contents of a database row.\n\n@param map column to field map\n@param row database row\n@param container field container",
    "generated": "Set the fields of a Map row.\n\n@param map Map row"
  },
  {
    "code": "private static void defineField(Map<String, FieldType> container, String name, FieldType type)\n   {\n      defineField(container, name, type, null);\n   }",
    "reference": "Configure the mapping between a database column and a field.\n\n@param container column to field map\n@param name column name\n@param type field type",
    "generated": "Define a field.\n\n@param container map\n@param name field name"
  },
  {
    "code": "private static void dumpTree(PrintWriter pw, DirectoryEntry dir, String prefix, boolean showData, boolean hex, String indent) throws Exception\n   {\n      long byteCount;\n\n      for (Iterator<Entry> iter = dir.getEntries(); iter.hasNext();)\n      {\n         Entry entry = iter.next();\n         if (entry instanceof DirectoryEntry)\n         {\n            String childIndent = indent;\n            if (childIndent != null)\n            {\n               childIndent += \" \";\n            }\n\n            String childPrefix = prefix + \"[\" + entry.getName() + \"].\";\n            pw.println(\"start dir: \" + prefix + entry.getName());\n            dumpTree(pw, (DirectoryEntry) entry, childPrefix, showData, hex, childIndent);\n            pw.println(\"end dir: \" + prefix + entry.getName());\n         }\n         else\n            if (entry instanceof DocumentEntry)\n            {\n               if (showData)\n               {\n                  pw.println(\"start doc: \" + prefix + entry.getName());\n                  if (hex == true)\n                  {\n                     byteCount = hexdump(new DocumentInputStream((DocumentEntry) entry), pw);\n                  }\n                  else\n                  {\n                     byteCount = asciidump(new DocumentInputStream((DocumentEntry) entry), pw);\n                  }\n                  pw.println(\"end doc: \" + prefix + entry.getName() + \" (\" + byteCount + \" bytes read)\");\n               }\n               else\n               {\n                  if (indent != null)\n                  {\n                     pw.print(indent);\n                  }\n                  pw.println(\"doc: \" + prefix + entry.getName());\n               }\n            }\n            else\n            {\n               pw.println(\"found unknown: \" + prefix + entry.getName());\n            }\n      }\n   }",
    "reference": "This method recursively descends the directory structure, dumping\ndetails of any files it finds to the output file.\n\n@param pw Output PrintWriter\n@param dir DirectoryEntry to dump\n@param prefix prefix used to identify path to this object\n@param showData flag indicating if data is dumped, or just structure\n@param hex set to true if hex output is required\n@param indent indent used if displaying structure only\n@throws Exception Thrown on file read errors",
    "generated": "Dump a directory entry, prefix, showData, hex, indent"
  },
  {
    "code": "private static long hexdump(InputStream is, PrintWriter pw) throws Exception\n   {\n      byte[] buffer = new byte[BUFFER_SIZE];\n      long byteCount = 0;\n\n      char c;\n      int loop;\n      int count;\n      StringBuilder sb = new StringBuilder();\n\n      while (true)\n      {\n         count = is.read(buffer);\n         if (count == -1)\n         {\n            break;\n         }\n\n         byteCount += count;\n\n         sb.setLength(0);\n\n         for (loop = 0; loop < count; loop++)\n         {\n            sb.append(\" \");\n            sb.append(HEX_DIGITS[(buffer[loop] & 0xF0) >> 4]);\n            sb.append(HEX_DIGITS[buffer[loop] & 0x0F]);\n         }\n\n         while (loop < BUFFER_SIZE)\n         {\n            sb.append(\"   \");\n            ++loop;\n         }\n\n         sb.append(\"   \");\n\n         for (loop = 0; loop < count; loop++)\n         {\n            c = (char) buffer[loop];\n            if (c > 200 || c < 27)\n            {\n               c = ' ';\n            }\n\n            sb.append(c);\n         }\n\n         pw.println(sb.toString());\n      }\n\n      return (byteCount);\n   }",
    "reference": "This method dumps the entire contents of a file to an output\nprint writer as hex and ASCII data.\n\n@param is Input Stream\n@param pw Output PrintWriter\n@return number of bytes read\n@throws Exception Thrown on file read errors",
    "generated": "Read a hexdump file.\n\n@param is the InputStream\n@param pw the PrintWriter\n@return the number of bytes written to the hexdump file.\n@throws Exception"
  },
  {
    "code": "private static long asciidump(InputStream is, PrintWriter pw) throws Exception\n   {\n      byte[] buffer = new byte[BUFFER_SIZE];\n      long byteCount = 0;\n\n      char c;\n      int loop;\n      int count;\n      StringBuilder sb = new StringBuilder();\n\n      while (true)\n      {\n         count = is.read(buffer);\n         if (count == -1)\n         {\n            break;\n         }\n\n         byteCount += count;\n\n         sb.setLength(0);\n         for (loop = 0; loop < count; loop++)\n         {\n            c = (char) buffer[loop];\n            if (c > 200 || c < 27)\n            {\n               c = ' ';\n            }\n\n            sb.append(c);\n         }\n\n         pw.print(sb.toString());\n      }\n\n      return (byteCount);\n   }",
    "reference": "This method dumps the entire contents of a file to an output\nprint writer as ascii data.\n\n@param is Input Stream\n@param pw Output PrintWriter\n@return number of bytes read\n@throws Exception Thrown on file read errors",
    "generated": "Read the asciidump data.\n\n@param is the InputStream\n@param pw the PrintWriter\n@return the byte count"
  },
  {
    "code": "protected void mergeSameCost(LinkedList<TimephasedCost> list)\n   {\n      LinkedList<TimephasedCost> result = new LinkedList<TimephasedCost>();\n\n      TimephasedCost previousAssignment = null;\n      for (TimephasedCost assignment : list)\n      {\n         if (previousAssignment == null)\n         {\n            assignment.setAmountPerDay(assignment.getTotalAmount());\n            result.add(assignment);\n         }\n         else\n         {\n            Number previousAssignmentCost = previousAssignment.getAmountPerDay();\n            Number assignmentCost = assignment.getTotalAmount();\n\n            if (NumberHelper.equals(previousAssignmentCost.doubleValue(), assignmentCost.doubleValue(), 0.01))\n            {\n               Date assignmentStart = previousAssignment.getStart();\n               Date assignmentFinish = assignment.getFinish();\n               double total = previousAssignment.getTotalAmount().doubleValue();\n               total += assignmentCost.doubleValue();\n\n               TimephasedCost merged = new TimephasedCost();\n               merged.setStart(assignmentStart);\n               merged.setFinish(assignmentFinish);\n               merged.setAmountPerDay(assignmentCost);\n               merged.setTotalAmount(Double.valueOf(total));\n\n               result.removeLast();\n               assignment = merged;\n            }\n            else\n            {\n               assignment.setAmountPerDay(assignment.getTotalAmount());\n            }\n            result.add(assignment);\n         }\n\n         previousAssignment = assignment;\n      }\n\n      list.clear();\n      list.addAll(result);\n   }",
    "reference": "This method merges together assignment data for the same cost.\n\n@param list assignment data",
    "generated": "Merge a list of timephased costs."
  },
  {
    "code": "@Override public void write(ProjectFile projectFile, OutputStream out) throws IOException\n   {\n      m_projectFile = projectFile;\n      m_eventManager = projectFile.getEventManager();\n\n      m_writer = new PrintStream(out); // the print stream class is the easiest way to create a text file\n      m_buffer = new StringBuilder();\n\n      try\n      {\n         write(); // method call a method, this is how MPXJ is structured, so I followed the lead?\n      }\n\n      //      catch (Exception e)\n      //      { // used during console debugging\n      //         System.out.println(\"Caught Exception in SDEFWriter.java\");\n      //         System.out.println(\" \" + e.toString());\n      //      }\n\n      finally\n      { // keeps things cool after we're done\n         m_writer = null;\n         m_projectFile = null;\n         m_buffer = null;\n      }\n   }",
    "reference": "Write a project file in SDEF format to an output stream.\n\n@param projectFile ProjectFile instance\n@param out output stream",
    "generated": "Write a project file.\n\n@param projectFile project file\n@param out the output stream\n@return null"
  },
  {
    "code": "private void writeProjectProperties(ProjectProperties record) throws IOException\n   {\n      // the ProjectProperties class from MPXJ has the details of how many days per week etc....\n      // so I've assigned these variables in here, but actually use them in other methods\n      // see the write task method, that's where they're used, but that method only has a Task object\n      m_minutesPerDay = record.getMinutesPerDay().doubleValue();\n      m_minutesPerWeek = record.getMinutesPerWeek().doubleValue();\n      m_daysPerMonth = record.getDaysPerMonth().doubleValue();\n\n      // reset buffer to be empty, then concatenate data as required by USACE\n      m_buffer.setLength(0);\n      m_buffer.append(\"PROJ \");\n      m_buffer.append(m_formatter.format(record.getStartDate()).toUpperCase() + \" \"); // DataDate\n      m_buffer.append(SDEFmethods.lset(record.getManager(), 4) + \" \"); // ProjIdent\n      m_buffer.append(SDEFmethods.lset(record.getProjectTitle(), 48) + \" \"); // ProjName\n      m_buffer.append(SDEFmethods.lset(record.getSubject(), 36) + \" \"); // ContrName\n      m_buffer.append(\"P \"); // ArrowP\n      m_buffer.append(SDEFmethods.lset(record.getKeywords(), 7)); // ContractNum\n      m_buffer.append(m_formatter.format(record.getStartDate()).toUpperCase() + \" \"); // ProjStart\n      m_buffer.append(m_formatter.format(record.getFinishDate()).toUpperCase()); // ProjEnd\n      m_writer.println(m_buffer);\n   }",
    "reference": "Write project properties.\n\n@param record project properties\n@throws IOException",
    "generated": "Write the ProjectProperties class from MPXJ has the details of how many days per week etc....\n\n@param record ProjectProperties object\n@return the ProjectProperties object"
  },
  {
    "code": "private void writeCalendars(List<ProjectCalendar> records)\n   {\n\n      //\n      // Write project calendars\n      //\n      for (ProjectCalendar record : records)\n      {\n         m_buffer.setLength(0);\n         m_buffer.append(\"CLDR \");\n         m_buffer.append(SDEFmethods.lset(record.getUniqueID().toString(), 2)); // 2 character used, USACE allows 1\n         String workDays = SDEFmethods.workDays(record); // custom line, like NYYYYYN for a week\n         m_buffer.append(SDEFmethods.lset(workDays, 8));\n         m_buffer.append(SDEFmethods.lset(record.getName(), 30));\n         m_writer.println(m_buffer);\n      }\n   }",
    "reference": "This will create a line in the SDEF file for each calendar\nif there are more than 9 calendars, you'll have a big error,\nas USACE numbers them 0-9.\n\n@param records list of ProjectCalendar instances",
    "generated": "Write project calendars."
  },
  {
    "code": "private void writeExceptions(List<ProjectCalendar> records) throws IOException\n   {\n      for (ProjectCalendar record : records)\n      {\n         if (!record.getCalendarExceptions().isEmpty())\n         {\n            // Need to move HOLI up here and get 15 exceptions per line as per USACE spec.\n            // for now, we'll write one line for each calendar exception, hope there aren't too many\n            //\n            // changing this would be a serious upgrade, too much coding to do today....\n            for (ProjectCalendarException ex : record.getCalendarExceptions())\n            {\n               writeCalendarException(record, ex);\n            }\n         }\n         m_eventManager.fireCalendarWrittenEvent(record); // left here from MPX template, maybe not needed???\n      }\n   }",
    "reference": "Write calendar exceptions.\n\n@param records list of ProjectCalendars\n@throws IOException",
    "generated": "Write a list of calendar exceptions."
  },
  {
    "code": "private void writeTaskPredecessors(Task record)\n   {\n      m_buffer.setLength(0);\n      //\n      // Write the task predecessor\n      //\n      if (!record.getSummary() && !record.getPredecessors().isEmpty())\n      { // I don't use summary tasks for SDEF\n         m_buffer.append(\"PRED \");\n         List<Relation> predecessors = record.getPredecessors();\n\n         for (Relation pred : predecessors)\n         {\n            m_buffer.append(SDEFmethods.rset(pred.getSourceTask().getUniqueID().toString(), 10) + \" \");\n            m_buffer.append(SDEFmethods.rset(pred.getTargetTask().getUniqueID().toString(), 10) + \" \");\n            String type = \"C\"; // default finish-to-start\n            if (!pred.getType().toString().equals(\"FS\"))\n            {\n               type = pred.getType().toString().substring(0, 1);\n            }\n            m_buffer.append(type + \" \");\n\n            Duration dd = pred.getLag();\n            double duration = dd.getDuration();\n            if (dd.getUnits() != TimeUnit.DAYS)\n            {\n               dd = Duration.convertUnits(duration, dd.getUnits(), TimeUnit.DAYS, m_minutesPerDay, m_minutesPerWeek, m_daysPerMonth);\n            }\n            Double days = Double.valueOf(dd.getDuration() + 0.5); // Add 0.5 so half day rounds up upon truncation\n            Integer est = Integer.valueOf(days.intValue());\n            m_buffer.append(SDEFmethods.rset(est.toString(), 4) + \" \"); // task duration in days required by USACE\n         }\n         m_writer.println(m_buffer.toString());\n      }\n   }",
    "reference": "Write each predecessor for a task.\n\n@param record Task instance",
    "generated": "Write the task predecessor\n@param record Task to write"
  },
  {
    "code": "private Duration getAssignmentWork(ProjectCalendar calendar, TimephasedWork assignment)\n   {\n      Date assignmentStart = assignment.getStart();\n\n      Date splitStart = assignmentStart;\n      Date splitFinishTime = calendar.getFinishTime(splitStart);\n      Date splitFinish = DateHelper.setTime(splitStart, splitFinishTime);\n\n      Duration calendarSplitWork = calendar.getWork(splitStart, splitFinish, TimeUnit.MINUTES);\n      Duration assignmentWorkPerDay = assignment.getAmountPerDay();\n      Duration splitWork;\n\n      double splitMinutes = assignmentWorkPerDay.getDuration();\n      splitMinutes *= calendarSplitWork.getDuration();\n      splitMinutes /= (8 * 60); // this appears to be a fixed value\n      splitWork = Duration.getInstance(splitMinutes, TimeUnit.MINUTES);\n      return splitWork;\n   }",
    "reference": "Retrieves the pro-rata work carried out on a given day.\n\n@param calendar current calendar\n@param assignment current assignment.\n@return assignment work duration",
    "generated": "Get the current timephased work.\n\n@param calendar calendar\n@param assignment TimephasedWork"
  },
  {
    "code": "private Map<Integer, List<Row>> createWorkPatternAssignmentMap(List<Row> rows) throws ParseException\n   {\n      Map<Integer, List<Row>> map = new HashMap<Integer, List<Row>>();\n      for (Row row : rows)\n      {\n         Integer calendarID = row.getInteger(\"ID\");\n         String workPatterns = row.getString(\"WORK_PATTERNS\");\n         map.put(calendarID, createWorkPatternAssignmentRowList(workPatterns));\n      }\n      return map;\n   }",
    "reference": "Create the work pattern assignment map.\n\n@param rows calendar rows\n@return work pattern assignment map",
    "generated": "Create the work pattern assignment map."
  },
  {
    "code": "private List<Row> createWorkPatternAssignmentRowList(String workPatterns) throws ParseException\n   {\n      List<Row> list = new ArrayList<Row>();\n      String[] patterns = workPatterns.split(\",|:\");\n      int index = 1;\n      while (index < patterns.length)\n      {\n         Integer workPattern = Integer.valueOf(patterns[index + 1]);\n         Date startDate = DatatypeConverter.parseBasicTimestamp(patterns[index + 3]);\n         Date endDate = DatatypeConverter.parseBasicTimestamp(patterns[index + 4]);\n\n         Map<String, Object> map = new HashMap<String, Object>();\n         map.put(\"WORK_PATTERN\", workPattern);\n         map.put(\"START_DATE\", startDate);\n         map.put(\"END_DATE\", endDate);\n\n         list.add(new MapRow(map));\n\n         index += 5;\n      }\n\n      return list;\n   }",
    "reference": "Extract a list of work pattern assignments.\n\n@param workPatterns string representation of work pattern assignments\n@return list of work pattern assignment rows",
    "generated": "Create a list of work patterns.\n\n@param workPatterns work patterns"
  },
  {
    "code": "private Map<Integer, List<Row>> createExceptionAssignmentMap(List<Row> rows)\n   {\n      Map<Integer, List<Row>> map = new HashMap<Integer, List<Row>>();\n      for (Row row : rows)\n      {\n         Integer calendarID = row.getInteger(\"ID\");\n         String exceptions = row.getString(\"EXCEPTIONS\");\n         map.put(calendarID, createExceptionAssignmentRowList(exceptions));\n      }\n      return map;\n   }",
    "reference": "Create the exception assignment map.\n\n@param rows calendar rows\n@return exception assignment map",
    "generated": "Create an exception assignment map."
  },
  {
    "code": "private List<Row> createExceptionAssignmentRowList(String exceptionData)\n   {\n      List<Row> list = new ArrayList<Row>();\n      String[] exceptions = exceptionData.split(\",|:\");\n      int index = 1;\n      while (index < exceptions.length)\n      {\n         Date startDate = DatatypeConverter.parseEpochTimestamp(exceptions[index + 0]);\n         Date endDate = DatatypeConverter.parseEpochTimestamp(exceptions[index + 1]);\n         //Integer exceptionTypeID = Integer.valueOf(exceptions[index + 2]);\n\n         Map<String, Object> map = new HashMap<String, Object>();\n         map.put(\"STARU_DATE\", startDate);\n         map.put(\"ENE_DATE\", endDate);\n\n         list.add(new MapRow(map));\n\n         index += 3;\n      }\n\n      return list;\n   }",
    "reference": "Extract a list of exception assignments.\n\n@param exceptionData string representation of exception assignments\n@return list of exception assignment rows",
    "generated": "Create an exception assignment row list.\n\n@param exceptionData"
  },
  {
    "code": "private Map<Integer, List<Row>> createTimeEntryMap(List<Row> rows) throws ParseException\n   {\n      Map<Integer, List<Row>> map = new HashMap<Integer, List<Row>>();\n      for (Row row : rows)\n      {\n         Integer workPatternID = row.getInteger(\"ID\");\n         String shifts = row.getString(\"SHIFTS\");\n         map.put(workPatternID, createTimeEntryRowList(shifts));\n      }\n      return map;\n   }",
    "reference": "Create the time entry map.\n\n@param rows work pattern rows\n@return time entry map",
    "generated": "Create a map of time entries.\n\n@param rows the list of time entries"
  },
  {
    "code": "private List<Row> createTimeEntryRowList(String shiftData) throws ParseException\n   {\n      List<Row> list = new ArrayList<Row>();\n      String[] shifts = shiftData.split(\",|:\");\n      int index = 1;\n      while (index < shifts.length)\n      {\n         index += 2;\n         int entryCount = Integer.parseInt(shifts[index]);\n         index++;\n\n         for (int entryIndex = 0; entryIndex < entryCount; entryIndex++)\n         {\n            Integer exceptionTypeID = Integer.valueOf(shifts[index + 0]);\n            Date startTime = DatatypeConverter.parseBasicTime(shifts[index + 1]);\n            Date endTime = DatatypeConverter.parseBasicTime(shifts[index + 2]);\n\n            Map<String, Object> map = new HashMap<String, Object>();\n            map.put(\"START_TIME\", startTime);\n            map.put(\"END_TIME\", endTime);\n            map.put(\"EXCEPTIOP\", exceptionTypeID);\n\n            list.add(new MapRow(map));\n\n            index += 3;\n         }\n      }\n\n      return list;\n   }",
    "reference": "Extract a list of time entries.\n\n@param shiftData string representation of time entries\n@return list of time entry rows",
    "generated": "Create a time entry row list."
  },
  {
    "code": "public Map<Integer, TableDefinition> tableDefinitions()\n   {\n      Map<Integer, TableDefinition> result = new HashMap<Integer, TableDefinition>();\n\n      result.put(Integer.valueOf(2), new TableDefinition(\"PROJECT_SUMMARY\", columnDefinitions(PROJECT_SUMMARY_COLUMNS, projectSummaryColumnsOrder())));\n      result.put(Integer.valueOf(7), new TableDefinition(\"BAR\", columnDefinitions(BAR_COLUMNS, barColumnsOrder())));\n      result.put(Integer.valueOf(11), new TableDefinition(\"CALENDAR\", columnDefinitions(CALENDAR_COLUMNS, calendarColumnsOrder())));\n      result.put(Integer.valueOf(12), new TableDefinition(\"EXCEPTIONN\", columnDefinitions(EXCEPTIONN_COLUMNS, exceptionColumnsOrder())));\n      result.put(Integer.valueOf(14), new TableDefinition(\"EXCEPTION_ASSIGNMENT\", columnDefinitions(EXCEPTION_ASSIGNMENT_COLUMNS, exceptionAssignmentColumnsOrder())));\n      result.put(Integer.valueOf(15), new TableDefinition(\"TIME_ENTRY\", columnDefinitions(TIME_ENTRY_COLUMNS, timeEntryColumnsOrder())));\n      result.put(Integer.valueOf(17), new TableDefinition(\"WORK_PATTERN\", columnDefinitions(WORK_PATTERN_COLUMNS, workPatternColumnsOrder())));      \n      result.put(Integer.valueOf(18), new TableDefinition(\"TASK_COMPLETED_SECTION\", columnDefinitions(TASK_COMPLETED_SECTION_COLUMNS, taskCompletedSectionColumnsOrder())));      \n      result.put(Integer.valueOf(21), new TableDefinition(\"TASK\", columnDefinitions(TASK_COLUMNS, taskColumnsOrder())));\n      result.put(Integer.valueOf(22), new TableDefinition(\"MILESTONE\", columnDefinitions(MILESTONE_COLUMNS, milestoneColumnsOrder())));\n      result.put(Integer.valueOf(23), new TableDefinition(\"EXPANDED_TASK\", columnDefinitions(EXPANDED_TASK_COLUMNS, expandedTaskColumnsOrder())));\n      result.put(Integer.valueOf(25), new TableDefinition(\"LINK\", columnDefinitions(LINK_COLUMNS, linkColumnsOrder())));\n      result.put(Integer.valueOf(61), new TableDefinition(\"CONSUMABLE_RESOURCE\", columnDefinitions(CONSUMABLE_RESOURCE_COLUMNS, consumableResourceColumnsOrder())));\n      result.put(Integer.valueOf(62), new TableDefinition(\"PERMANENT_RESOURCE\", columnDefinitions(PERMANENT_RESOURCE_COLUMNS, permanentResourceColumnsOrder())));\n      result.put(Integer.valueOf(63), new TableDefinition(\"PERM_RESOURCE_SKILL\", columnDefinitions(PERMANENT_RESOURCE_SKILL_COLUMNS, permanentResourceSkillColumnsOrder())));\n      result.put(Integer.valueOf(67), new TableDefinition(\"PERMANENT_SCHEDUL_ALLOCATION\", columnDefinitions(PERMANENT_SCHEDULE_ALLOCATION_COLUMNS, permanentScheduleAllocationColumnsOrder())));\n      result.put(Integer.valueOf(190), new TableDefinition(\"WBS_ENTRY\", columnDefinitions(WBS_ENTRY_COLUMNS, wbsEntryColumnsOrder())));\n\n      return result;\n   }",
    "reference": "Retrieves the table structure for an Asta PP file. Subclasses determine the exact contents of the structure\nfor a specific version of the Asta PP file.\n\n@return PP file table structure",
    "generated": "Generate a map of table definitions.\n\n@param tableDefinitions data\n@return a map of table definitions"
  },
  {
    "code": "private ColumnDefinition[] columnDefinitions(ColumnDefinition[] columns, String[] order)\n   {\n      Map<String, ColumnDefinition> map = makeColumnMap(columns);\n      ColumnDefinition[] result = new ColumnDefinition[order.length];\n      for (int index = 0; index < order.length; index++)\n      {\n         result[index] = map.get(order[index]);\n      }\n      return result;\n   }",
    "reference": "Generate an ordered set of column definitions from an ordered set of column names.\n\n@param columns column definitions\n@param order column names\n@return ordered set of column definitions",
    "generated": "Generate a list of column definitions.\n\n@param columns ColumnDefinition[] order"
  },
  {
    "code": "private Map<String, ColumnDefinition> makeColumnMap(ColumnDefinition[] columns)\n   {\n      Map<String, ColumnDefinition> map = new HashMap<String, ColumnDefinition>();\n      for (ColumnDefinition def : columns)\n      {\n         map.put(def.getName(), def);\n      }\n      return map;\n   }",
    "reference": "Convert an array of column definitions into a map keyed by column name.\n\n@param columns array of column definitions\n@return map of column definitions",
    "generated": "Make a map of column definitions.\n\n@param columns column definitions"
  },
  {
    "code": "private void writeProjectExtendedAttributes(Project project)\n   {\n      Project.ExtendedAttributes attributes = m_factory.createProjectExtendedAttributes();\n      project.setExtendedAttributes(attributes);\n      List<Project.ExtendedAttributes.ExtendedAttribute> list = attributes.getExtendedAttribute();\n\n      Set<FieldType> customFields = new HashSet<FieldType>();\n      for (CustomField customField : m_projectFile.getCustomFields())\n      {\n         FieldType fieldType = customField.getFieldType();\n         if (fieldType != null)\n         {\n            customFields.add(fieldType);\n         }\n      }\n\n      customFields.addAll(m_extendedAttributesInUse);\n      \n      List<FieldType> customFieldsList = new ArrayList<FieldType>();\n      customFieldsList.addAll(customFields);\n      \n\n      // Sort to ensure consistent order in file\n      final CustomFieldContainer customFieldContainer =  m_projectFile.getCustomFields();\n      Collections.sort(customFieldsList, new Comparator<FieldType>()\n      {\n         @Override public int compare(FieldType o1, FieldType o2)\n         {\n            CustomField customField1 = customFieldContainer.getCustomField(o1);\n            CustomField customField2 = customFieldContainer.getCustomField(o2);\n            String name1 = o1.getClass().getSimpleName() + \".\" + o1.getName() + \" \" + customField1.getAlias();\n            String name2 = o2.getClass().getSimpleName() + \".\" + o2.getName() + \" \" + customField2.getAlias();\n            return name1.compareTo(name2);\n         }\n      });\n\n      for (FieldType fieldType : customFieldsList)\n      {\n         Project.ExtendedAttributes.ExtendedAttribute attribute = m_factory.createProjectExtendedAttributesExtendedAttribute();\n         list.add(attribute);\n         attribute.setFieldID(String.valueOf(FieldTypeHelper.getFieldID(fieldType)));\n         attribute.setFieldName(fieldType.getName());\n\n         CustomField customField = customFieldContainer.getCustomField(fieldType);\n         attribute.setAlias(customField.getAlias());\n      }\n   }",
    "reference": "This method writes project extended attribute data into an MSPDI file.\n\n@param project Root node of the MSPDI file",
    "generated": "Write the project extended attributes.\n\n@param project Project object\n@return project extended attributes"
  },
  {
    "code": "private void writeCalendars(Project project)\n   {\n      //\n      // Create the new MSPDI calendar list\n      //\n      Project.Calendars calendars = m_factory.createProjectCalendars();\n      project.setCalendars(calendars);\n      List<Project.Calendars.Calendar> calendar = calendars.getCalendar();\n\n      //\n      // Process each calendar in turn\n      //\n      for (ProjectCalendar cal : m_projectFile.getCalendars())\n      {\n         calendar.add(writeCalendar(cal));\n      }\n   }",
    "reference": "This method writes calendar data to an MSPDI file.\n\n@param project Root node of the MSPDI file",
    "generated": "Write the new MSPDI calendar list"
  },
  {
    "code": "private Project.Calendars.Calendar writeCalendar(ProjectCalendar bc)\n   {\n      //\n      // Create a calendar\n      //\n      Project.Calendars.Calendar calendar = m_factory.createProjectCalendarsCalendar();\n      calendar.setUID(NumberHelper.getBigInteger(bc.getUniqueID()));\n      calendar.setIsBaseCalendar(Boolean.valueOf(!bc.isDerived()));\n\n      ProjectCalendar base = bc.getParent();\n      // SF-329: null default required to keep Powerproject happy when importing MSPDI files\n      calendar.setBaseCalendarUID(base == null ? NULL_CALENDAR_ID : NumberHelper.getBigInteger(base.getUniqueID()));\n      calendar.setName(bc.getName());\n\n      //\n      // Create a list of normal days\n      //\n      Project.Calendars.Calendar.WeekDays days = m_factory.createProjectCalendarsCalendarWeekDays();\n      Project.Calendars.Calendar.WeekDays.WeekDay.WorkingTimes.WorkingTime time;\n      ProjectCalendarHours bch;\n\n      List<Project.Calendars.Calendar.WeekDays.WeekDay> dayList = days.getWeekDay();\n\n      for (int loop = 1; loop < 8; loop++)\n      {\n         DayType workingFlag = bc.getWorkingDay(Day.getInstance(loop));\n\n         if (workingFlag != DayType.DEFAULT)\n         {\n            Project.Calendars.Calendar.WeekDays.WeekDay day = m_factory.createProjectCalendarsCalendarWeekDaysWeekDay();\n            dayList.add(day);\n            day.setDayType(BigInteger.valueOf(loop));\n            day.setDayWorking(Boolean.valueOf(workingFlag == DayType.WORKING));\n\n            if (workingFlag == DayType.WORKING)\n            {\n               Project.Calendars.Calendar.WeekDays.WeekDay.WorkingTimes times = m_factory.createProjectCalendarsCalendarWeekDaysWeekDayWorkingTimes();\n               day.setWorkingTimes(times);\n               List<Project.Calendars.Calendar.WeekDays.WeekDay.WorkingTimes.WorkingTime> timesList = times.getWorkingTime();\n\n               bch = bc.getCalendarHours(Day.getInstance(loop));\n               if (bch != null)\n               {\n                  for (DateRange range : bch)\n                  {\n                     if (range != null)\n                     {\n                        time = m_factory.createProjectCalendarsCalendarWeekDaysWeekDayWorkingTimesWorkingTime();\n                        timesList.add(time);\n\n                        time.setFromTime(range.getStart());\n                        time.setToTime(range.getEnd());\n                     }\n                  }\n               }\n            }\n         }\n      }\n\n      //\n      // Create a list of exceptions\n      //\n      // A quirk of MS Project is that these exceptions must be\n      // in date order in the file, otherwise they are ignored\n      //\n      List<ProjectCalendarException> exceptions = new ArrayList<ProjectCalendarException>(bc.getCalendarExceptions());\n      if (!exceptions.isEmpty())\n      {\n         Collections.sort(exceptions);\n         writeExceptions(calendar, dayList, exceptions);\n      }\n\n      //\n      // Do not add a weekdays tag to the calendar unless it\n      // has valid entries.\n      // Fixes SourceForge bug 1854747: MPXJ and MSP 2007 XML formats\n      //\n      if (!dayList.isEmpty())\n      {\n         calendar.setWeekDays(days);\n      }\n\n      writeWorkWeeks(calendar, bc);\n\n      m_eventManager.fireCalendarWrittenEvent(bc);\n\n      return (calendar);\n   }",
    "reference": "This method writes data for a single calendar to an MSPDI file.\n\n@param bc Base calendar data\n@return New MSPDI calendar instance",
    "generated": "Write a project calendar.\n\n@param bc Project calendar\n@return Project calendar"
  },
  {
    "code": "private void writeExceptions(Project.Calendars.Calendar calendar, List<Project.Calendars.Calendar.WeekDays.WeekDay> dayList, List<ProjectCalendarException> exceptions)\n   {\n      // Always write legacy exception data:\n      // Powerproject appears not to recognise new format data at all,\n      // and legacy data is ignored in preference to new data post MSP 2003\n      writeExceptions9(dayList, exceptions);\n\n      if (m_saveVersion.getValue() > SaveVersion.Project2003.getValue())\n      {\n         writeExceptions12(calendar, exceptions);\n      }\n   }",
    "reference": "Main entry point used to determine the format used to write\ncalendar exceptions.\n\n@param calendar parent calendar\n@param dayList list of calendar days\n@param exceptions list of exceptions",
    "generated": "Write legacy exception data."
  },
  {
    "code": "private void writeExceptions9(List<Project.Calendars.Calendar.WeekDays.WeekDay> dayList, List<ProjectCalendarException> exceptions)\n   {\n      for (ProjectCalendarException exception : exceptions)\n      {\n         boolean working = exception.getWorking();\n\n         Project.Calendars.Calendar.WeekDays.WeekDay day = m_factory.createProjectCalendarsCalendarWeekDaysWeekDay();\n         dayList.add(day);\n         day.setDayType(BIGINTEGER_ZERO);\n         day.setDayWorking(Boolean.valueOf(working));\n\n         Project.Calendars.Calendar.WeekDays.WeekDay.TimePeriod period = m_factory.createProjectCalendarsCalendarWeekDaysWeekDayTimePeriod();\n         day.setTimePeriod(period);\n         period.setFromDate(exception.getFromDate());\n         period.setToDate(exception.getToDate());\n\n         if (working)\n         {\n            Project.Calendars.Calendar.WeekDays.WeekDay.WorkingTimes times = m_factory.createProjectCalendarsCalendarWeekDaysWeekDayWorkingTimes();\n            day.setWorkingTimes(times);\n            List<Project.Calendars.Calendar.WeekDays.WeekDay.WorkingTimes.WorkingTime> timesList = times.getWorkingTime();\n\n            for (DateRange range : exception)\n            {\n               Project.Calendars.Calendar.WeekDays.WeekDay.WorkingTimes.WorkingTime time = m_factory.createProjectCalendarsCalendarWeekDaysWeekDayWorkingTimesWorkingTime();\n               timesList.add(time);\n\n               time.setFromTime(range.getStart());\n               time.setToTime(range.getEnd());\n            }\n         }\n      }\n   }",
    "reference": "Write exceptions in the format used by MSPDI files prior to Project 2007.\n\n@param dayList list of calendar days\n@param exceptions list of exceptions",
    "generated": "Write a list of ProjectCalendarExceptions.\n\n@param dayList list of ProjectCalendarExceptions"
  },
  {
    "code": "private void writeExceptions12(Project.Calendars.Calendar calendar, List<ProjectCalendarException> exceptions)\n   {\n      Exceptions ce = m_factory.createProjectCalendarsCalendarExceptions();\n      calendar.setExceptions(ce);\n      List<Exceptions.Exception> el = ce.getException();\n\n      for (ProjectCalendarException exception : exceptions)\n      {\n         Exceptions.Exception ex = m_factory.createProjectCalendarsCalendarExceptionsException();\n         el.add(ex);\n\n         ex.setName(exception.getName());\n         boolean working = exception.getWorking();\n         ex.setDayWorking(Boolean.valueOf(working));\n\n         if (exception.getRecurring() == null)\n         {\n            ex.setEnteredByOccurrences(Boolean.FALSE);\n            ex.setOccurrences(BigInteger.ONE);\n            ex.setType(BigInteger.ONE);\n         }\n         else\n         {\n            populateRecurringException(exception, ex);\n         }\n\n         Project.Calendars.Calendar.Exceptions.Exception.TimePeriod period = m_factory.createProjectCalendarsCalendarExceptionsExceptionTimePeriod();\n         ex.setTimePeriod(period);\n         period.setFromDate(exception.getFromDate());\n         period.setToDate(exception.getToDate());\n\n         if (working)\n         {\n            Project.Calendars.Calendar.Exceptions.Exception.WorkingTimes times = m_factory.createProjectCalendarsCalendarExceptionsExceptionWorkingTimes();\n            ex.setWorkingTimes(times);\n            List<Project.Calendars.Calendar.Exceptions.Exception.WorkingTimes.WorkingTime> timesList = times.getWorkingTime();\n\n            for (DateRange range : exception)\n            {\n               Project.Calendars.Calendar.Exceptions.Exception.WorkingTimes.WorkingTime time = m_factory.createProjectCalendarsCalendarExceptionsExceptionWorkingTimesWorkingTime();\n               timesList.add(time);\n\n               time.setFromTime(range.getStart());\n               time.setToTime(range.getEnd());\n            }\n         }\n      }\n   }",
    "reference": "Write exceptions into the format used by MSPDI files from\nProject 2007 onwards.\n\n@param calendar parent calendar\n@param exceptions list of exceptions",
    "generated": "Write a list of exceptions."
  },
  {
    "code": "private void populateRecurringException(ProjectCalendarException mpxjException, Exceptions.Exception xmlException)\n   {\n      RecurringData data = mpxjException.getRecurring();\n      xmlException.setEnteredByOccurrences(Boolean.TRUE);\n      xmlException.setOccurrences(NumberHelper.getBigInteger(data.getOccurrences()));\n\n      switch (data.getRecurrenceType())\n      {\n         case DAILY:\n         {\n            xmlException.setType(BigInteger.valueOf(7));\n            xmlException.setPeriod(NumberHelper.getBigInteger(data.getFrequency()));\n            break;\n         }\n\n         case WEEKLY:\n         {\n            xmlException.setType(BigInteger.valueOf(6));\n            xmlException.setPeriod(NumberHelper.getBigInteger(data.getFrequency()));\n            xmlException.setDaysOfWeek(getDaysOfTheWeek(data));\n            break;\n         }\n\n         case MONTHLY:\n         {\n            xmlException.setPeriod(NumberHelper.getBigInteger(data.getFrequency()));\n            if (data.getRelative())\n            {\n               xmlException.setType(BigInteger.valueOf(5));\n               xmlException.setMonthItem(BigInteger.valueOf(data.getDayOfWeek().getValue() + 2));\n               xmlException.setMonthPosition(BigInteger.valueOf(NumberHelper.getInt(data.getDayNumber()) - 1));\n            }\n            else\n            {\n               xmlException.setType(BigInteger.valueOf(4));\n               xmlException.setMonthDay(NumberHelper.getBigInteger(data.getDayNumber()));\n            }\n            break;\n         }\n\n         case YEARLY:\n         {\n            xmlException.setMonth(BigInteger.valueOf(NumberHelper.getInt(data.getMonthNumber()) - 1));\n            if (data.getRelative())\n            {\n               xmlException.setType(BigInteger.valueOf(3));\n               xmlException.setMonthItem(BigInteger.valueOf(data.getDayOfWeek().getValue() + 2));\n               xmlException.setMonthPosition(BigInteger.valueOf(NumberHelper.getInt(data.getDayNumber()) - 1));\n            }\n            else\n            {\n               xmlException.setType(BigInteger.valueOf(2));\n               xmlException.setMonthDay(NumberHelper.getBigInteger(data.getDayNumber()));\n            }\n         }\n      }\n   }",
    "reference": "Writes the details of a recurring exception.\n\n@param mpxjException source MPXJ calendar exception\n@param xmlException target MSPDI exception",
    "generated": "Populate the Recurring exception.\n\n@param mpxjException ProjectCalendarException mpxjException\n@param xmlException ProjectCalendarException xmlException"
  },
  {
    "code": "private BigInteger getDaysOfTheWeek(RecurringData data)\n   {\n      int value = 0;\n      for (Day day : Day.values())\n      {\n         if (data.getWeeklyDay(day))\n         {\n            value = value | DAY_MASKS[day.getValue()];\n         }\n      }\n      return BigInteger.valueOf(value);\n   }",
    "reference": "Converts days of the week into a bit field.\n\n@param data recurring data\n@return bit field",
    "generated": "Get the days of the week."
  },
  {
    "code": "private void writeWorkWeeks(Project.Calendars.Calendar xmlCalendar, ProjectCalendar mpxjCalendar)\n   {\n      List<ProjectCalendarWeek> weeks = mpxjCalendar.getWorkWeeks();\n      if (!weeks.isEmpty())\n      {\n         WorkWeeks xmlWorkWeeks = m_factory.createProjectCalendarsCalendarWorkWeeks();\n         xmlCalendar.setWorkWeeks(xmlWorkWeeks);\n         List<WorkWeek> xmlWorkWeekList = xmlWorkWeeks.getWorkWeek();\n\n         for (ProjectCalendarWeek week : weeks)\n         {\n            WorkWeek xmlWeek = m_factory.createProjectCalendarsCalendarWorkWeeksWorkWeek();\n            xmlWorkWeekList.add(xmlWeek);\n\n            xmlWeek.setName(week.getName());\n            TimePeriod xmlTimePeriod = m_factory.createProjectCalendarsCalendarWorkWeeksWorkWeekTimePeriod();\n            xmlWeek.setTimePeriod(xmlTimePeriod);\n            xmlTimePeriod.setFromDate(week.getDateRange().getStart());\n            xmlTimePeriod.setToDate(week.getDateRange().getEnd());\n\n            WeekDays xmlWeekDays = m_factory.createProjectCalendarsCalendarWorkWeeksWorkWeekWeekDays();\n            xmlWeek.setWeekDays(xmlWeekDays);\n\n            List<Project.Calendars.Calendar.WorkWeeks.WorkWeek.WeekDays.WeekDay> dayList = xmlWeekDays.getWeekDay();\n\n            for (int loop = 1; loop < 8; loop++)\n            {\n               DayType workingFlag = week.getWorkingDay(Day.getInstance(loop));\n\n               if (workingFlag != DayType.DEFAULT)\n               {\n                  Project.Calendars.Calendar.WorkWeeks.WorkWeek.WeekDays.WeekDay day = m_factory.createProjectCalendarsCalendarWorkWeeksWorkWeekWeekDaysWeekDay();\n                  dayList.add(day);\n                  day.setDayType(BigInteger.valueOf(loop));\n                  day.setDayWorking(Boolean.valueOf(workingFlag == DayType.WORKING));\n\n                  if (workingFlag == DayType.WORKING)\n                  {\n                     Project.Calendars.Calendar.WorkWeeks.WorkWeek.WeekDays.WeekDay.WorkingTimes times = m_factory.createProjectCalendarsCalendarWorkWeeksWorkWeekWeekDaysWeekDayWorkingTimes();\n                     day.setWorkingTimes(times);\n                     List<Project.Calendars.Calendar.WorkWeeks.WorkWeek.WeekDays.WeekDay.WorkingTimes.WorkingTime> timesList = times.getWorkingTime();\n\n                     ProjectCalendarHours bch = week.getCalendarHours(Day.getInstance(loop));\n                     if (bch != null)\n                     {\n                        for (DateRange range : bch)\n                        {\n                           if (range != null)\n                           {\n                              Project.Calendars.Calendar.WorkWeeks.WorkWeek.WeekDays.WeekDay.WorkingTimes.WorkingTime time = m_factory.createProjectCalendarsCalendarWorkWeeksWorkWeekWeekDaysWeekDayWorkingTimesWorkingTime();\n                              timesList.add(time);\n\n                              time.setFromTime(range.getStart());\n                              time.setToTime(range.getEnd());\n                           }\n                        }\n                     }\n                  }\n               }\n            }\n         }\n      }\n   }",
    "reference": "Write the work weeks associated with this calendar.\n\n@param xmlCalendar XML calendar instance\n@param mpxjCalendar MPXJ calendar instance",
    "generated": "Write a list of workWeeks.\n\n@param xmlCalendar Project calendar\n@param mpxjCalendar Project calendar\n@param mpxjCalendar Project calendar to write"
  },
  {
    "code": "private void writeResources(Project project)\n   {\n      Project.Resources resources = m_factory.createProjectResources();\n      project.setResources(resources);\n      List<Project.Resources.Resource> list = resources.getResource();\n\n      for (Resource resource : m_projectFile.getResources())\n      {\n         list.add(writeResource(resource));\n      }\n   }",
    "reference": "This method writes resource data to an MSPDI file.\n\n@param project Root node of the MSPDI file",
    "generated": "Write the project resources.\n\n@param project project"
  },
  {
    "code": "private void writeResourceBaselines(Project.Resources.Resource xmlResource, Resource mpxjResource)\n   {\n      Project.Resources.Resource.Baseline baseline = m_factory.createProjectResourcesResourceBaseline();\n      boolean populated = false;\n\n      Number cost = mpxjResource.getBaselineCost();\n      if (cost != null && cost.intValue() != 0)\n      {\n         populated = true;\n         baseline.setCost(DatatypeConverter.printCurrency(cost));\n      }\n\n      Duration work = mpxjResource.getBaselineWork();\n      if (work != null && work.getDuration() != 0)\n      {\n         populated = true;\n         baseline.setWork(DatatypeConverter.printDuration(this, work));\n      }\n\n      if (populated)\n      {\n         xmlResource.getBaseline().add(baseline);\n         baseline.setNumber(BigInteger.ZERO);\n      }\n\n      for (int loop = 1; loop <= 10; loop++)\n      {\n         baseline = m_factory.createProjectResourcesResourceBaseline();\n         populated = false;\n\n         cost = mpxjResource.getBaselineCost(loop);\n         if (cost != null && cost.intValue() != 0)\n         {\n            populated = true;\n            baseline.setCost(DatatypeConverter.printCurrency(cost));\n         }\n\n         work = mpxjResource.getBaselineWork(loop);\n         if (work != null && work.getDuration() != 0)\n         {\n            populated = true;\n            baseline.setWork(DatatypeConverter.printDuration(this, work));\n         }\n\n         if (populated)\n         {\n            xmlResource.getBaseline().add(baseline);\n            baseline.setNumber(BigInteger.valueOf(loop));\n         }\n      }\n   }",
    "reference": "Writes resource baseline data.\n\n@param xmlResource MSPDI resource\n@param mpxjResource MPXJ resource",
    "generated": "Write the resources of a project resource.\n\n@param xmlResource project resource\n@param mpxjResource project resource\n@param mpxjResource project resource\n@return"
  },
  {
    "code": "private void writeResourceExtendedAttributes(Project.Resources.Resource xml, Resource mpx)\n   {\n      Project.Resources.Resource.ExtendedAttribute attrib;\n      List<Project.Resources.Resource.ExtendedAttribute> extendedAttributes = xml.getExtendedAttribute();\n\n      for (ResourceField mpxFieldID : getAllResourceExtendedAttributes())\n      {\n         Object value = mpx.getCachedValue(mpxFieldID);\n\n         if (FieldTypeHelper.valueIsNotDefault(mpxFieldID, value))\n         {\n            m_extendedAttributesInUse.add(mpxFieldID);\n\n            Integer xmlFieldID = Integer.valueOf(MPPResourceField.getID(mpxFieldID) | MPPResourceField.RESOURCE_FIELD_BASE);\n\n            attrib = m_factory.createProjectResourcesResourceExtendedAttribute();\n            extendedAttributes.add(attrib);\n            attrib.setFieldID(xmlFieldID.toString());\n            attrib.setValue(DatatypeConverter.printExtendedAttribute(this, value, mpxFieldID.getDataType()));\n            attrib.setDurationFormat(printExtendedAttributeDurationFormat(value));\n         }\n      }\n   }",
    "reference": "This method writes extended attribute data for a resource.\n\n@param xml MSPDI resource\n@param mpx MPXJ resource",
    "generated": "Write the resource extended attributes."
  },
  {
    "code": "private void writeCostRateTables(Project.Resources.Resource xml, Resource mpx)\n   {\n      //Rates rates = m_factory.createProjectResourcesResourceRates();\n      //xml.setRates(rates);\n      //List<Project.Resources.Resource.Rates.Rate> ratesList = rates.getRate();\n\n      List<Project.Resources.Resource.Rates.Rate> ratesList = null;\n\n      for (int tableIndex = 0; tableIndex < 5; tableIndex++)\n      {\n         CostRateTable table = mpx.getCostRateTable(tableIndex);\n         if (table != null)\n         {\n            Date from = DateHelper.FIRST_DATE;\n            for (CostRateTableEntry entry : table)\n            {\n               if (costRateTableWriteRequired(entry, from))\n               {\n                  if (ratesList == null)\n                  {\n                     Rates rates = m_factory.createProjectResourcesResourceRates();\n                     xml.setRates(rates);\n                     ratesList = rates.getRate();\n                  }\n\n                  Project.Resources.Resource.Rates.Rate rate = m_factory.createProjectResourcesResourceRatesRate();\n                  ratesList.add(rate);\n\n                  rate.setCostPerUse(DatatypeConverter.printCurrency(entry.getCostPerUse()));\n                  rate.setOvertimeRate(DatatypeConverter.printRate(entry.getOvertimeRate()));\n                  rate.setOvertimeRateFormat(DatatypeConverter.printTimeUnit(entry.getOvertimeRateFormat()));\n                  rate.setRatesFrom(from);\n                  from = entry.getEndDate();\n                  rate.setRatesTo(from);\n                  rate.setRateTable(BigInteger.valueOf(tableIndex));\n                  rate.setStandardRate(DatatypeConverter.printRate(entry.getStandardRate()));\n                  rate.setStandardRateFormat(DatatypeConverter.printTimeUnit(entry.getStandardRateFormat()));\n               }\n            }\n         }\n      }\n   }",
    "reference": "Writes a resource's cost rate tables.\n\n@param xml MSPDI resource\n@param mpx MPXJ resource",
    "generated": "Write the cost rate tables.\n\n@param xml Resource object\n@param mpx Resource object\n@return Resource object"
  },
  {
    "code": "private boolean costRateTableWriteRequired(CostRateTableEntry entry, Date from)\n   {\n      boolean fromDate = (DateHelper.compare(from, DateHelper.FIRST_DATE) > 0);\n      boolean toDate = (DateHelper.compare(entry.getEndDate(), DateHelper.LAST_DATE) > 0);\n      boolean costPerUse = (NumberHelper.getDouble(entry.getCostPerUse()) != 0);\n      boolean overtimeRate = (entry.getOvertimeRate() != null && entry.getOvertimeRate().getAmount() != 0);\n      boolean standardRate = (entry.getStandardRate() != null && entry.getStandardRate().getAmount() != 0);\n      return (fromDate || toDate || costPerUse || overtimeRate || standardRate);\n   }",
    "reference": "This method determines whether the cost rate table should be written.\nA default cost rate table should not be written to the file.\n\n@param entry cost rate table entry\n@param from from date\n@return boolean flag",
    "generated": "Write a cost rate table.\n\n@param entry cost rate table entry"
  },
  {
    "code": "private void writeAvailability(Project.Resources.Resource xml, Resource mpx)\n   {\n      AvailabilityPeriods periods = m_factory.createProjectResourcesResourceAvailabilityPeriods();\n      xml.setAvailabilityPeriods(periods);\n      List<AvailabilityPeriod> list = periods.getAvailabilityPeriod();\n      for (Availability availability : mpx.getAvailability())\n      {\n         AvailabilityPeriod period = m_factory.createProjectResourcesResourceAvailabilityPeriodsAvailabilityPeriod();\n         list.add(period);\n         DateRange range = availability.getRange();\n\n         period.setAvailableFrom(range.getStart());\n         period.setAvailableTo(range.getEnd());\n         period.setAvailableUnits(DatatypeConverter.printUnits(availability.getUnits()));\n      }\n   }",
    "reference": "This method writes a resource's availability table.\n\n@param xml MSPDI resource\n@param mpx MPXJ resource",
    "generated": "Write the availability of a project resource.\n\n@param xml Resource\n@param mpx Resource\n@return Resource"
  },
  {
    "code": "private void writeTasks(Project project)\n   {\n      Project.Tasks tasks = m_factory.createProjectTasks();\n      project.setTasks(tasks);\n      List<Project.Tasks.Task> list = tasks.getTask();\n\n      for (Task task : m_projectFile.getTasks())\n      {\n         list.add(writeTask(task));\n      }\n   }",
    "reference": "This method writes task data to an MSPDI file.\n\n@param project Root node of the MSPDI file",
    "generated": "Write the project tasks.\n\n@param project the project\n@param project the project\n@return the project"
  },
  {
    "code": "private void writeTaskBaselines(Project.Tasks.Task xmlTask, Task mpxjTask)\n   {\n      Project.Tasks.Task.Baseline baseline = m_factory.createProjectTasksTaskBaseline();\n      boolean populated = false;\n\n      Number cost = mpxjTask.getBaselineCost();\n      if (cost != null && cost.intValue() != 0)\n      {\n         populated = true;\n         baseline.setCost(DatatypeConverter.printCurrency(cost));\n      }\n\n      Duration duration = mpxjTask.getBaselineDuration();\n      if (duration != null && duration.getDuration() != 0)\n      {\n         populated = true;\n         baseline.setDuration(DatatypeConverter.printDuration(this, duration));\n         baseline.setDurationFormat(DatatypeConverter.printDurationTimeUnits(duration, false));\n      }\n\n      Date date = mpxjTask.getBaselineFinish();\n      if (date != null)\n      {\n         populated = true;\n         baseline.setFinish(date);\n      }\n\n      date = mpxjTask.getBaselineStart();\n      if (date != null)\n      {\n         populated = true;\n         baseline.setStart(date);\n      }\n\n      duration = mpxjTask.getBaselineWork();\n      if (duration != null && duration.getDuration() != 0)\n      {\n         populated = true;\n         baseline.setWork(DatatypeConverter.printDuration(this, duration));\n      }\n\n      if (populated)\n      {\n         baseline.setNumber(BigInteger.ZERO);\n         xmlTask.getBaseline().add(baseline);\n      }\n\n      for (int loop = 1; loop <= 10; loop++)\n      {\n         baseline = m_factory.createProjectTasksTaskBaseline();\n         populated = false;\n\n         cost = mpxjTask.getBaselineCost(loop);\n         if (cost != null && cost.intValue() != 0)\n         {\n            populated = true;\n            baseline.setCost(DatatypeConverter.printCurrency(cost));\n         }\n\n         duration = mpxjTask.getBaselineDuration(loop);\n         if (duration != null && duration.getDuration() != 0)\n         {\n            populated = true;\n            baseline.setDuration(DatatypeConverter.printDuration(this, duration));\n            baseline.setDurationFormat(DatatypeConverter.printDurationTimeUnits(duration, false));\n         }\n\n         date = mpxjTask.getBaselineFinish(loop);\n         if (date != null)\n         {\n            populated = true;\n            baseline.setFinish(date);\n         }\n\n         date = mpxjTask.getBaselineStart(loop);\n         if (date != null)\n         {\n            populated = true;\n            baseline.setStart(date);\n         }\n\n         duration = mpxjTask.getBaselineWork(loop);\n         if (duration != null && duration.getDuration() != 0)\n         {\n            populated = true;\n            baseline.setWork(DatatypeConverter.printDuration(this, duration));\n         }\n\n         if (populated)\n         {\n            baseline.setNumber(BigInteger.valueOf(loop));\n            xmlTask.getBaseline().add(baseline);\n         }\n      }\n   }",
    "reference": "Writes task baseline data.\n\n@param xmlTask MSPDI task\n@param mpxjTask MPXJ task",
    "generated": "Write a project task baseline.\n\n@param xmlTask Task\n@param mpxjTask Task\n@param mpxjTask Task\n@return project task baseline"
  },
  {
    "code": "private void writeTaskExtendedAttributes(Project.Tasks.Task xml, Task mpx)\n   {\n      Project.Tasks.Task.ExtendedAttribute attrib;\n      List<Project.Tasks.Task.ExtendedAttribute> extendedAttributes = xml.getExtendedAttribute();\n\n      for (TaskField mpxFieldID : getAllTaskExtendedAttributes())\n      {\n         Object value = mpx.getCachedValue(mpxFieldID);\n\n         if (FieldTypeHelper.valueIsNotDefault(mpxFieldID, value))\n         {\n            m_extendedAttributesInUse.add(mpxFieldID);\n\n            Integer xmlFieldID = Integer.valueOf(MPPTaskField.getID(mpxFieldID) | MPPTaskField.TASK_FIELD_BASE);\n\n            attrib = m_factory.createProjectTasksTaskExtendedAttribute();\n            extendedAttributes.add(attrib);\n            attrib.setFieldID(xmlFieldID.toString());\n            attrib.setValue(DatatypeConverter.printExtendedAttribute(this, value, mpxFieldID.getDataType()));\n            attrib.setDurationFormat(printExtendedAttributeDurationFormat(value));\n         }\n      }\n   }",
    "reference": "This method writes extended attribute data for a task.\n\n@param xml MSPDI task\n@param mpx MPXJ task",
    "generated": "Write the task extended attributes."
  },
  {
    "code": "private BigInteger printExtendedAttributeDurationFormat(Object value)\n   {\n      BigInteger result = null;\n      if (value instanceof Duration)\n      {\n         result = DatatypeConverter.printDurationTimeUnits(((Duration) value).getUnits(), false);\n      }\n      return (result);\n   }",
    "reference": "Converts a duration to duration time units.\n\n@param value duration value\n@return duration time units",
    "generated": "Print the extended attribute duration format.\n\n@param value the value\n@return the extended attribute duration format"
  },
  {
    "code": "private BigInteger getTaskCalendarID(Task mpx)\n   {\n      BigInteger result = null;\n      ProjectCalendar cal = mpx.getCalendar();\n      if (cal != null)\n      {\n         result = NumberHelper.getBigInteger(cal.getUniqueID());\n      }\n      else\n      {\n         result = NULL_CALENDAR_ID;\n      }\n      return (result);\n   }",
    "reference": "This method retrieves the UID for a calendar associated with a task.\n\n@param mpx MPX Task instance\n@return calendar UID",
    "generated": "Get the task calendar ID.\n\n@param mpx Task object\n@return task calendar ID"
  },
  {
    "code": "private void writePredecessors(Project.Tasks.Task xml, Task mpx)\n   {\n      List<Project.Tasks.Task.PredecessorLink> list = xml.getPredecessorLink();\n\n      List<Relation> predecessors = mpx.getPredecessors();\n      for (Relation rel : predecessors)\n      {\n         Integer taskUniqueID = rel.getTargetTask().getUniqueID();\n         list.add(writePredecessor(taskUniqueID, rel.getType(), rel.getLag()));\n         m_eventManager.fireRelationWrittenEvent(rel);\n      }\n   }",
    "reference": "This method writes predecessor data to an MSPDI file.\nWe have to deal with a slight anomaly in this method that is introduced\nby the MPX file format. It would be possible for someone to create an\nMPX file with both the predecessor list and the unique ID predecessor\nlist populated... which means that we must process both and avoid adding\nduplicate predecessors. Also interesting to note is that MSP98 populates\nthe predecessor list, not the unique ID predecessor list, as you might\nexpect.\n\n@param xml MSPDI task data\n@param mpx MPX task data",
    "generated": "Write the predecessors of a task.\n\n@param xml Task object\n@param mpx Task object\n@param mpx Task object\n@return"
  },
  {
    "code": "private Project.Tasks.Task.PredecessorLink writePredecessor(Integer taskID, RelationType type, Duration lag)\n   {\n      Project.Tasks.Task.PredecessorLink link = m_factory.createProjectTasksTaskPredecessorLink();\n\n      link.setPredecessorUID(NumberHelper.getBigInteger(taskID));\n      link.setType(BigInteger.valueOf(type.getValue()));\n      link.setCrossProject(Boolean.FALSE); // SF-300: required to keep P6 happy when importing MSPDI files\n\n      if (lag != null && lag.getDuration() != 0)\n      {\n         double linkLag = lag.getDuration();\n         if (lag.getUnits() != TimeUnit.PERCENT && lag.getUnits() != TimeUnit.ELAPSED_PERCENT)\n         {\n            linkLag = 10.0 * Duration.convertUnits(linkLag, lag.getUnits(), TimeUnit.MINUTES, m_projectFile.getProjectProperties()).getDuration();\n         }\n         link.setLinkLag(BigInteger.valueOf((long) linkLag));\n         link.setLagFormat(DatatypeConverter.printDurationTimeUnits(lag.getUnits(), false));\n      }\n      else\n      {\n         // SF-329: default required to keep Powerproject happy when importing MSPDI files\n         link.setLinkLag(BIGINTEGER_ZERO);\n         link.setLagFormat(DatatypeConverter.printDurationTimeUnits(m_projectFile.getProjectProperties().getDefaultDurationUnits(), false));\n      }\n\n      return (link);\n   }",
    "reference": "This method writes a single predecessor link to the MSPDI file.\n\n@param taskID The task UID\n@param type The predecessor type\n@param lag The lag duration\n@return A new link to be added to the MSPDI file",
    "generated": "Write a Predecessor to the given task.\n\n@param taskID Task ID to write\n@param type Predecessor type"
  },
  {
    "code": "private void writeAssignments(Project project)\n   {\n      Project.Assignments assignments = m_factory.createProjectAssignments();\n      project.setAssignments(assignments);\n      List<Project.Assignments.Assignment> list = assignments.getAssignment();\n\n      for (ResourceAssignment assignment : m_projectFile.getResourceAssignments())\n      {\n         list.add(writeAssignment(assignment));\n      }\n\n      //\n      // Check to see if we have any tasks that have a percent complete value\n      // but do not have resource assignments. If any exist, then we must\n      // write a dummy resource assignment record to ensure that the MSPDI\n      // file shows the correct percent complete amount for the task.\n      //\n      ProjectConfig config = m_projectFile.getProjectConfig();\n      boolean autoUniqueID = config.getAutoAssignmentUniqueID();\n      if (!autoUniqueID)\n      {\n         config.setAutoAssignmentUniqueID(true);\n      }\n\n      for (Task task : m_projectFile.getTasks())\n      {\n         double percentComplete = NumberHelper.getDouble(task.getPercentageComplete());\n         if (percentComplete != 0 && task.getResourceAssignments().isEmpty() == true)\n         {\n            ResourceAssignment dummy = new ResourceAssignment(m_projectFile, task);\n            Duration duration = task.getDuration();\n            if (duration == null)\n            {\n               duration = Duration.getInstance(0, TimeUnit.HOURS);\n            }\n            double durationValue = duration.getDuration();\n            TimeUnit durationUnits = duration.getUnits();\n            double actualWork = (durationValue * percentComplete) / 100;\n            double remainingWork = durationValue - actualWork;\n\n            dummy.setResourceUniqueID(NULL_RESOURCE_ID);\n            dummy.setWork(duration);\n            dummy.setActualWork(Duration.getInstance(actualWork, durationUnits));\n            dummy.setRemainingWork(Duration.getInstance(remainingWork, durationUnits));\n            \n            // Without this, MS Project will mark a 100% complete milestone as 99% complete\n            if (percentComplete == 100 && duration.getDuration() == 0)\n            {              \n               dummy.setActualFinish(task.getActualStart());\n            }\n            \n            list.add(writeAssignment(dummy));\n         }\n      }\n\n      config.setAutoAssignmentUniqueID(autoUniqueID);\n   }",
    "reference": "This method writes assignment data to an MSPDI file.\n\n@param project Root node of the MSPDI file",
    "generated": "Write a project assignment record to ensure that the MSPDI\n      file shows the correct percent complete amount for the task."
  },
  {
    "code": "private void writeAssignmentBaselines(Project.Assignments.Assignment xml, ResourceAssignment mpxj)\n   {\n      Project.Assignments.Assignment.Baseline baseline = m_factory.createProjectAssignmentsAssignmentBaseline();\n      boolean populated = false;\n\n      Number cost = mpxj.getBaselineCost();\n      if (cost != null && cost.intValue() != 0)\n      {\n         populated = true;\n         baseline.setCost(DatatypeConverter.printExtendedAttributeCurrency(cost));\n      }\n\n      Date date = mpxj.getBaselineFinish();\n      if (date != null)\n      {\n         populated = true;\n         baseline.setFinish(DatatypeConverter.printExtendedAttributeDate(date));\n      }\n\n      date = mpxj.getBaselineStart();\n      if (date != null)\n      {\n         populated = true;\n         baseline.setStart(DatatypeConverter.printExtendedAttributeDate(date));\n      }\n\n      Duration duration = mpxj.getBaselineWork();\n      if (duration != null && duration.getDuration() != 0)\n      {\n         populated = true;\n         baseline.setWork(DatatypeConverter.printDuration(this, duration));\n      }\n\n      if (populated)\n      {\n         baseline.setNumber(\"0\");\n         xml.getBaseline().add(baseline);\n      }\n\n      for (int loop = 1; loop <= 10; loop++)\n      {\n         baseline = m_factory.createProjectAssignmentsAssignmentBaseline();\n         populated = false;\n\n         cost = mpxj.getBaselineCost(loop);\n         if (cost != null && cost.intValue() != 0)\n         {\n            populated = true;\n            baseline.setCost(DatatypeConverter.printExtendedAttributeCurrency(cost));\n         }\n\n         date = mpxj.getBaselineFinish(loop);\n         if (date != null)\n         {\n            populated = true;\n            baseline.setFinish(DatatypeConverter.printExtendedAttributeDate(date));\n         }\n\n         date = mpxj.getBaselineStart(loop);\n         if (date != null)\n         {\n            populated = true;\n            baseline.setStart(DatatypeConverter.printExtendedAttributeDate(date));\n         }\n\n         duration = mpxj.getBaselineWork(loop);\n         if (duration != null && duration.getDuration() != 0)\n         {\n            populated = true;\n            baseline.setWork(DatatypeConverter.printDuration(this, duration));\n         }\n\n         if (populated)\n         {\n            baseline.setNumber(Integer.toString(loop));\n            xml.getBaseline().add(baseline);\n         }\n      }\n   }",
    "reference": "Writes assignment baseline data.\n\n@param xml MSPDI assignment\n@param mpxj MPXJ assignment",
    "generated": "Write the project assignments for the given resource assignment.\n\n@param xml project assignment object\n@param mpxj resource assignment object\n@param mpxj resource assignment object\n@return null"
  },
  {
    "code": "private void writeAssignmentExtendedAttributes(Project.Assignments.Assignment xml, ResourceAssignment mpx)\n   {\n      Project.Assignments.Assignment.ExtendedAttribute attrib;\n      List<Project.Assignments.Assignment.ExtendedAttribute> extendedAttributes = xml.getExtendedAttribute();\n\n      for (AssignmentField mpxFieldID : getAllAssignmentExtendedAttributes())\n      {\n         Object value = mpx.getCachedValue(mpxFieldID);\n\n         if (FieldTypeHelper.valueIsNotDefault(mpxFieldID, value))\n         {\n            m_extendedAttributesInUse.add(mpxFieldID);\n\n            Integer xmlFieldID = Integer.valueOf(MPPAssignmentField.getID(mpxFieldID) | MPPAssignmentField.ASSIGNMENT_FIELD_BASE);\n\n            attrib = m_factory.createProjectAssignmentsAssignmentExtendedAttribute();\n            extendedAttributes.add(attrib);\n            attrib.setFieldID(xmlFieldID.toString());\n            attrib.setValue(DatatypeConverter.printExtendedAttribute(this, value, mpxFieldID.getDataType()));\n            attrib.setDurationFormat(printExtendedAttributeDurationFormat(value));\n         }\n      }\n   }",
    "reference": "This method writes extended attribute data for an assignment.\n\n@param xml MSPDI assignment\n@param mpx MPXJ assignment",
    "generated": "Write the extended attributes for a resource assignment.\n\n@param xml ResourceAssignment"
  },
  {
    "code": "private void writeAssignmentTimephasedData(ResourceAssignment mpx, Project.Assignments.Assignment xml)\n   {\n      if (m_writeTimphasedData && mpx.getHasTimephasedData())\n      {\n         List<TimephasedDataType> list = xml.getTimephasedData();\n         ProjectCalendar calendar = mpx.getCalendar();\n         BigInteger assignmentID = xml.getUID();\n\n         List<TimephasedWork> complete = mpx.getTimephasedActualWork();\n         List<TimephasedWork> planned = mpx.getTimephasedWork();\n\n         if (m_splitTimephasedAsDays)\n         {\n            TimephasedWork lastComplete = null;\n            if (complete != null && !complete.isEmpty())\n            {\n               lastComplete = complete.get(complete.size() - 1);\n            }\n\n            TimephasedWork firstPlanned = null;\n            if (planned != null && !planned.isEmpty())\n            {\n               firstPlanned = planned.get(0);\n            }\n\n            if (planned != null)\n            {\n               planned = splitDays(calendar, mpx.getTimephasedWork(), null, lastComplete);\n            }\n\n            if (complete != null)\n            {\n               complete = splitDays(calendar, complete, firstPlanned, null);\n            }\n         }\n\n         if (planned != null)\n         {\n            writeAssignmentTimephasedData(assignmentID, list, planned, 1);\n         }\n\n         if (complete != null)\n         {\n            writeAssignmentTimephasedData(assignmentID, list, complete, 2);\n         }\n      }\n   }",
    "reference": "Writes the timephased data for a resource assignment.\n\n@param mpx MPXJ assignment\n@param xml MSDPI assignment",
    "generated": "Write the timephased data.\n\n@param mpx the resource assignment\n@param xml the resource assignment to write\n@return the timephased data"
  },
  {
    "code": "private void writeAssignmentTimephasedData(BigInteger assignmentID, List<TimephasedDataType> list, List<TimephasedWork> data, int type)\n   {\n      for (TimephasedWork mpx : data)\n      {\n         TimephasedDataType xml = m_factory.createTimephasedDataType();\n         list.add(xml);\n\n         xml.setStart(mpx.getStart());\n         xml.setFinish(mpx.getFinish());\n         xml.setType(BigInteger.valueOf(type));\n         xml.setUID(assignmentID);\n         xml.setUnit(DatatypeConverter.printDurationTimeUnits(mpx.getTotalAmount(), false));\n         xml.setValue(DatatypeConverter.printDuration(this, mpx.getTotalAmount()));\n      }\n   }",
    "reference": "Writes a list of timephased data to the MSPDI file.\n\n@param assignmentID current assignment ID\n@param list output list of timephased data items\n@param data input list of timephased data\n@param type list type (planned or completed)",
    "generated": "Write a list of timephased data."
  },
  {
    "code": "private List<AssignmentField> getAllAssignmentExtendedAttributes()\n   {\n      ArrayList<AssignmentField> result = new ArrayList<AssignmentField>();\n      result.addAll(Arrays.asList(AssignmentFieldLists.CUSTOM_COST));\n      result.addAll(Arrays.asList(AssignmentFieldLists.CUSTOM_DATE));\n      result.addAll(Arrays.asList(AssignmentFieldLists.CUSTOM_DURATION));\n      result.addAll(Arrays.asList(AssignmentFieldLists.ENTERPRISE_COST));\n      result.addAll(Arrays.asList(AssignmentFieldLists.ENTERPRISE_DATE));\n      result.addAll(Arrays.asList(AssignmentFieldLists.ENTERPRISE_DURATION));\n      result.addAll(Arrays.asList(AssignmentFieldLists.ENTERPRISE_FLAG));\n      result.addAll(Arrays.asList(AssignmentFieldLists.ENTERPRISE_NUMBER));\n      result.addAll(Arrays.asList(AssignmentFieldLists.ENTERPRISE_RESOURCE_MULTI_VALUE));\n      result.addAll(Arrays.asList(AssignmentFieldLists.ENTERPRISE_RESOURCE_OUTLINE_CODE));\n      result.addAll(Arrays.asList(AssignmentFieldLists.ENTERPRISE_TEXT));\n      result.addAll(Arrays.asList(AssignmentFieldLists.CUSTOM_FINISH));\n      result.addAll(Arrays.asList(AssignmentFieldLists.CUSTOM_FLAG));\n      result.addAll(Arrays.asList(AssignmentFieldLists.CUSTOM_NUMBER));\n      result.addAll(Arrays.asList(AssignmentFieldLists.CUSTOM_START));\n      result.addAll(Arrays.asList(AssignmentFieldLists.CUSTOM_TEXT));\n      return result;\n   }",
    "reference": "Retrieve list of assignment extended attributes.\n\n@return list of extended attributes",
    "generated": "Generate a list of all assignment extended attributes."
  },
  {
    "code": "private List<TaskField> getAllTaskExtendedAttributes()\n   {\n      ArrayList<TaskField> result = new ArrayList<TaskField>();\n      result.addAll(Arrays.asList(TaskFieldLists.CUSTOM_TEXT));\n      result.addAll(Arrays.asList(TaskFieldLists.CUSTOM_START));\n      result.addAll(Arrays.asList(TaskFieldLists.CUSTOM_FINISH));\n      result.addAll(Arrays.asList(TaskFieldLists.CUSTOM_COST));\n      result.addAll(Arrays.asList(TaskFieldLists.CUSTOM_DATE));\n      result.addAll(Arrays.asList(TaskFieldLists.CUSTOM_FLAG));\n      result.addAll(Arrays.asList(TaskFieldLists.CUSTOM_NUMBER));\n      result.addAll(Arrays.asList(TaskFieldLists.CUSTOM_DURATION));\n      result.addAll(Arrays.asList(TaskFieldLists.CUSTOM_OUTLINE_CODE));\n      result.addAll(Arrays.asList(TaskFieldLists.ENTERPRISE_COST));\n      result.addAll(Arrays.asList(TaskFieldLists.ENTERPRISE_DATE));\n      result.addAll(Arrays.asList(TaskFieldLists.ENTERPRISE_DURATION));\n      result.addAll(Arrays.asList(TaskFieldLists.ENTERPRISE_FLAG));\n      result.addAll(Arrays.asList(TaskFieldLists.ENTERPRISE_NUMBER));\n      result.addAll(Arrays.asList(TaskFieldLists.ENTERPRISE_TEXT));\n      return result;\n   }",
    "reference": "Retrieve list of task extended attributes.\n\n@return list of extended attributes",
    "generated": "Generate a list of all task extended attributes."
  },
  {
    "code": "private List<ResourceField> getAllResourceExtendedAttributes()\n   {\n      ArrayList<ResourceField> result = new ArrayList<ResourceField>();\n      result.addAll(Arrays.asList(ResourceFieldLists.CUSTOM_TEXT));\n      result.addAll(Arrays.asList(ResourceFieldLists.CUSTOM_START));\n      result.addAll(Arrays.asList(ResourceFieldLists.CUSTOM_FINISH));\n      result.addAll(Arrays.asList(ResourceFieldLists.CUSTOM_COST));\n      result.addAll(Arrays.asList(ResourceFieldLists.CUSTOM_DATE));\n      result.addAll(Arrays.asList(ResourceFieldLists.CUSTOM_FLAG));\n      result.addAll(Arrays.asList(ResourceFieldLists.CUSTOM_NUMBER));\n      result.addAll(Arrays.asList(ResourceFieldLists.CUSTOM_DURATION));\n      result.addAll(Arrays.asList(ResourceFieldLists.CUSTOM_OUTLINE_CODE));\n      result.addAll(Arrays.asList(ResourceFieldLists.ENTERPRISE_COST));\n      result.addAll(Arrays.asList(ResourceFieldLists.ENTERPRISE_DATE));\n      result.addAll(Arrays.asList(ResourceFieldLists.ENTERPRISE_DURATION));\n      result.addAll(Arrays.asList(ResourceFieldLists.ENTERPRISE_FLAG));\n      result.addAll(Arrays.asList(ResourceFieldLists.ENTERPRISE_NUMBER));\n      result.addAll(Arrays.asList(ResourceFieldLists.ENTERPRISE_TEXT));\n      return result;\n   }",
    "reference": "Retrieve list of resource extended attributes.\n\n@return list of extended attributes",
    "generated": "Generate a list of all resource extended attributes."
  },
  {
    "code": "private void processUDF(UDFTypeType udf)\n   {\n      FieldTypeClass fieldType = FIELD_TYPE_MAP.get(udf.getSubjectArea());\n      if (fieldType != null)\n      {\n         UserFieldDataType dataType = UserFieldDataType.getInstanceFromXmlName(udf.getDataType());\n         String name = udf.getTitle();\n         FieldType field = addUserDefinedField(fieldType, dataType, name);\n         if (field != null)\n         {\n            m_fieldTypeMap.put(udf.getObjectId(), field);\n         }\n      }\n   }",
    "reference": "Process an individual UDF.\n\n@param udf UDF definition",
    "generated": "Process the UDF type type.\n\n@param udf UDF type type"
  },
  {
    "code": "private FieldType addUserDefinedField(FieldTypeClass fieldType, UserFieldDataType dataType, String name)\n   {\n      FieldType field = null;\n      \n      try\n      {\n         switch (fieldType)\n         {\n            case TASK:\n            {\n               do\n               {\n                  field = m_taskUdfCounters.nextField(TaskField.class, dataType);\n               }\n               while (RESERVED_TASK_FIELDS.contains(field));\n\n               m_projectFile.getCustomFields().getCustomField(field).setAlias(name);\n\n               break;\n            }\n            \n            case RESOURCE:\n            {\n               field = m_resourceUdfCounters.nextField(ResourceField.class, dataType);\n               m_projectFile.getCustomFields().getCustomField(field).setAlias(name);\n               break;\n            }\n            \n            case ASSIGNMENT:\n            {\n               field = m_assignmentUdfCounters.nextField(AssignmentField.class, dataType);\n               m_projectFile.getCustomFields().getCustomField(field).setAlias(name);\n               break;\n            }\n            \n            default:\n            {\n               break;\n            }\n         }         \n      }\n\n      catch (Exception ex)\n      {\n         //\n         // SF#227: If we get an exception thrown here... it's likely that\n         // we've run out of user defined fields, for example\n         // there are only 30 TEXT fields. We'll ignore this: the user\n         // defined field won't be mapped to an alias, so we'll\n         // ignore it when we read in the values.\n         //\n      }\n      \n      return field;\n   }",
    "reference": "Map the Primavera UDF to a custom field.\n\n@param fieldType parent object type\n@param dataType UDF data type\n@param name UDF name\n@return FieldType instance",
    "generated": "Add a user defined field.\n\n@param fieldType UserFieldDataType dataType\n@param name name"
  },
  {
    "code": "private Double zeroIsNull(Double value)\n   {\n      if (value != null && value.doubleValue() == 0)\n      {\n         value = null;\n      }\n      return value;\n   }",
    "reference": "Render a zero Double as null.\n\n@param value double value\n@return null if the double value is zero",
    "generated": "Returns zero if the value is null."
  },
  {
    "code": "private Duration getDuration(Double duration)\n   {\n      Duration result = null;\n\n      if (duration != null)\n      {\n         result = Duration.getInstance(NumberHelper.getDouble(duration), TimeUnit.HOURS);\n      }\n\n      return result;\n   }",
    "reference": "Extracts a duration from a JAXBElement instance.\n\n@param duration duration expressed in hours\n@return duration instance",
    "generated": "Get the duration.\n\n@param duration duration"
  },
  {
    "code": "private void readUDFTypes(FieldContainer mpxj, List<UDFAssignmentType> udfs)\n   {\n      for (UDFAssignmentType udf : udfs)\n      {\n         FieldType fieldType = m_fieldTypeMap.get(Integer.valueOf(udf.getTypeObjectId()));\n         if (fieldType != null)\n         {\n            mpxj.set(fieldType, getUdfValue(udf));\n         }\n      }\n   }",
    "reference": "Process UDFs for a specific object.\n\n@param mpxj field container\n@param udfs UDF values",
    "generated": "Read UDF types.\n\n@param mpxj the field container\n@param udfs the list of UDF types"
  },
  {
    "code": "private Object getUdfValue(UDFAssignmentType udf)\n   {\n      if (udf.getCostValue() != null)\n      {\n         return udf.getCostValue();\n      }\n      \n      if (udf.getDoubleValue() != null)\n      {\n         return udf.getDoubleValue();\n      }\n      \n      if (udf.getFinishDateValue() != null)\n      {\n         return udf.getFinishDateValue();\n      }\n      \n      if (udf.getIndicatorValue() != null)\n      {\n         return udf.getIndicatorValue();\n      }\n      \n      if (udf.getIntegerValue() != null)\n      {\n         return udf.getIntegerValue();\n      }\n      \n      if (udf.getStartDateValue() != null)\n      {\n         return udf.getStartDateValue();\n      }\n      \n      if (udf.getTextValue() != null)\n      {\n         return udf.getTextValue();\n      }\n      \n      return null;\n   }",
    "reference": "Retrieve the value of a UDF.\n\n@param udf UDF value holder\n@return UDF value",
    "generated": "Get the UDF value.\n\n@param udf UDF value"
  },
  {
    "code": "private Integer mapTaskID(Integer id)\n   {\n      Integer mappedID = m_clashMap.get(id);\n      if (mappedID == null)\n      {\n         mappedID = id;\n      }\n      return (mappedID);\n   }",
    "reference": "Deals with the case where we have had to map a task ID to a new value.\n\n@param id task ID from database\n@return mapped task ID",
    "generated": "Map task ID.\n\n@param id Task ID"
  },
  {
    "code": "public int evaluate(FieldContainer container)\n   {\n      //\n      // First step - determine the list of criteria we are should use\n      //\n      List<GraphicalIndicatorCriteria> criteria;\n      if (container instanceof Task)\n      {\n         Task task = (Task) container;\n         if (NumberHelper.getInt(task.getUniqueID()) == 0)\n         {\n            if (m_projectSummaryInheritsFromSummaryRows == false)\n            {\n               criteria = m_projectSummaryCriteria;\n            }\n            else\n            {\n               if (m_summaryRowsInheritFromNonSummaryRows == false)\n               {\n                  criteria = m_summaryRowCriteria;\n               }\n               else\n               {\n                  criteria = m_nonSummaryRowCriteria;\n               }\n            }\n         }\n         else\n         {\n            if (task.getSummary() == true)\n            {\n               if (m_summaryRowsInheritFromNonSummaryRows == false)\n               {\n                  criteria = m_summaryRowCriteria;\n               }\n               else\n               {\n                  criteria = m_nonSummaryRowCriteria;\n               }\n            }\n            else\n            {\n               criteria = m_nonSummaryRowCriteria;\n            }\n         }\n      }\n      else\n      {\n         // It is possible to have a resource summary row, but at the moment\n         // I can't see how you can determine this.\n         criteria = m_nonSummaryRowCriteria;\n      }\n\n      //\n      // Now we have the criteria, evaluate each one until we get a result\n      //\n      int result = -1;\n      for (GraphicalIndicatorCriteria gic : criteria)\n      {\n         result = gic.evaluate(container);\n         if (result != -1)\n         {\n            break;\n         }\n      }\n\n      //\n      // If we still don't have a result at the end, return the\n      // default value, which is 0\n      //\n      if (result == -1)\n      {\n         result = 0;\n      }\n\n      return (result);\n   }",
    "reference": "This method evaluates a if a graphical indicator should\nbe displayed, given a set of Task or Resource data. The\nmethod will return -1 if no indicator should be displayed.\n\n@param container Task or Resource instance\n@return indicator index",
    "generated": "Determine the list of criteria we are should use\n      //\n@param container the field container\n@return the value of the result"
  },
  {
    "code": "private void writeFileCreationRecord() throws IOException\n   {\n      ProjectProperties properties = m_projectFile.getProjectProperties();\n\n      m_buffer.setLength(0);\n      m_buffer.append(\"MPX\");\n      m_buffer.append(m_delimiter);\n      m_buffer.append(properties.getMpxProgramName());\n      m_buffer.append(m_delimiter);\n      m_buffer.append(properties.getMpxFileVersion());\n      m_buffer.append(m_delimiter);\n      m_buffer.append(properties.getMpxCodePage());\n      m_buffer.append(MPXConstants.EOL);\n      m_writer.write(m_buffer.toString());\n   }",
    "reference": "Write file creation record.\n\n@throws IOException",
    "generated": "Write the file creation record.\n\n@param m_projectFile project file\n@param m_projectFile project file\n@param m_projectFile project file\n@param m_projectFile project file\n@param m_projectFile version of the file\n@param m_delimiter"
  },
  {
    "code": "private void writeCalendar(ProjectCalendar record) throws IOException\n   {\n      //\n      // Test used to ensure that we don't write the default calendar used for the \"Unassigned\" resource\n      //\n      if (record.getParent() == null || record.getResource() != null)\n      {\n         m_buffer.setLength(0);\n\n         if (record.getParent() == null)\n         {\n            m_buffer.append(MPXConstants.BASE_CALENDAR_RECORD_NUMBER);\n            m_buffer.append(m_delimiter);\n            if (record.getName() != null)\n            {\n               m_buffer.append(record.getName());\n            }\n         }\n         else\n         {\n            m_buffer.append(MPXConstants.RESOURCE_CALENDAR_RECORD_NUMBER);\n            m_buffer.append(m_delimiter);\n            m_buffer.append(record.getParent().getName());\n         }\n\n         for (DayType day : record.getDays())\n         {\n            if (day == null)\n            {\n               day = DayType.DEFAULT;\n            }\n            m_buffer.append(m_delimiter);\n            m_buffer.append(day.getValue());\n         }\n\n         m_buffer.append(MPXConstants.EOL);\n         m_writer.write(m_buffer.toString());\n\n         ProjectCalendarHours[] hours = record.getHours();\n         for (int loop = 0; loop < hours.length; loop++)\n         {\n            if (hours[loop] != null)\n            {\n               writeCalendarHours(record, hours[loop]);\n            }\n         }\n\n         if (!record.getCalendarExceptions().isEmpty())\n         {\n            //\n            // A quirk of MS Project is that these exceptions must be\n            // in date order in the file, otherwise they are ignored.\n            // The getCalendarExceptions method now guarantees that\n            // the exceptions list is sorted when retrieved.\n            //\n            for (ProjectCalendarException ex : record.getCalendarExceptions())\n            {\n               writeCalendarException(record, ex);\n            }\n         }\n\n         m_eventManager.fireCalendarWrittenEvent(record);\n      }\n   }",
    "reference": "Write a calendar.\n\n@param record calendar instance\n@throws IOException",
    "generated": "Write the default calendar used for the \"Unassigned\" resource\n\n@param record ProjectCalendar to write"
  },
  {
    "code": "private void writeCalendarHours(ProjectCalendar parentCalendar, ProjectCalendarHours record) throws IOException\n   {\n      m_buffer.setLength(0);\n\n      int recordNumber;\n\n      if (!parentCalendar.isDerived())\n      {\n         recordNumber = MPXConstants.BASE_CALENDAR_HOURS_RECORD_NUMBER;\n      }\n      else\n      {\n         recordNumber = MPXConstants.RESOURCE_CALENDAR_HOURS_RECORD_NUMBER;\n      }\n\n      DateRange range1 = record.getRange(0);\n      if (range1 == null)\n      {\n         range1 = DateRange.EMPTY_RANGE;\n      }\n\n      DateRange range2 = record.getRange(1);\n      if (range2 == null)\n      {\n         range2 = DateRange.EMPTY_RANGE;\n      }\n\n      DateRange range3 = record.getRange(2);\n      if (range3 == null)\n      {\n         range3 = DateRange.EMPTY_RANGE;\n      }\n\n      m_buffer.append(recordNumber);\n      m_buffer.append(m_delimiter);\n      m_buffer.append(format(record.getDay()));\n      m_buffer.append(m_delimiter);\n      m_buffer.append(format(formatTime(range1.getStart())));\n      m_buffer.append(m_delimiter);\n      m_buffer.append(format(formatTime(range1.getEnd())));\n      m_buffer.append(m_delimiter);\n      m_buffer.append(format(formatTime(range2.getStart())));\n      m_buffer.append(m_delimiter);\n      m_buffer.append(format(formatTime(range2.getEnd())));\n      m_buffer.append(m_delimiter);\n      m_buffer.append(format(formatTime(range3.getStart())));\n      m_buffer.append(m_delimiter);\n      m_buffer.append(format(formatTime(range3.getEnd())));\n      stripTrailingDelimiters(m_buffer);\n      m_buffer.append(MPXConstants.EOL);\n\n      m_writer.write(m_buffer.toString());\n   }",
    "reference": "Write calendar hours.\n\n@param parentCalendar parent calendar instance\n@param record calendar hours instance\n@throws IOException",
    "generated": "Write a ProjectCalendarHours to a ProjectCalendarHours object.\n\n@param parentCalendar ProjectCalendar instance\n@param record ProjectCalendarHours record"
  },
  {
    "code": "private void writeResource(Resource record) throws IOException\n   {\n      m_buffer.setLength(0);\n\n      //\n      // Write the resource record\n      //\n      int[] fields = m_resourceModel.getModel();\n\n      m_buffer.append(MPXConstants.RESOURCE_RECORD_NUMBER);\n      for (int loop = 0; loop < fields.length; loop++)\n      {\n         int mpxFieldType = fields[loop];\n         if (mpxFieldType == -1)\n         {\n            break;\n         }\n\n         ResourceField resourceField = MPXResourceField.getMpxjField(mpxFieldType);\n         Object value = record.getCachedValue(resourceField);\n         value = formatType(resourceField.getDataType(), value);\n\n         m_buffer.append(m_delimiter);\n         m_buffer.append(format(value));\n      }\n\n      stripTrailingDelimiters(m_buffer);\n      m_buffer.append(MPXConstants.EOL);\n      m_writer.write(m_buffer.toString());\n\n      //\n      // Write the resource notes\n      //\n      String notes = record.getNotes();\n      if (notes.length() != 0)\n      {\n         writeNotes(MPXConstants.RESOURCE_NOTES_RECORD_NUMBER, notes);\n      }\n\n      //\n      // Write the resource calendar\n      //\n      if (record.getResourceCalendar() != null)\n      {\n         writeCalendar(record.getResourceCalendar());\n      }\n\n      m_eventManager.fireResourceWrittenEvent(record);\n   }",
    "reference": "Write a resource.\n\n@param record resource instance\n@throws IOException",
    "generated": "Write a resource.\n\n@param record the resource to write\n@return the resource to write"
  },
  {
    "code": "private void writeNotes(int recordNumber, String text) throws IOException\n   {\n      m_buffer.setLength(0);\n\n      m_buffer.append(recordNumber);\n      m_buffer.append(m_delimiter);\n\n      if (text != null)\n      {\n         String note = stripLineBreaks(text, MPXConstants.EOL_PLACEHOLDER_STRING);\n         boolean quote = (note.indexOf(m_delimiter) != -1 || note.indexOf('\"') != -1);\n         int length = note.length();\n         char c;\n\n         if (quote == true)\n         {\n            m_buffer.append('\"');\n         }\n\n         for (int loop = 0; loop < length; loop++)\n         {\n            c = note.charAt(loop);\n\n            switch (c)\n            {\n               case '\"':\n               {\n                  m_buffer.append(\"\\\"\\\"\");\n                  break;\n               }\n\n               default:\n               {\n                  m_buffer.append(c);\n                  break;\n               }\n            }\n         }\n\n         if (quote == true)\n         {\n            m_buffer.append('\"');\n         }\n      }\n\n      m_buffer.append(MPXConstants.EOL);\n\n      m_writer.write(m_buffer.toString());\n   }",
    "reference": "Write notes.\n\n@param recordNumber record number\n@param text note text\n@throws IOException",
    "generated": "Write the notes of a record.\n\n@param recordNumber record number\n@param text text to write"
  },
  {
    "code": "private void writeResourceAssignment(ResourceAssignment record) throws IOException\n   {\n      m_buffer.setLength(0);\n\n      m_buffer.append(MPXConstants.RESOURCE_ASSIGNMENT_RECORD_NUMBER);\n      m_buffer.append(m_delimiter);\n      m_buffer.append(formatResource(record.getResource()));\n      m_buffer.append(m_delimiter);\n      m_buffer.append(format(formatUnits(record.getUnits())));\n      m_buffer.append(m_delimiter);\n      m_buffer.append(format(formatDuration(record.getWork())));\n      m_buffer.append(m_delimiter);\n      m_buffer.append(format(formatDuration(record.getBaselineWork())));\n      m_buffer.append(m_delimiter);\n      m_buffer.append(format(formatDuration(record.getActualWork())));\n      m_buffer.append(m_delimiter);\n      m_buffer.append(format(formatDuration(record.getOvertimeWork())));\n      m_buffer.append(m_delimiter);\n      m_buffer.append(format(formatCurrency(record.getCost())));\n      m_buffer.append(m_delimiter);\n      m_buffer.append(format(formatCurrency(record.getBaselineCost())));\n      m_buffer.append(m_delimiter);\n      m_buffer.append(format(formatCurrency(record.getActualCost())));\n      m_buffer.append(m_delimiter);\n      m_buffer.append(format(formatDateTime(record.getStart())));\n      m_buffer.append(m_delimiter);\n      m_buffer.append(format(formatDateTime(record.getFinish())));\n      m_buffer.append(m_delimiter);\n      m_buffer.append(format(formatDuration(record.getDelay())));\n      m_buffer.append(m_delimiter);\n      m_buffer.append(format(record.getResourceUniqueID()));\n      stripTrailingDelimiters(m_buffer);\n      m_buffer.append(MPXConstants.EOL);\n      m_writer.write(m_buffer.toString());\n\n      ResourceAssignmentWorkgroupFields workgroup = record.getWorkgroupAssignment();\n      if (workgroup == null)\n      {\n         workgroup = ResourceAssignmentWorkgroupFields.EMPTY;\n      }\n      writeResourceAssignmentWorkgroupFields(workgroup);\n\n      m_eventManager.fireAssignmentWrittenEvent(record);\n   }",
    "reference": "Write resource assignment.\n\n@param record resource assignment instance\n@throws IOException",
    "generated": "Write a resource assignment record.\n\n@param record ResourceAssignment record\n@param record ResourceAssignment record\n@throws IOException"
  },
  {
    "code": "private void writeResourceAssignmentWorkgroupFields(ResourceAssignmentWorkgroupFields record) throws IOException\n   {\n      m_buffer.setLength(0);\n\n      m_buffer.append(MPXConstants.RESOURCE_ASSIGNMENT_WORKGROUP_FIELDS_RECORD_NUMBER);\n      m_buffer.append(m_delimiter);\n      m_buffer.append(format(record.getMessageUniqueID()));\n      m_buffer.append(m_delimiter);\n      m_buffer.append(record.getConfirmed() ? \"1\" : \"0\");\n      m_buffer.append(m_delimiter);\n      m_buffer.append(record.getResponsePending() ? \"1\" : \"0\");\n      m_buffer.append(m_delimiter);\n      m_buffer.append(format(formatDateTimeNull(record.getUpdateStart())));\n      m_buffer.append(m_delimiter);\n      m_buffer.append(format(formatDateTimeNull(record.getUpdateFinish())));\n      m_buffer.append(m_delimiter);\n      m_buffer.append(format(record.getScheduleID()));\n\n      stripTrailingDelimiters(m_buffer);\n      m_buffer.append(MPXConstants.EOL);\n\n      m_writer.write(m_buffer.toString());\n   }",
    "reference": "Write resource assignment workgroup.\n\n@param record resource assignment workgroup instance\n@throws IOException",
    "generated": "Write a resource assignment workgroup fields.\n\n@param record ResourceAssignmentWorkgroupFields record"
  },
  {
    "code": "private void writeTasks(List<Task> tasks) throws IOException\n   {\n      for (Task task : tasks)\n      {\n         writeTask(task);\n         writeTasks(task.getChildTasks());\n      }\n   }",
    "reference": "Recursively write tasks.\n\n@param tasks list of tasks\n@throws IOException",
    "generated": "Write a list of tasks.\n\n@param tasks list of tasks\n@return"
  },
  {
    "code": "private Integer getIntegerTimeInMinutes(Date date)\n   {\n      Integer result = null;\n      if (date != null)\n      {\n         Calendar cal = DateHelper.popCalendar(date);\n         int time = cal.get(Calendar.HOUR_OF_DAY) * 60;\n         time += cal.get(Calendar.MINUTE);\n         DateHelper.pushCalendar(cal);\n         result = Integer.valueOf(time);         \n      }\n      return (result);\n   }",
    "reference": "This internal method is used to convert from a Date instance to an\ninteger representing the number of minutes past midnight.\n\n@param date date instance\n@return minutes past midnight as an integer",
    "generated": "Get the timeInMinutes of a date.\n\n@param date Date"
  },
  {
    "code": "private String escapeQuotes(String value)\n   {\n      StringBuilder sb = new StringBuilder();\n      int length = value.length();\n      char c;\n\n      sb.append('\"');\n      for (int index = 0; index < length; index++)\n      {\n         c = value.charAt(index);\n         sb.append(c);\n\n         if (c == '\"')\n         {\n            sb.append('\"');\n         }\n      }\n      sb.append('\"');\n\n      return (sb.toString());\n   }",
    "reference": "This method is called when double quotes are found as part of\na value. The quotes are escaped by adding a second quote character\nand the entire value is quoted.\n\n@param value text containing quote characters\n@return escaped and quoted text",
    "generated": "Generate a quoted string.\n\n@param value the value to escape\n@return the quoted string"
  },
  {
    "code": "private String stripLineBreaks(String text, String replacement)\n   {\n      if (text.indexOf('\\r') != -1 || text.indexOf('\\n') != -1)\n      {\n         StringBuilder sb = new StringBuilder(text);\n\n         int index;\n\n         while ((index = sb.indexOf(\"\\r\\n\")) != -1)\n         {\n            sb.replace(index, index + 2, replacement);\n         }\n\n         while ((index = sb.indexOf(\"\\n\\r\")) != -1)\n         {\n            sb.replace(index, index + 2, replacement);\n         }\n\n         while ((index = sb.indexOf(\"\\r\")) != -1)\n         {\n            sb.replace(index, index + 1, replacement);\n         }\n\n         while ((index = sb.indexOf(\"\\n\")) != -1)\n         {\n            sb.replace(index, index + 1, replacement);\n         }\n\n         text = sb.toString();\n      }\n\n      return (text);\n   }",
    "reference": "This method removes line breaks from a piece of text, and replaces\nthem with the supplied text.\n\n@param text source text\n@param replacement line break replacement text\n@return text with line breaks removed.",
    "generated": "Strip line breaks.\n\n@param text text to strip"
  },
  {
    "code": "private String format(Object o)\n   {\n      String result;\n\n      if (o == null)\n      {\n         result = \"\";\n      }\n      else\n      {\n         if (o instanceof Boolean == true)\n         {\n            result = LocaleData.getString(m_locale, (((Boolean) o).booleanValue() == true ? LocaleData.YES : LocaleData.NO));\n         }\n         else\n         {\n            if (o instanceof Float == true || o instanceof Double == true)\n            {\n               result = (m_formats.getDecimalFormat().format(((Number) o).doubleValue()));\n            }\n            else\n            {\n               if (o instanceof Day)\n               {\n                  result = Integer.toString(((Day) o).getValue());\n               }\n               else\n               {\n                  result = o.toString();\n               }\n            }\n         }\n\n         //\n         // At this point there should be no line break characters in\n         // the file. If we find any, replace them with spaces\n         //\n         result = stripLineBreaks(result, MPXConstants.EOL_PLACEHOLDER_STRING);\n\n         //\n         // Finally we check to ensure that there are no embedded\n         // quotes or separator characters in the value. If there are, then\n         // we quote the value and escape any existing quote characters.\n         //\n         if (result.indexOf('\"') != -1)\n         {\n            result = escapeQuotes(result);\n         }\n         else\n         {\n            if (result.indexOf(m_delimiter) != -1)\n            {\n               result = '\"' + result + '\"';\n            }\n         }\n      }\n\n      return (result);\n   }",
    "reference": "This method returns the string representation of an object. In most\ncases this will simply involve calling the normal toString method\non the object, but a couple of exceptions are handled here.\n\n@param o the object to formatted\n@return formatted string representing input Object",
    "generated": "Format the object. If we find any, replace them with spaces\n\n@param o object\n@return the formatted string"
  },
  {
    "code": "private void stripTrailingDelimiters(StringBuilder buffer)\n   {\n      int index = buffer.length() - 1;\n\n      while (index > 0 && buffer.charAt(index) == m_delimiter)\n      {\n         --index;\n      }\n\n      buffer.setLength(index + 1);\n   }",
    "reference": "This method removes trailing delimiter characters.\n\n@param buffer input sring buffer",
    "generated": "Strip trailing delimiters.\n\n@param buffer the buffer to strip"
  },
  {
    "code": "private String formatTime(Date value)\n   {\n      return (value == null ? null : m_formats.getTimeFormat().format(value));\n   }",
    "reference": "This method is called to format a time value.\n\n@param value time value\n@return formatted time value",
    "generated": "Format the time value.\n\n@param value Time value\n@return Time value"
  },
  {
    "code": "private String formatCurrency(Number value)\n   {\n      return (value == null ? null : m_formats.getCurrencyFormat().format(value));\n   }",
    "reference": "This method is called to format a currency value.\n\n@param value numeric value\n@return currency value",
    "generated": "Format a currency value.\n\n@param value Currency value"
  },
  {
    "code": "private String formatUnits(Number value)\n   {\n      return (value == null ? null : m_formats.getUnitsDecimalFormat().format(value.doubleValue() / 100));\n   }",
    "reference": "This method is called to format a units value.\n\n@param value numeric value\n@return currency value",
    "generated": "Format the units of a number.\n\n@param value the unit value\n@return the unit value"
  },
  {
    "code": "private String formatDateTimeNull(Date value)\n   {\n      return (value == null ? m_formats.getNullText() : m_formats.getDateTimeFormat().format(value));\n   }",
    "reference": "This method is called to format a date. It will return the null text\nif a null value is supplied.\n\n@param value date value\n@return formatted date value",
    "generated": "Format a DateTime value.\n\n@param value DateTime value"
  },
  {
    "code": "private String formatPercentage(Number value)\n   {\n      return (value == null ? null : m_formats.getPercentageDecimalFormat().format(value) + \"%\");\n   }",
    "reference": "This method is called to format a percentage value.\n\n@param value numeric value\n@return percentage value",
    "generated": "Format a percentage value.\n\n@param value value value"
  },
  {
    "code": "private String formatAccrueType(AccrueType type)\n   {\n      return (type == null ? null : LocaleData.getStringArray(m_locale, LocaleData.ACCRUE_TYPES)[type.getValue() - 1]);\n   }",
    "reference": "This method is called to format an accrue type value.\n\n@param type accrue type\n@return formatted accrue type",
    "generated": "Format the accrue type.\n\n@param type the accrue type\n@return the accrue type"
  },
  {
    "code": "private String formatConstraintType(ConstraintType type)\n   {\n      return (type == null ? null : LocaleData.getStringArray(m_locale, LocaleData.CONSTRAINT_TYPES)[type.getValue()]);\n   }",
    "reference": "This method is called to format a constraint type.\n\n@param type constraint type\n@return formatted constraint type",
    "generated": "Format a constraint type.\n\n@param type constraint type"
  },
  {
    "code": "private String formatDuration(Object value)\n   {\n      String result = null;\n      if (value instanceof Duration)\n      {\n         Duration duration = (Duration) value;\n         result = m_formats.getDurationDecimalFormat().format(duration.getDuration()) + formatTimeUnit(duration.getUnits());\n      }\n      return result;\n   }",
    "reference": "This method is called to format a duration.\n\n@param value duration value\n@return formatted duration value",
    "generated": "Format a duration value.\n\n@param value the duration value\n@return the duration value"
  },
  {
    "code": "private String formatRate(Rate value)\n   {\n      String result = null;\n      if (value != null)\n      {\n         StringBuilder buffer = new StringBuilder(m_formats.getCurrencyFormat().format(value.getAmount()));\n         buffer.append(\"/\");\n         buffer.append(formatTimeUnit(value.getUnits()));\n         result = buffer.toString();\n      }\n      return (result);\n   }",
    "reference": "This method is called to format a rate.\n\n@param value rate value\n@return formatted rate",
    "generated": "Format a rate.\n\n@param value rate value"
  },
  {
    "code": "private String formatPriority(Priority value)\n   {\n      String result = null;\n\n      if (value != null)\n      {\n         String[] priorityTypes = LocaleData.getStringArray(m_locale, LocaleData.PRIORITY_TYPES);\n         int priority = value.getValue();\n         if (priority < Priority.LOWEST)\n         {\n            priority = Priority.LOWEST;\n         }\n         else\n         {\n            if (priority > Priority.DO_NOT_LEVEL)\n            {\n               priority = Priority.DO_NOT_LEVEL;\n            }\n         }\n\n         priority /= 100;\n\n         result = priorityTypes[priority - 1];\n      }\n\n      return (result);\n   }",
    "reference": "This method is called to format a priority.\n\n@param value priority instance\n@return formatted priority value",
    "generated": "Format a priority value.\n\n@param value Priority value\n@return Priority value"
  },
  {
    "code": "private String formatTaskType(TaskType value)\n   {\n      return (LocaleData.getString(m_locale, (value == TaskType.FIXED_DURATION ? LocaleData.YES : LocaleData.NO)));\n   }",
    "reference": "This method is called to format a task type.\n\n@param value task type value\n@return formatted task type",
    "generated": "Format the task type.\n\n@param value TaskType value\n@return the task type"
  },
  {
    "code": "private String formatRelationList(List<Relation> value)\n   {\n      String result = null;\n\n      if (value != null && value.size() != 0)\n      {\n         StringBuilder sb = new StringBuilder();\n         for (Relation relation : value)\n         {\n            if (sb.length() != 0)\n            {\n               sb.append(m_delimiter);\n            }\n\n            sb.append(formatRelation(relation));\n         }\n\n         result = sb.toString();\n      }\n\n      return (result);\n   }",
    "reference": "This method is called to format a relation list.\n\n@param value relation list instance\n@return formatted relation list",
    "generated": "Format a list of relations.\n\n@param value list of relations"
  },
  {
    "code": "private String formatRelation(Relation relation)\n   {\n      String result = null;\n\n      if (relation != null)\n      {\n         StringBuilder sb = new StringBuilder(relation.getTargetTask().getID().toString());\n\n         Duration duration = relation.getLag();\n         RelationType type = relation.getType();\n         double durationValue = duration.getDuration();\n\n         if ((durationValue != 0) || (type != RelationType.FINISH_START))\n         {\n            String[] typeNames = LocaleData.getStringArray(m_locale, LocaleData.RELATION_TYPES);\n            sb.append(typeNames[type.getValue()]);\n         }\n\n         if (durationValue != 0)\n         {\n            if (durationValue > 0)\n            {\n               sb.append('+');\n            }\n\n            sb.append(formatDuration(duration));\n         }\n\n         result = sb.toString();\n      }\n\n      m_eventManager.fireRelationWrittenEvent(relation);\n      return (result);\n   }",
    "reference": "This method is called to format a relation.\n\n@param relation relation instance\n@return formatted relation instance",
    "generated": "Format a relation.\n\n@param relation relation"
  },
  {
    "code": "private String formatTimeUnit(TimeUnit timeUnit)\n   {\n      int units = timeUnit.getValue();\n      String result;\n      String[][] unitNames = LocaleData.getStringArrays(m_locale, LocaleData.TIME_UNITS_ARRAY);\n\n      if (units < 0 || units >= unitNames.length)\n      {\n         result = \"\";\n      }\n      else\n      {\n         result = unitNames[units][0];\n      }\n\n      return (result);\n   }",
    "reference": "This method formats a time unit.\n\n@param timeUnit time unit instance\n@return formatted time unit instance",
    "generated": "Format a time unit.\n\n@param timeUnit timeUnit"
  },
  {
    "code": "@SuppressWarnings(\"unchecked\") private Object formatType(DataType type, Object value)\n   {\n      switch (type)\n      {\n         case DATE:\n         {\n            value = formatDateTime(value);\n            break;\n         }\n\n         case CURRENCY:\n         {\n            value = formatCurrency((Number) value);\n            break;\n         }\n\n         case UNITS:\n         {\n            value = formatUnits((Number) value);\n            break;\n         }\n\n         case PERCENTAGE:\n         {\n            value = formatPercentage((Number) value);\n            break;\n         }\n\n         case ACCRUE:\n         {\n            value = formatAccrueType((AccrueType) value);\n            break;\n         }\n\n         case CONSTRAINT:\n         {\n            value = formatConstraintType((ConstraintType) value);\n            break;\n         }\n\n         case WORK:\n         case DURATION:\n         {\n            value = formatDuration(value);\n            break;\n         }\n\n         case RATE:\n         {\n            value = formatRate((Rate) value);\n            break;\n         }\n\n         case PRIORITY:\n         {\n            value = formatPriority((Priority) value);\n            break;\n         }\n\n         case RELATION_LIST:\n         {\n            value = formatRelationList((List<Relation>) value);\n            break;\n         }\n\n         case TASK_TYPE:\n         {\n            value = formatTaskType((TaskType) value);\n            break;\n         }\n\n         default:\n         {\n            break;\n         }\n      }\n\n      return (value);\n   }",
    "reference": "Converts a value to the appropriate type.\n\n@param type target type\n@param value input value\n@return output value",
    "generated": "Format a data type.\n\n@param type data type\n@param value data type\n@return data type"
  },
  {
    "code": "public final boolean getBoolean(String name)\n   {\n      boolean result = false;\n      Boolean value = (Boolean) getObject(name);\n      if (value != null)\n      {\n         result = BooleanHelper.getBoolean(value);\n      }\n      return result;\n   }",
    "reference": "Retrieve a boolean value.\n\n@param name column name\n@return boolean value",
    "generated": "Get a boolean value.\n\n@param name the name of the object\n@return boolean value"
  },
  {
    "code": "public InputStream getInstance(DirectoryEntry directory, String name) throws IOException\n   {\n      DocumentEntry entry = (DocumentEntry) directory.getEntry(name);\n      InputStream stream;\n      if (m_encrypted)\n      {\n         stream = new EncryptedDocumentInputStream(entry, m_encryptionCode);\n      }\n      else\n      {\n         stream = new DocumentInputStream(entry);\n      }\n\n      return stream;\n   }",
    "reference": "Method used to instantiate the appropriate input stream reader,\na standard one, or one which can deal with \"encrypted\" data.\n\n@param directory directory entry\n@param name file name\n@return new input stream\n@throws IOException",
    "generated": "Read a document from a directory entry.\n\n@param directory Directory entry\n@param name"
  },
  {
    "code": "private TaskField getTaskField(int field)\n   {\n      TaskField result = MPPTaskField14.getInstance(field);\n\n      if (result != null)\n      {\n         switch (result)\n         {\n            case START_TEXT:\n            {\n               result = TaskField.START;\n               break;\n            }\n\n            case FINISH_TEXT:\n            {\n               result = TaskField.FINISH;\n               break;\n            }\n\n            case DURATION_TEXT:\n            {\n               result = TaskField.DURATION;\n               break;\n            }\n\n            default:\n            {\n               break;\n            }\n         }\n      }\n\n      return result;\n   }",
    "reference": "Maps an integer field ID to a field type.\n\n@param field field ID\n@return field type",
    "generated": "Get a task field.\n\n@param field field field"
  },
  {
    "code": "public void updateUniqueCounters()\n   {\n      //\n      // Update task unique IDs\n      //\n      for (Task task : m_parent.getTasks())\n      {\n         int uniqueID = NumberHelper.getInt(task.getUniqueID());\n         if (uniqueID > m_taskUniqueID)\n         {\n            m_taskUniqueID = uniqueID;\n         }\n      }\n\n      //\n      // Update resource unique IDs\n      //\n      for (Resource resource : m_parent.getResources())\n      {\n         int uniqueID = NumberHelper.getInt(resource.getUniqueID());\n         if (uniqueID > m_resourceUniqueID)\n         {\n            m_resourceUniqueID = uniqueID;\n         }\n      }\n\n      //\n      // Update calendar unique IDs\n      //\n      for (ProjectCalendar calendar : m_parent.getCalendars())\n      {\n         int uniqueID = NumberHelper.getInt(calendar.getUniqueID());\n         if (uniqueID > m_calendarUniqueID)\n         {\n            m_calendarUniqueID = uniqueID;\n         }\n      }\n\n      //\n      // Update assignment unique IDs\n      //\n      for (ResourceAssignment assignment : m_parent.getResourceAssignments())\n      {\n         int uniqueID = NumberHelper.getInt(assignment.getUniqueID());\n         if (uniqueID > m_assignmentUniqueID)\n         {\n            m_assignmentUniqueID = uniqueID;\n         }\n      }\n   }",
    "reference": "This method is called to ensure that after a project file has been\nread, the cached unique ID values used to generate new unique IDs\nstart after the end of the existing set of unique IDs.",
    "generated": "Update the unique counters."
  },
  {
    "code": "public static Date min(Date d1, Date d2)\n   {\n      Date result;\n      if (d1 == null)\n      {\n         result = d2;\n      }\n      else\n         if (d2 == null)\n         {\n            result = d1;\n         }\n         else\n         {\n            result = (d1.compareTo(d2) < 0) ? d1 : d2;\n         }\n      return result;\n   }",
    "reference": "Returns the earlier of two dates, handling null values. A non-null Date\nis always considered to be earlier than a null Date.\n\n@param d1 Date instance\n@param d2 Date instance\n@return Date earliest date",
    "generated": "Compare a date between two dates.\n\n@return Date"
  },
  {
    "code": "public static Date max(Date d1, Date d2)\n   {\n      Date result;\n      if (d1 == null)\n      {\n         result = d2;\n      }\n      else\n         if (d2 == null)\n         {\n            result = d1;\n         }\n         else\n         {\n            result = (d1.compareTo(d2) > 0) ? d1 : d2;\n         }\n      return result;\n   }",
    "reference": "Returns the later of two dates, handling null values. A non-null Date\nis always considered to be later than a null Date.\n\n@param d1 Date instance\n@param d2 Date instance\n@return Date latest date",
    "generated": "Returns the maximum value of the given date.\n\n@param d1 date\n@param d2 date"
  },
  {
    "code": "public static Duration getVariance(Task task, Date date1, Date date2, TimeUnit format)\n   {\n      Duration variance = null;\n\n      if (date1 != null && date2 != null)\n      {\n         ProjectCalendar calendar = task.getEffectiveCalendar();\n         if (calendar != null)\n         {\n            variance = calendar.getWork(date1, date2, format);\n         }\n      }\n\n      if (variance == null)\n      {\n         variance = Duration.getInstance(0, format);\n      }\n\n      return (variance);\n   }",
    "reference": "This utility method calculates the difference in working\ntime between two dates, given the context of a task.\n\n@param task parent task\n@param date1 first date\n@param date2 second date\n@param format required format for the resulting duration\n@return difference in working time between the two dates",
    "generated": "Get the variance of a task.\n\n@param task Task\n@param date1 date2 time"
  },
  {
    "code": "public static Date getDateFromLong(long date)\n   {\n      TimeZone tz = TimeZone.getDefault();\n      return (new Date(date - tz.getRawOffset()));\n   }",
    "reference": "Creates a date from the equivalent long value. This conversion\ntakes account of the time zone.\n\n@param date date expressed as a long integer\n@return new Date instance",
    "generated": "Convert a long to a date.\n\n@param date the date\n@return the date"
  },
  {
    "code": "public static Date getTimestampFromLong(long timestamp)\n   {\n      TimeZone tz = TimeZone.getDefault();\n      Date result = new Date(timestamp - tz.getRawOffset());\n\n      if (tz.inDaylightTime(result) == true)\n      {\n         int savings;\n\n         if (HAS_DST_SAVINGS == true)\n         {\n            savings = tz.getDSTSavings();\n         }\n         else\n         {\n            savings = DEFAULT_DST_SAVINGS;\n         }\n\n         result = new Date(result.getTime() - savings);\n      }\n      return (result);\n   }",
    "reference": "Creates a timestamp from the equivalent long value. This conversion\ntakes account of the time zone and any daylight savings time.\n\n@param timestamp timestamp expressed as a long integer\n@return new Date instance",
    "generated": "Get the timestamp from a long.\n\n@param timestamp timestamp"
  },
  {
    "code": "public static Date getTime(int hour, int minutes)\n   {\n      Calendar cal = popCalendar();\n      cal.set(Calendar.HOUR_OF_DAY, hour);\n      cal.set(Calendar.MINUTE, minutes);\n      cal.set(Calendar.SECOND, 0);\n      Date result = cal.getTime();\n      pushCalendar(cal);\n      return result;\n   }",
    "reference": "Create a Date instance representing a specific time.\n\n@param hour hour 0-23\n@param minutes minutes 0-59\n@return new Date instance",
    "generated": "Get the current time.\n\n@param hour hour and minutes\n@return current time"
  },
  {
    "code": "public static void setTime(Calendar cal, Date time)\n   {\n      if (time != null)\n      {\n         Calendar startCalendar = popCalendar(time);\n         cal.set(Calendar.HOUR_OF_DAY, startCalendar.get(Calendar.HOUR_OF_DAY));\n         cal.set(Calendar.MINUTE, startCalendar.get(Calendar.MINUTE));\n         cal.set(Calendar.SECOND, startCalendar.get(Calendar.SECOND));\n         pushCalendar(startCalendar);\n      }\n   }",
    "reference": "Given a date represented by a Calendar instance, set the time\ncomponent of the date based on the hours and minutes of the\ntime supplied by the Date instance.\n\n@param cal Calendar instance representing the date\n@param time Date instance representing the time of day",
    "generated": "Set the time of a calendar.\n\n@param cal Calendar to be set"
  },
  {
    "code": "public static Date setTime(Date date, Date canonicalTime)\n   {\n      Date result;\n      if (canonicalTime == null)\n      {\n         result = date;\n      }\n      else\n      {\n         //\n         // The original naive implementation of this method generated\n         // the \"start of day\" date (midnight) for the required day\n         // then added the milliseconds from the canonical time\n         // to move the time forward to the required point. Unfortunately\n         // if the date we'e trying to do this for is the entry or\n         // exit from DST, the result is wrong, hence I've switched to\n         // the approach below.\n         //\n         Calendar cal = popCalendar(canonicalTime);\n         int dayOffset = cal.get(Calendar.DAY_OF_YEAR) - 1;\n         int hourOfDay = cal.get(Calendar.HOUR_OF_DAY);\n         int minute = cal.get(Calendar.MINUTE);\n         int second = cal.get(Calendar.SECOND);\n         int millisecond = cal.get(Calendar.MILLISECOND);\n\n         cal.setTime(date);\n\n         if (dayOffset != 0)\n         {\n            // The canonical time can be +1 day.\n            // It's to do with the way we've historically\n            // managed time ranges and midnight.\n            cal.add(Calendar.DAY_OF_YEAR, dayOffset);\n         }\n\n         cal.set(Calendar.MILLISECOND, millisecond);\n         cal.set(Calendar.SECOND, second);\n         cal.set(Calendar.MINUTE, minute);\n         cal.set(Calendar.HOUR_OF_DAY, hourOfDay);\n\n         result = cal.getTime();\n         pushCalendar(cal);\n      }\n      return result;\n   }",
    "reference": "Given a date represented by a Date instance, set the time\ncomponent of the date based on the hours and minutes of the\ntime supplied by the Date instance.\n\n@param date Date instance representing the date\n@param canonicalTime Date instance representing the time of day\n@return new Date instance with the required time set",
    "generated": "The original naive implementation of this method generated\n@param date Date"
  },
  {
    "code": "public static Date addDays(Date date, int days)\n   {\n      Calendar cal = popCalendar(date);\n      cal.add(Calendar.DAY_OF_YEAR, days);\n      Date result = cal.getTime();\n      pushCalendar(cal);\n      return result;   \n   }",
    "reference": "Add a number of days to the supplied date.\n\n@param date start date\n@param days number of days to add\n@return  new date",
    "generated": "Add days to a date.\n\n@param date date\n@param days days"
  },
  {
    "code": "public static Calendar popCalendar()\n   {\n      Calendar result;\n      Deque<Calendar> calendars = CALENDARS.get();\n      if (calendars.isEmpty())\n      {\n         result = Calendar.getInstance();\n      }\n      else\n      {\n         result = calendars.pop();\n      }\n      return result;\n   }",
    "reference": "Acquire a calendar instance.\n\n@return Calendar instance",
    "generated": "Pop a calendar.\n\n@param calendar calendar"
  },
  {
    "code": "public List<MapRow> read() throws IOException\n   {\n      List<MapRow> result = new ArrayList<MapRow>();\n      int fileCount = m_stream.readInt();\n      if (fileCount != 0)\n      {\n         for (int index = 0; index < fileCount; index++)\n         {\n            // We use a LinkedHashMap to preserve insertion order in iteration\n            // Useful when debugging the file format.\n            Map<String, Object> map = new LinkedHashMap<String, Object>();\n            readBlock(map);\n            result.add(new MapRow(map));\n         }\n      }\n      return result;\n   }",
    "reference": "Read a list of fixed sized blocks from the input stream.\n\n@return List of MapRow instances representing the fixed size blocks",
    "generated": "Read the data from the stream.\n\n@param m_stream the stream to read."
  },
  {
    "code": "protected int readByte(InputStream is) throws IOException\n   {\n      byte[] data = new byte[1];\n      if (is.read(data) != data.length)\n      {\n         throw new EOFException();\n      }\n\n      return (MPPUtility.getByte(data, 0));\n   }",
    "reference": "This method reads a single byte from the input stream.\n\n@param is the input stream\n@return byte value\n@throws IOException on file read error or EOF",
    "generated": "Read a byte from the input stream.\n\n@param is the input stream\n@return byte value"
  },
  {
    "code": "protected int readShort(InputStream is) throws IOException\n   {\n      byte[] data = new byte[2];\n      if (is.read(data) != data.length)\n      {\n         throw new EOFException();\n      }\n\n      return (MPPUtility.getShort(data, 0));\n   }",
    "reference": "This method reads a two byte integer from the input stream.\n\n@param is the input stream\n@return integer value\n@throws IOException on file read error or EOF",
    "generated": "Read the short value.\n\n@param is the input stream\n@return short value"
  },
  {
    "code": "protected int readInt(InputStream is) throws IOException\n   {\n      byte[] data = new byte[4];\n      if (is.read(data) != data.length)\n      {\n         throw new EOFException();\n      }\n\n      return (MPPUtility.getInt(data, 0));\n   }",
    "reference": "This method reads a four byte integer from the input stream.\n\n@param is the input stream\n@return byte value\n@throws IOException on file read error or EOF",
    "generated": "Read an integer from the input stream.\n\n@param is the input stream\n@return the integer value"
  },
  {
    "code": "protected byte[] readByteArray(InputStream is, int size) throws IOException\n   {\n      byte[] buffer = new byte[size];\n      if (is.read(buffer) != buffer.length)\n      {\n         throw new EOFException();\n      }\n      return (buffer);\n   }",
    "reference": "This method reads a byte array from the input stream.\n\n@param is the input stream\n@param size number of bytes to read\n@return byte array\n@throws IOException on file read error or EOF",
    "generated": "Read a byte array.\n\n@param is the InputStream\n@param size the size of the byte array\n@return the byte array"
  },
  {
    "code": "public int blast(InputStream input, OutputStream output) throws IOException\n   {\n      m_input = input;\n      m_output = output;\n\n      int lit; /* true if literals are coded */\n      int dict; /* log2(dictionary size) - 6 */\n      int symbol; /* decoded symbol, extra bits for distance */\n      int len; /* length for copy */\n      int dist; /* distance for copy */\n      int copy; /* copy counter */\n      //unsigned char *from, *to;   /* copy pointers */\n\n      /* read header */\n      lit = bits(8);\n      if (lit > 1)\n      {\n         return -1;\n      }\n      dict = bits(8);\n      if (dict < 4 || dict > 6)\n      {\n         return -2;\n      }\n\n      /* decode literals and length/distance pairs */\n      do\n      {\n         if (bits(1) != 0)\n         {\n            /* get length */\n            symbol = decode(LENCODE);\n            len = BASE[symbol] + bits(EXTRA[symbol]);\n            if (len == 519)\n            {\n               break; /* end code */\n            }\n\n            /* get distance */\n            symbol = len == 2 ? 2 : dict;\n            dist = decode(DISTCODE) << symbol;\n            dist += bits(symbol);\n            dist++;\n            if (m_first != 0 && dist > m_next)\n            {\n               return -3; /* distance too far back */\n            }\n\n            /* copy length bytes from distance bytes back */\n            do\n            {\n               //to = m_out + m_next;\n               int to = m_next;\n               int from = to - dist;\n               copy = MAXWIN;\n               if (m_next < dist)\n               {\n                  from += copy;\n                  copy = dist;\n               }\n               copy -= m_next;\n               if (copy > len)\n               {\n                  copy = len;\n               }\n               len -= copy;\n               m_next += copy;\n               do\n               {\n                  //*to++ = *from++;\n                  m_out[to++] = m_out[from++];\n               }\n               while (--copy != 0);\n               if (m_next == MAXWIN)\n               {\n                  //if (s->outfun(s->outhow, s->out, s->next)) return 1;\n                  m_output.write(m_out, 0, m_next);\n                  m_next = 0;\n                  m_first = 0;\n               }\n            }\n            while (len != 0);\n         }\n         else\n         {\n            /* get literal and write it */\n            symbol = lit != 0 ? decode(LITCODE) : bits(8);\n            m_out[m_next++] = (byte) symbol;\n            if (m_next == MAXWIN)\n            {\n               //if (s->outfun(s->outhow, s->out, s->next)) return 1;\n               m_output.write(m_out, 0, m_next);\n               m_next = 0;\n               m_first = 0;\n            }\n         }\n      }\n      while (true);\n\n      if (m_next != 0)\n      {\n         m_output.write(m_out, 0, m_next);\n      }\n\n      return 0;\n   }",
    "reference": "Decode PKWare Compression Library stream.\n\nFormat notes:\n\n- First byte is 0 if literals are uncoded or 1 if they are coded.  Second\nbyte is 4, 5, or 6 for the number of extra bits in the distance code.\nThis is the base-2 logarithm of the dictionary size minus six.\n\n- Compressed data is a combination of literals and length/distance pairs\nterminated by an end code.  Literals are either Huffman coded or\nuncoded bytes.  A length/distance pair is a coded length followed by a\ncoded distance to represent a string that occurs earlier in the\nuncompressed data that occurs again at the current location.\n\n- A bit preceding a literal or length/distance pair indicates which comes\nnext, 0 for literals, 1 for length/distance.\n\n- If literals are uncoded, then the next eight bits are the literal, in the\nnormal bit order in the stream, i.e. no bit-reversal is needed. Similarly,\nno bit reversal is needed for either the length extra bits or the distance\nextra bits.\n\n- Literal bytes are simply written to the output.  A length/distance pair is\nan instruction to copy previously uncompressed bytes to the output.  The\ncopy is from distance bytes back in the output stream, copying for length\nbytes.\n\n- Distances pointing before the beginning of the output data are not\npermitted.\n\n- Overlapped copies, where the length is greater than the distance, are\nallowed and common.  For example, a distance of one and a length of 518\nsimply copies the last byte 518 times.  A distance of four and a length of\ntwelve copies the last four bytes three times.  A simple forward copy\nignoring whether the length is greater than the distance or not implements\nthis correctly.\n\n@param input InputStream instance\n@param output OutputStream instance\n@return status code",
    "generated": "Generate a blast sequence.\n\n@param input InputStream output stream\n@param output stream\n@param output stream\n@param output stream\n@return blast sequence"
  },
  {
    "code": "private int decode(Huffman h) throws IOException\n   {\n      int len; /* current number of bits in code */\n      int code; /* len bits being decoded */\n      int first; /* first code of length len */\n      int count; /* number of codes of length len */\n      int index; /* index of first code of length len in symbol table */\n      int bitbuf; /* bits from stream */\n      int left; /* bits left in next or left to process */\n      //short *next;        /* next number of codes */\n\n      bitbuf = m_bitbuf;\n      left = m_bitcnt;\n      code = first = index = 0;\n      len = 1;\n      int nextIndex = 1; // next = h->count + 1;\n      while (true)\n      {\n         while (left-- != 0)\n         {\n            code |= (bitbuf & 1) ^ 1; /* invert code */\n            bitbuf >>= 1;\n            //count = *next++;\n            count = h.m_count[nextIndex++];\n            if (code < first + count)\n            { /* if length len, return symbol */\n               m_bitbuf = bitbuf;\n               m_bitcnt = (m_bitcnt - len) & 7;\n               return h.m_symbol[index + (code - first)];\n            }\n            index += count; /* else update for next length */\n            first += count;\n            first <<= 1;\n            code <<= 1;\n            len++;\n         }\n         left = (MAXBITS + 1) - len;\n         if (left == 0)\n         {\n            break;\n         }\n         if (m_left == 0)\n         {\n            m_in = m_input.read();\n            m_left = m_in == -1 ? 0 : 1;\n            if (m_left == 0)\n            {\n               throw new IOException(\"out of input\"); /* out of input */\n            }\n         }\n         bitbuf = m_in;\n         m_left--;\n         if (left > 8)\n         {\n            left = 8;\n         }\n      }\n      return -9; /* ran out of codes */\n   }",
    "reference": "Decode a code from the stream s using huffman table h.  Return the symbol or\na negative value if there is an error.  If all of the lengths are zero, i.e.\nan empty code, or if the code is incomplete and an invalid code is received,\nthen -9 is returned after reading MAXBITS bits.\n\nFormat notes:\n\n- The codes as stored in the compressed data are bit-reversed relative to\na simple integer ordering of codes of the same lengths.  Hence below the\nbits are pulled from the compressed data one at a time and used to\nbuild the code value reversed from what is in the stream in order to\npermit simple integer comparisons for decoding.\n\n- The first code for the shortest length is all ones.  Subsequent codes of\nthe same length are simply integer decrements of the previous code.  When\nmoving up a length, a one bit is appended to the code.  For a complete\ncode, the last code of the longest length will be all zeros.  To support\nthis ordering, the bits pulled during decoding are inverted to apply the\nmore \"natural\" ordering starting with all zeros and incrementing.\n\n@param h Huffman table\n@return status code",
    "generated": "Decode a Huffman instance.\n\n@param h Huffman instance\n@param h Huffman instance\n@return -9"
  },
  {
    "code": "private void validateSameDay(ProjectCalendar calendar, LinkedList<TimephasedWork> list)\n   {\n      for (TimephasedWork assignment : list)\n      {\n         Date assignmentStart = assignment.getStart();\n         Date calendarStartTime = calendar.getStartTime(assignmentStart);\n         Date assignmentStartTime = DateHelper.getCanonicalTime(assignmentStart);\n         Date assignmentFinish = assignment.getFinish();\n         Date calendarFinishTime = calendar.getFinishTime(assignmentFinish);\n         Date assignmentFinishTime = DateHelper.getCanonicalTime(assignmentFinish);\n         double totalWork = assignment.getTotalAmount().getDuration();\n\n         if (assignmentStartTime != null && calendarStartTime != null)\n         {\n            if ((totalWork == 0 && assignmentStartTime.getTime() != calendarStartTime.getTime()) || (assignmentStartTime.getTime() < calendarStartTime.getTime()))\n            {\n               assignmentStart = DateHelper.setTime(assignmentStart, calendarStartTime);\n               assignment.setStart(assignmentStart);\n            }\n         }\n\n         if (assignmentFinishTime != null && calendarFinishTime != null)\n         {\n            if ((totalWork == 0 && assignmentFinishTime.getTime() != calendarFinishTime.getTime()) || (assignmentFinishTime.getTime() > calendarFinishTime.getTime()))\n            {\n               assignmentFinish = DateHelper.setTime(assignmentFinish, calendarFinishTime);\n               assignment.setFinish(assignmentFinish);\n            }\n         }\n      }\n   }",
    "reference": "Ensures that the start and end dates for ranges fit within the\nworking times for a given day.\n\n@param calendar current calendar\n@param list assignment data",
    "generated": "Validate same day.\n\n@param calendar project calendar\n@param list"
  },
  {
    "code": "public Table createTable(ProjectFile file, byte[] data, VarMeta varMeta, Var2Data varData)\n   {\n      Table table = new Table();\n\n      table.setID(MPPUtility.getInt(data, 0));\n      table.setResourceFlag(MPPUtility.getShort(data, 108) == 1);\n      table.setName(MPPUtility.removeAmpersands(MPPUtility.getUnicodeString(data, 4)));\n\n      byte[] columnData = null;\n      Integer tableID = Integer.valueOf(table.getID());\n      if (m_tableColumnDataBaseline != null)\n      {\n         columnData = varData.getByteArray(varMeta.getOffset(tableID, m_tableColumnDataBaseline));\n      }\n\n      if (columnData == null)\n      {\n         columnData = varData.getByteArray(varMeta.getOffset(tableID, m_tableColumnDataEnterprise));\n         if (columnData == null)\n         {\n            columnData = varData.getByteArray(varMeta.getOffset(tableID, m_tableColumnDataStandard));\n         }\n      }\n\n      processColumnData(file, table, columnData);\n\n      //System.out.println(table);\n\n      return (table);\n   }",
    "reference": "Creates a new Table instance from data extracted from an MPP file.\n\n@param file parent project file\n@param data fixed data\n@param varMeta var meta\n@param varData var data\n@return Table instance",
    "generated": "Create a table.\n\n@param file the project file\n@param varMeta Var2Data varData"
  },
  {
    "code": "private final boolean parseBoolean(String value)\n   {\n      return value != null && (value.equalsIgnoreCase(\"true\") || value.equalsIgnoreCase(\"y\") || value.equalsIgnoreCase(\"yes\"));\n   }",
    "reference": "Parse a string representation of a Boolean value.\nXER files sometimes have \"N\" and \"Y\" to indicate boolean\n\n@param value string representation\n@return Boolean value",
    "generated": "Parse a boolean value."
  },
  {
    "code": "public void processActivityCodes(List<Row> types, List<Row> typeValues, List<Row> assignments)\n   {\n      ActivityCodeContainer container = m_project.getActivityCodes();\n      Map<Integer, ActivityCode> map = new HashMap<Integer, ActivityCode>();\n\n      for (Row row : types)\n      {\n         ActivityCode code = new ActivityCode(row.getInteger(\"actv_code_type_id\"), row.getString(\"actv_code_type\"));\n         container.add(code);\n         map.put(code.getUniqueID(), code);\n      }\n\n      for (Row row : typeValues)\n      {\n         ActivityCode code = map.get(row.getInteger(\"actv_code_type_id\"));\n         if (code != null)\n         {\n            ActivityCodeValue value = code.addValue(row.getInteger(\"actv_code_id\"), row.getString(\"short_name\"), row.getString(\"actv_code_name\"));\n            m_activityCodeMap.put(value.getUniqueID(), value);\n         }\n      }\n\n      for (Row row : assignments)\n      {\n         Integer taskID = row.getInteger(\"task_id\");\n         List<Integer> list = m_activityCodeAssignments.get(taskID);\n         if (list == null)\n         {\n            list = new ArrayList<Integer>();\n            m_activityCodeAssignments.put(taskID, list);\n         }\n         list.add(row.getInteger(\"actv_code_id\"));\n      }\n   }",
    "reference": "Read activity code types and values.\n\n@param types activity code type data\n@param typeValues activity code value data\n@param assignments activity code task assignments",
    "generated": "Process a list of ActivityCodes."
  },
  {
    "code": "public void processCalendar(Row row)\n   {\n      ProjectCalendar calendar = m_project.addCalendar();\n\n      Integer id = row.getInteger(\"clndr_id\");\n      m_calMap.put(id, calendar);\n      calendar.setName(row.getString(\"clndr_name\"));\n\n      try\n      {\n         calendar.setMinutesPerDay(Integer.valueOf((int) NumberHelper.getDouble(row.getDouble(\"day_hr_cnt\")) * 60));\n         calendar.setMinutesPerWeek(Integer.valueOf((int) (NumberHelper.getDouble(row.getDouble(\"week_hr_cnt\")) * 60)));\n         calendar.setMinutesPerMonth(Integer.valueOf((int) (NumberHelper.getDouble(row.getDouble(\"month_hr_cnt\")) * 60)));\n         calendar.setMinutesPerYear(Integer.valueOf((int) (NumberHelper.getDouble(row.getDouble(\"year_hr_cnt\")) * 60)));\n      }\n      catch (ClassCastException ex)\n      {\n         // We have seen examples of malformed calendar data where fields have been missing\n         // from the record. We'll typically get a class cast exception here as we're trying\n         // to process something which isn't a double.\n         // We'll just return at this point as it's not clear that we can salvage anything\n         // sensible from this record.\n         return;\n      }\n\n      // Process data\n      String calendarData = row.getString(\"clndr_data\");\n      if (calendarData != null && !calendarData.isEmpty())\n      {\n         Record root = Record.getRecord(calendarData);\n         if (root != null)\n         {\n            processCalendarDays(calendar, root);\n            processCalendarExceptions(calendar, root);\n         }\n      }\n      else\n      {\n         // if there is not DaysOfWeek data, Primavera seems to default to Mon-Fri, 8:00-16:00\n         DateRange defaultHourRange = new DateRange(DateHelper.getTime(8, 0), DateHelper.getTime(16, 0));\n         for (Day day : Day.values())\n         {\n            if (day != Day.SATURDAY && day != Day.SUNDAY)\n            {\n               calendar.setWorkingDay(day, true);\n               ProjectCalendarHours hours = calendar.addCalendarHours(day);\n               hours.addRange(defaultHourRange);\n            }\n            else\n            {\n               calendar.setWorkingDay(day, false);\n            }\n         }\n      }\n\n      m_eventManager.fireCalendarReadEvent(calendar);\n   }",
    "reference": "Process data for an individual calendar.\n\n@param row calendar data",
    "generated": "Process the calendar data.\n\n@param row the row to process\n@return the calendar data"
  },
  {
    "code": "private void processCalendarDays(ProjectCalendar calendar, Record root)\n   {\n      // Retrieve working hours ...\n      Record daysOfWeek = root.getChild(\"DaysOfWeek\");\n      if (daysOfWeek != null)\n      {\n         for (Record dayRecord : daysOfWeek.getChildren())\n         {\n            processCalendarHours(calendar, dayRecord);\n         }\n      }\n   }",
    "reference": "Process calendar days of the week.\n\n@param calendar project calendar\n@param root calendar data",
    "generated": "Retrieve working hours ..."
  },
  {
    "code": "private void processCalendarHours(ProjectCalendar calendar, Record dayRecord)\n   {\n      // ... for each day of the week\n      Day day = Day.getInstance(Integer.parseInt(dayRecord.getField()));\n      // Get hours\n      List<Record> recHours = dayRecord.getChildren();\n      if (recHours.size() == 0)\n      {\n         // No data -> not working\n         calendar.setWorkingDay(day, false);\n      }\n      else\n      {\n         calendar.setWorkingDay(day, true);\n         // Read hours\n         ProjectCalendarHours hours = calendar.addCalendarHours(day);\n         for (Record recWorkingHours : recHours)\n         {\n            addHours(hours, recWorkingHours);\n         }\n      }\n   }",
    "reference": "Process hours in a working day.\n\n@param calendar project calendar\n@param dayRecord working day data",
    "generated": "Process the calendar hours."
  },
  {
    "code": "private void addHours(ProjectCalendarDateRanges ranges, Record hoursRecord)\n   {\n      if (hoursRecord.getValue() != null)\n      {\n         String[] wh = hoursRecord.getValue().split(\"\\\\|\");\n         try\n         {\n            String startText;\n            String endText;\n\n            if (wh[0].equals(\"s\"))\n            {\n               startText = wh[1];\n               endText = wh[3];\n            }\n            else\n            {\n               startText = wh[3];\n               endText = wh[1];\n            }\n\n            // for end time treat midnight as midnight next day\n            if (endText.equals(\"00:00\"))\n            {\n               endText = \"24:00\";\n            }\n            Date start = m_calendarTimeFormat.parse(startText);\n            Date end = m_calendarTimeFormat.parse(endText);\n\n            ranges.addRange(new DateRange(start, end));\n         }\n         catch (ParseException e)\n         {\n            // silently ignore date parse exceptions\n         }\n      }\n   }",
    "reference": "Parses a record containing hours and add them to a container.\n\n@param ranges hours container\n@param hoursRecord hours record",
    "generated": "Add the hours record to the ranges."
  },
  {
    "code": "private ProjectCalendar getResourceCalendar(Integer calendarID)\n   {\n      ProjectCalendar result = null;\n      if (calendarID != null)\n      {\n         ProjectCalendar calendar = m_calMap.get(calendarID);\n         if (calendar != null)\n         {\n            //\n            // If the resource is linked to a base calendar, derive\n            // a default calendar from the base calendar.\n            //\n            if (!calendar.isDerived())\n            {\n               ProjectCalendar resourceCalendar = m_project.addCalendar();\n               resourceCalendar.setParent(calendar);\n               resourceCalendar.setWorkingDay(Day.MONDAY, DayType.DEFAULT);\n               resourceCalendar.setWorkingDay(Day.TUESDAY, DayType.DEFAULT);\n               resourceCalendar.setWorkingDay(Day.WEDNESDAY, DayType.DEFAULT);\n               resourceCalendar.setWorkingDay(Day.THURSDAY, DayType.DEFAULT);\n               resourceCalendar.setWorkingDay(Day.FRIDAY, DayType.DEFAULT);\n               resourceCalendar.setWorkingDay(Day.SATURDAY, DayType.DEFAULT);\n               resourceCalendar.setWorkingDay(Day.SUNDAY, DayType.DEFAULT);\n               result = resourceCalendar;\n            }\n            else\n            {\n               //\n               // Primavera seems to allow a calendar to be shared between resources\n               // whereas in the MS Project model there is a one-to-one\n               // relationship. If we find a shared calendar, take a copy of it\n               //\n               if (calendar.getResource() == null)\n               {\n                  result = calendar;\n               }\n               else\n               {\n                  ProjectCalendar copy = m_project.addCalendar();\n                  copy.copy(calendar);\n                  result = copy;\n               }\n            }\n         }\n      }\n\n      return result;\n   }",
    "reference": "Retrieve the correct calendar for a resource.\n\n@param calendarID calendar ID\n@return calendar for resource",
    "generated": "Get the resource calendar.\n\n@param calendarID calendar ID"
  },
  {
    "code": "private FieldType getActivityIDField(Map<FieldType, String> map)\n   {\n      FieldType result = null;\n      for (Map.Entry<FieldType, String> entry : map.entrySet())\n      {\n         if (entry.getValue().equals(\"task_code\"))\n         {\n            result = entry.getKey();\n            break;\n         }\n      }\n      return result;\n   }",
    "reference": "Determine which field the Activity ID has been mapped to.\n\n@param map field map\n@return field",
    "generated": "Get the activity ID field.\n\n@param map map"
  },
  {
    "code": "private void addUserDefinedField(FieldTypeClass fieldType, UserFieldDataType dataType, String name)\n   {\n      try\n      {\n         switch (fieldType)\n         {\n            case TASK:\n               TaskField taskField;\n\n               do\n               {\n                  taskField = m_taskUdfCounters.nextField(TaskField.class, dataType);\n               }\n               while (m_taskFields.containsKey(taskField) || m_wbsFields.containsKey(taskField));\n\n               m_project.getCustomFields().getCustomField(taskField).setAlias(name);\n\n               break;\n            case RESOURCE:\n               ResourceField resourceField;\n\n               do\n               {\n                  resourceField = m_resourceUdfCounters.nextField(ResourceField.class, dataType);\n               }\n               while (m_resourceFields.containsKey(resourceField));\n\n               m_project.getCustomFields().getCustomField(resourceField).setAlias(name);\n\n               break;\n            case ASSIGNMENT:\n               AssignmentField assignmentField;\n\n               do\n               {\n                  assignmentField = m_assignmentUdfCounters.nextField(AssignmentField.class, dataType);\n               }\n               while (m_assignmentFields.containsKey(assignmentField));\n\n               m_project.getCustomFields().getCustomField(assignmentField).setAlias(name);\n\n               break;\n            default:\n               break;\n         }\n      }\n\n      catch (Exception ex)\n      {\n         //\n         // SF#227: If we get an exception thrown here... it's likely that\n         // we've run out of user defined fields, for example\n         // there are only 30 TEXT fields. We'll ignore this: the user\n         // defined field won't be mapped to an alias, so we'll\n         // ignore it when we read in the values.\n         //\n      }\n   }",
    "reference": "Configure a new user defined field.\n\n@param fieldType field type\n@param dataType field data type\n@param name field name",
    "generated": "Add a user defined field."
  },
  {
    "code": "private void addUDFValue(FieldTypeClass fieldType, FieldContainer container, Row row)\n   {\n      Integer fieldId = row.getInteger(\"udf_type_id\");\n      String fieldName = m_udfFields.get(fieldId);\n\n      Object value = null;\n      FieldType field = m_project.getCustomFields().getFieldByAlias(fieldType, fieldName);\n      if (field != null)\n      {\n         DataType fieldDataType = field.getDataType();\n\n         switch (fieldDataType)\n         {\n            case DATE:\n            {\n               value = row.getDate(\"udf_date\");\n               break;\n            }\n\n            case CURRENCY:\n            case NUMERIC:\n            {\n               value = row.getDouble(\"udf_number\");\n               break;\n            }\n\n            case GUID:\n            case INTEGER:\n            {\n               value = row.getInteger(\"udf_code_id\");\n               break;\n            }\n\n            case BOOLEAN:\n            {\n               String text = row.getString(\"udf_text\");\n               if (text != null)\n               {\n                  // before a normal boolean parse, we try to lookup the text as a P6 static type indicator UDF\n                  value = STATICTYPE_UDF_MAP.get(text);\n                  if (value == null)\n                  {\n                     value = Boolean.valueOf(row.getBoolean(\"udf_text\"));\n                  }\n               }\n               else\n               {\n                  value = Boolean.valueOf(row.getBoolean(\"udf_number\"));\n               }\n               break;\n            }\n\n            default:\n            {\n               value = row.getString(\"udf_text\");\n               break;\n            }\n         }\n\n         container.set(field, value);\n      }\n   }",
    "reference": "Adds a user defined field value to a task.\n\n@param fieldType field type\n@param container FieldContainer instance\n@param row UDF data",
    "generated": "Add a UDF value to the field container.\n\n@param fieldType the field type\n@param row the row\n@param container the field container\n@param row the row\n@param row the row\n@return null"
  },
  {
    "code": "private void populateUserDefinedFieldValues(String tableName, FieldTypeClass type, FieldContainer container, Integer uniqueID)\n   {\n      Map<Integer, List<Row>> tableData = m_udfValues.get(tableName);\n      if (tableData != null)\n      {\n         List<Row> udf = tableData.get(uniqueID);\n         if (udf != null)\n         {\n            for (Row r : udf)\n            {\n               addUDFValue(type, container, r);\n            }\n         }\n      }\n   }",
    "reference": "Populate the UDF values for this entity.\n\n@param tableName parent table name\n@param type entity type\n@param container entity\n@param uniqueID entity Unique ID",
    "generated": "Populate the user defined field values.\n\n@param tableName the table name\n@param type the FieldTypeClass type\n@param container the field container\n@param uniqueID the unique ID"
  },
  {
    "code": "public void processDefaultCurrency(Row row)\n   {\n      ProjectProperties properties = m_project.getProjectProperties();\n      properties.setCurrencySymbol(row.getString(\"curr_symbol\"));\n      properties.setSymbolPosition(CURRENCY_SYMBOL_POSITION_MAP.get(row.getString(\"pos_curr_fmt_type\")));\n      properties.setCurrencyDigits(row.getInteger(\"decimal_digit_cnt\"));\n      properties.setThousandsSeparator(row.getString(\"digit_group_symbol\").charAt(0));\n      properties.setDecimalSeparator(row.getString(\"decimal_symbol\").charAt(0));\n   }",
    "reference": "Code common to both XER and database readers to extract\ncurrency format data.\n\n@param row row containing currency data",
    "generated": "Process the default currency.\n\n@param row the row to be processed"
  },
  {
    "code": "private void processFields(Map<FieldType, String> map, Row row, FieldContainer container)\n   {\n      for (Map.Entry<FieldType, String> entry : map.entrySet())\n      {\n         FieldType field = entry.getKey();\n         String name = entry.getValue();\n\n         Object value;\n         switch (field.getDataType())\n         {\n            case INTEGER:\n            {\n               value = row.getInteger(name);\n               break;\n            }\n\n            case BOOLEAN:\n            {\n               value = Boolean.valueOf(row.getBoolean(name));\n               break;\n            }\n\n            case DATE:\n            {\n               value = row.getDate(name);\n               break;\n            }\n\n            case CURRENCY:\n            case NUMERIC:\n            case PERCENTAGE:\n            {\n               value = row.getDouble(name);\n               break;\n            }\n\n            case DELAY:\n            case WORK:\n            case DURATION:\n            {\n               value = row.getDuration(name);\n               break;\n            }\n\n            case RESOURCE_TYPE:\n            {\n               value = RESOURCE_TYPE_MAP.get(row.getString(name));\n               break;\n            }\n\n            case TASK_TYPE:\n            {\n               value = TASK_TYPE_MAP.get(row.getString(name));\n               break;\n            }\n\n            case CONSTRAINT:\n            {\n               value = CONSTRAINT_TYPE_MAP.get(row.getString(name));\n               break;\n            }\n\n            case PRIORITY:\n            {\n               value = PRIORITY_MAP.get(row.getString(name));\n               break;\n            }\n\n            case GUID:\n            {\n               value = row.getUUID(name);\n               break;\n            }\n\n            default:\n            {\n               value = row.getString(name);\n               break;\n            }\n         }\n\n         container.set(field, value);\n      }\n   }",
    "reference": "Generic method to extract Primavera fields and assign to MPXJ fields.\n\n@param map map of MPXJ field types and Primavera field names\n@param row Primavera data container\n@param container MPXJ data contain",
    "generated": "Process the fields from a row.\n\n@param map map to a FieldContainer\n@param row the row\n@param container the field container\n@param container the field container\n@return"
  },
  {
    "code": "private void applyAliases(Map<FieldType, String> aliases)\n   {\n      CustomFieldContainer fields = m_project.getCustomFields();\n      for (Map.Entry<FieldType, String> entry : aliases.entrySet())\n      {\n         fields.getCustomField(entry.getKey()).setAlias(entry.getValue());\n      }\n   }",
    "reference": "Apply aliases to task and resource fields.\n\n@param aliases map of aliases",
    "generated": "Apply the aliases to the given map.\n\n@param aliases the aliases to apply"
  },
  {
    "code": "private Number calculatePercentComplete(Row row)\n   {\n      Number result;\n      switch (PercentCompleteType.getInstance(row.getString(\"complete_pct_type\")))\n      {\n         case UNITS:\n         {\n            result = calculateUnitsPercentComplete(row);\n            break;\n         }\n\n         case DURATION:\n         {\n            result = calculateDurationPercentComplete(row);\n            break;\n         }\n\n         default:\n         {\n            result = calculatePhysicalPercentComplete(row);\n            break;\n         }\n      }\n\n      return result;\n   }",
    "reference": "Determine which type of percent complete is used on on this task,\nand calculate the required value.\n\n@param row task data\n@return percent complete value",
    "generated": "Calculate the percent complete value.\n\n@param row the row\n@return percent complete value"
  },
  {
    "code": "private Number calculateUnitsPercentComplete(Row row)\n   {\n      double result = 0;\n\n      double actualWorkQuantity = NumberHelper.getDouble(row.getDouble(\"act_work_qty\"));\n      double actualEquipmentQuantity = NumberHelper.getDouble(row.getDouble(\"act_equip_qty\"));\n      double numerator = actualWorkQuantity + actualEquipmentQuantity;\n\n      if (numerator != 0)\n      {\n         double remainingWorkQuantity = NumberHelper.getDouble(row.getDouble(\"remain_work_qty\"));\n         double remainingEquipmentQuantity = NumberHelper.getDouble(row.getDouble(\"remain_equip_qty\"));\n         double denominator = remainingWorkQuantity + actualWorkQuantity + remainingEquipmentQuantity + actualEquipmentQuantity;\n         result = denominator == 0 ? 0 : ((numerator * 100) / denominator);\n      }\n\n      return NumberHelper.getDouble(result);\n   }",
    "reference": "Calculate the units percent complete.\n\n@param row task data\n@return percent complete",
    "generated": "Calculate units percent complete."
  },
  {
    "code": "private Number calculateDurationPercentComplete(Row row)\n   {\n      double result = 0;\n      double targetDuration = row.getDuration(\"target_drtn_hr_cnt\").getDuration();\n      double remainingDuration = row.getDuration(\"remain_drtn_hr_cnt\").getDuration();\n\n      if (targetDuration == 0)\n      {\n         if (remainingDuration == 0)\n         {\n            if (\"TK_Complete\".equals(row.getString(\"status_code\")))\n            {\n               result = 100;\n            }\n         }\n      }\n      else\n      {\n         if (remainingDuration < targetDuration)\n         {\n            result = ((targetDuration - remainingDuration) * 100) / targetDuration;\n         }\n      }\n\n      return NumberHelper.getDouble(result);\n   }",
    "reference": "Calculate the duration percent complete.\n\n@param row task data\n@return percent complete",
    "generated": "Calculate the duration percent complete value.\n\n@param row the row to calculate"
  },
  {
    "code": "public static Map<FieldType, String> getDefaultResourceFieldMap()\n   {\n      Map<FieldType, String> map = new LinkedHashMap<FieldType, String>();\n\n      map.put(ResourceField.UNIQUE_ID, \"rsrc_id\");\n      map.put(ResourceField.GUID, \"guid\");\n      map.put(ResourceField.NAME, \"rsrc_name\");\n      map.put(ResourceField.CODE, \"employee_code\");\n      map.put(ResourceField.EMAIL_ADDRESS, \"email_addr\");\n      map.put(ResourceField.NOTES, \"rsrc_notes\");\n      map.put(ResourceField.CREATED, \"create_date\");\n      map.put(ResourceField.TYPE, \"rsrc_type\");\n      map.put(ResourceField.INITIALS, \"rsrc_short_name\");\n      map.put(ResourceField.PARENT_ID, \"parent_rsrc_id\");\n\n      return map;\n   }",
    "reference": "Retrieve the default mapping between MPXJ resource fields and Primavera resource field names.\n\n@return mapping",
    "generated": "Generate a default resource field map."
  },
  {
    "code": "public static Map<FieldType, String> getDefaultWbsFieldMap()\n   {\n      Map<FieldType, String> map = new LinkedHashMap<FieldType, String>();\n\n      map.put(TaskField.UNIQUE_ID, \"wbs_id\");\n      map.put(TaskField.GUID, \"guid\");\n      map.put(TaskField.NAME, \"wbs_name\");\n      map.put(TaskField.BASELINE_COST, \"orig_cost\");\n      map.put(TaskField.REMAINING_COST, \"indep_remain_total_cost\");\n      map.put(TaskField.REMAINING_WORK, \"indep_remain_work_qty\");\n      map.put(TaskField.DEADLINE, \"anticip_end_date\");\n      map.put(TaskField.DATE1, \"suspend_date\");\n      map.put(TaskField.DATE2, \"resume_date\");\n      map.put(TaskField.TEXT1, \"task_code\");\n      map.put(TaskField.WBS, \"wbs_short_name\");\n\n      return map;\n   }",
    "reference": "Retrieve the default mapping between MPXJ task fields and Primavera wbs field names.\n\n@return mapping",
    "generated": "Generate a default WBS field map."
  },
  {
    "code": "public static Map<FieldType, String> getDefaultTaskFieldMap()\n   {\n      Map<FieldType, String> map = new LinkedHashMap<FieldType, String>();\n\n      map.put(TaskField.UNIQUE_ID, \"task_id\");\n      map.put(TaskField.GUID, \"guid\");\n      map.put(TaskField.NAME, \"task_name\");\n      map.put(TaskField.ACTUAL_DURATION, \"act_drtn_hr_cnt\");\n      map.put(TaskField.REMAINING_DURATION, \"remain_drtn_hr_cnt\");\n      map.put(TaskField.ACTUAL_WORK, \"act_work_qty\");\n      map.put(TaskField.REMAINING_WORK, \"remain_work_qty\");\n      map.put(TaskField.BASELINE_WORK, \"target_work_qty\");\n      map.put(TaskField.BASELINE_DURATION, \"target_drtn_hr_cnt\");\n      map.put(TaskField.DURATION, \"target_drtn_hr_cnt\");\n      map.put(TaskField.CONSTRAINT_DATE, \"cstr_date\");\n      map.put(TaskField.ACTUAL_START, \"act_start_date\");\n      map.put(TaskField.ACTUAL_FINISH, \"act_end_date\");\n      map.put(TaskField.LATE_START, \"late_start_date\");\n      map.put(TaskField.LATE_FINISH, \"late_end_date\");\n      map.put(TaskField.EARLY_START, \"early_start_date\");\n      map.put(TaskField.EARLY_FINISH, \"early_end_date\");\n      map.put(TaskField.REMAINING_EARLY_START, \"restart_date\");\n      map.put(TaskField.REMAINING_EARLY_FINISH, \"reend_date\");\n      map.put(TaskField.BASELINE_START, \"target_start_date\");\n      map.put(TaskField.BASELINE_FINISH, \"target_end_date\");\n      map.put(TaskField.CONSTRAINT_TYPE, \"cstr_type\");\n      map.put(TaskField.PRIORITY, \"priority_type\");\n      map.put(TaskField.CREATED, \"create_date\");\n      map.put(TaskField.TYPE, \"duration_type\");\n      map.put(TaskField.FREE_SLACK, \"free_float_hr_cnt\");\n      map.put(TaskField.TOTAL_SLACK, \"total_float_hr_cnt\");\n      map.put(TaskField.TEXT1, \"task_code\");\n      map.put(TaskField.TEXT2, \"task_type\");\n      map.put(TaskField.TEXT3, \"status_code\");\n      map.put(TaskField.NUMBER1, \"rsrc_id\");\n\n      return map;\n   }",
    "reference": "Retrieve the default mapping between MPXJ task fields and Primavera task field names.\n\n@return mapping",
    "generated": "Generate a default task field map."
  },
  {
    "code": "public static Map<FieldType, String> getDefaultAssignmentFieldMap()\n   {\n      Map<FieldType, String> map = new LinkedHashMap<FieldType, String>();\n\n      map.put(AssignmentField.UNIQUE_ID, \"taskrsrc_id\");\n      map.put(AssignmentField.GUID, \"guid\");\n      map.put(AssignmentField.REMAINING_WORK, \"remain_qty\");\n      map.put(AssignmentField.BASELINE_WORK, \"target_qty\");\n      map.put(AssignmentField.ACTUAL_OVERTIME_WORK, \"act_ot_qty\");\n      map.put(AssignmentField.BASELINE_COST, \"target_cost\");\n      map.put(AssignmentField.ACTUAL_OVERTIME_COST, \"act_ot_cost\");\n      map.put(AssignmentField.REMAINING_COST, \"remain_cost\");\n      map.put(AssignmentField.ACTUAL_START, \"act_start_date\");\n      map.put(AssignmentField.ACTUAL_FINISH, \"act_end_date\");\n      map.put(AssignmentField.BASELINE_START, \"target_start_date\");\n      map.put(AssignmentField.BASELINE_FINISH, \"target_end_date\");\n      map.put(AssignmentField.ASSIGNMENT_DELAY, \"target_lag_drtn_hr_cnt\");\n\n      return map;\n   }",
    "reference": "Retrieve the default mapping between MPXJ assignment fields and Primavera assignment field names.\n\n@return mapping",
    "generated": "Generate a default assignment field map.\n\n@param taskrsrc_id Taskrsrc id"
  },
  {
    "code": "public static Map<FieldType, String> getDefaultAliases()\n   {\n      Map<FieldType, String> map = new HashMap<FieldType, String>();\n\n      map.put(TaskField.DATE1, \"Suspend Date\");\n      map.put(TaskField.DATE2, \"Resume Date\");\n      map.put(TaskField.TEXT1, \"Code\");\n      map.put(TaskField.TEXT2, \"Activity Type\");\n      map.put(TaskField.TEXT3, \"Status\");\n      map.put(TaskField.NUMBER1, \"Primary Resource Unique ID\");\n\n      return map;\n   }",
    "reference": "Retrieve the default aliases to be applied to MPXJ task and resource fields.\n\n@return map of aliases",
    "generated": "Generate a map of default aliases."
  },
  {
    "code": "private static void filter(String filename, String filtername) throws Exception\n   {\n      ProjectFile project = new UniversalProjectReader().read(filename);\n      Filter filter = project.getFilters().getFilterByName(filtername);\n\n      if (filter == null)\n      {\n         displayAvailableFilters(project);\n      }\n      else\n      {\n         System.out.println(filter);\n         System.out.println();\n\n         if (filter.isTaskFilter())\n         {\n            processTaskFilter(project, filter);\n         }\n         else\n         {\n            processResourceFilter(project, filter);\n         }\n      }\n   }",
    "reference": "This method opens the named project, applies the named filter\nand displays the filtered list of tasks or resources. If an\ninvalid filter name is supplied, a list of valid filter names\nis shown.\n\n@param filename input file name\n@param filtername input filter name",
    "generated": "Search for a filter.\n\n@param filename the file to search for.\n@param filtername the filter name\n@return null"
  },
  {
    "code": "private static void displayAvailableFilters(ProjectFile project)\n   {\n      System.out.println(\"Unknown filter name supplied.\");\n      System.out.println(\"Available task filters:\");\n      for (Filter filter : project.getFilters().getTaskFilters())\n      {\n         System.out.println(\"   \" + filter.getName());\n      }\n\n      System.out.println(\"Available resource filters:\");\n      for (Filter filter : project.getFilters().getResourceFilters())\n      {\n         System.out.println(\"   \" + filter.getName());\n      }\n\n   }",
    "reference": "This utility displays a list of available task filters, and a\nlist of available resource filters.\n\n@param project project file",
    "generated": "Display the available filters."
  },
  {
    "code": "private static void processTaskFilter(ProjectFile project, Filter filter)\n   {\n      for (Task task : project.getTasks())\n      {\n         if (filter.evaluate(task, null))\n         {\n            System.out.println(task.getID() + \",\" + task.getUniqueID() + \",\" + task.getName());\n         }\n      }\n   }",
    "reference": "Apply a filter to the list of all tasks, and show the results.\n\n@param project project file\n@param filter filter",
    "generated": "Process task filter.\n\n@param project project file\n@param filter filter"
  },
  {
    "code": "private static void processResourceFilter(ProjectFile project, Filter filter)\n   {\n      for (Resource resource : project.getResources())\n      {\n         if (filter.evaluate(resource, null))\n         {\n            System.out.println(resource.getID() + \",\" + resource.getUniqueID() + \",\" + resource.getName());\n         }\n      }\n   }",
    "reference": "Apply a filter to the list of all resources, and show the results.\n\n@param project project file\n@param filter filter",
    "generated": "Process the resource filter.\n\n@param project project file"
  },
  {
    "code": "public void process(String inputFile, String outputFile) throws Exception\n   {\n      System.out.println(\"Reading input file started.\");\n      long start = System.currentTimeMillis();\n      ProjectFile projectFile = readFile(inputFile);\n      long elapsed = System.currentTimeMillis() - start;\n      System.out.println(\"Reading input file completed in \" + elapsed + \"ms.\");\n\n      System.out.println(\"Writing output file started.\");\n      start = System.currentTimeMillis();\n      ProjectWriter writer = ProjectWriterUtility.getProjectWriter(outputFile);\n      writer.write(projectFile, outputFile);\n      elapsed = System.currentTimeMillis() - start;\n      System.out.println(\"Writing output completed in \" + elapsed + \"ms.\");\n   }",
    "reference": "Convert one project file format to another.\n\n@param inputFile input file\n@param outputFile output file\n@throws Exception",
    "generated": "Reads input file and output file.\n\n@param inputFile input file\n@param outputFile output file"
  },
  {
    "code": "private ProjectFile readFile(String inputFile) throws MPXJException\n   {\n      ProjectReader reader = new UniversalProjectReader();\n      ProjectFile projectFile = reader.read(inputFile);\n      if (projectFile == null)\n      {\n         throw new IllegalArgumentException(\"Unsupported file type\");\n      }\n      return projectFile;\n   }",
    "reference": "Use the universal project reader to open the file.\nThrow an exception if we can't determine the file type.\n\n@param inputFile file name\n@return ProjectFile instance",
    "generated": "Read a project file.\n\n@param inputFile file file\n@return project file"
  },
  {
    "code": "private void readProjectProperties(Settings phoenixSettings, Storepoint storepoint)\n   {\n      ProjectProperties mpxjProperties = m_projectFile.getProjectProperties();\n      mpxjProperties.setName(phoenixSettings.getTitle());\n      mpxjProperties.setDefaultDurationUnits(phoenixSettings.getBaseunit());\n      mpxjProperties.setStatusDate(storepoint.getDataDate());\n   }",
    "reference": "This method extracts project properties from a Phoenix file.\n\n@param phoenixSettings Phoenix settings\n@param storepoint Current storepoint",
    "generated": "Read the project properties.\n\n@param phoenixSettings the phoenix settings\n@param storepoint the storepoint\n@return the project properties"
  },
  {
    "code": "private void readCalendars(Storepoint phoenixProject)\n   {\n      Calendars calendars = phoenixProject.getCalendars();\n      if (calendars != null)\n      {\n         for (Calendar calendar : calendars.getCalendar())\n         {\n            readCalendar(calendar);\n         }\n\n         ProjectCalendar defaultCalendar = m_projectFile.getCalendarByName(phoenixProject.getDefaultCalendar());\n         if (defaultCalendar != null)\n         {\n            m_projectFile.getProjectProperties().setDefaultCalendarName(defaultCalendar.getName());\n         }\n      }\n   }",
    "reference": "This method extracts calendar data from a Phoenix file.\n\n@param phoenixProject Root node of the Phoenix file",
    "generated": "Reads the calendars from the phoenix project.\n\n@param phoenix project"
  },
  {
    "code": "private void readCalendar(Calendar calendar)\n   {\n      // Create the calendar\n      ProjectCalendar mpxjCalendar = m_projectFile.addCalendar();\n      mpxjCalendar.setName(calendar.getName());\n\n      // Default all days to working\n      for (Day day : Day.values())\n      {\n         mpxjCalendar.setWorkingDay(day, true);\n      }\n\n      // Mark non-working days\n      List<NonWork> nonWorkingDays = calendar.getNonWork();\n      for (NonWork nonWorkingDay : nonWorkingDays)\n      {\n         // TODO: handle recurring exceptions\n         if (nonWorkingDay.getType().equals(\"internal_weekly\"))\n         {\n            mpxjCalendar.setWorkingDay(nonWorkingDay.getWeekday(), false);\n         }\n      }\n\n      // Add default working hours for working days\n      for (Day day : Day.values())\n      {\n         if (mpxjCalendar.isWorkingDay(day))\n         {\n            ProjectCalendarHours hours = mpxjCalendar.addCalendarHours(day);\n            hours.addRange(ProjectCalendarWeek.DEFAULT_WORKING_MORNING);\n            hours.addRange(ProjectCalendarWeek.DEFAULT_WORKING_AFTERNOON);\n         }\n      }\n   }",
    "reference": "This method extracts data for a single calendar from a Phoenix file.\n\n@param calendar calendar data",
    "generated": "Read the calendar\n@param calendar calendar\n@return calendar"
  },
  {
    "code": "private void readResources(Storepoint phoenixProject)\n   {\n      Resources resources = phoenixProject.getResources();\n      if (resources != null)\n      {\n         for (net.sf.mpxj.phoenix.schema.Project.Storepoints.Storepoint.Resources.Resource res : resources.getResource())\n         {\n            Resource resource = readResource(res);\n            readAssignments(resource, res);\n         }\n      }\n   }",
    "reference": "This method extracts resource data from a Phoenix file.\n\n@param phoenixProject parent node for resources",
    "generated": "Reads the resources from the phoenix project.\n\n@param phoenix project"
  },
  {
    "code": "private Resource readResource(net.sf.mpxj.phoenix.schema.Project.Storepoints.Storepoint.Resources.Resource phoenixResource)\n   {\n      Resource mpxjResource = m_projectFile.addResource();\n\n      TimeUnit rateUnits = phoenixResource.getMonetarybase();\n      if (rateUnits == null)\n      {\n         rateUnits = TimeUnit.HOURS;\n      }\n\n      // phoenixResource.getMaximum()\n      mpxjResource.setCostPerUse(phoenixResource.getMonetarycostperuse());\n      mpxjResource.setStandardRate(new Rate(phoenixResource.getMonetaryrate(), rateUnits));\n      mpxjResource.setStandardRateUnits(rateUnits);\n      mpxjResource.setName(phoenixResource.getName());\n      mpxjResource.setType(phoenixResource.getType());\n      mpxjResource.setMaterialLabel(phoenixResource.getUnitslabel());\n      //phoenixResource.getUnitsperbase()\n      mpxjResource.setGUID(phoenixResource.getUuid());\n\n      m_eventManager.fireResourceReadEvent(mpxjResource);\n\n      return mpxjResource;\n   }",
    "reference": "This method extracts data for a single resource from a Phoenix file.\n\n@param phoenixResource resource data\n@return Resource instance",
    "generated": "Add a resource to the project file.\n\n@param phoenixResource resource"
  },
  {
    "code": "private void readTasks(Project phoenixProject, Storepoint storepoint)\n   {\n      processLayouts(phoenixProject);\n      processActivityCodes(storepoint);\n      processActivities(storepoint);\n      updateDates();\n   }",
    "reference": "Read phases and activities from the Phoenix file to create the task hierarchy.\n\n@param phoenixProject all project data\n@param storepoint storepoint containing current project data",
    "generated": "Read tasks from the phoenix project.\n\n@param phoenixProject phoenix project\n@param storepoint storepoint"
  },
  {
    "code": "private void processActivityCodes(Storepoint storepoint)\n   {\n      for (Code code : storepoint.getActivityCodes().getCode())\n      {\n         int sequence = 0;\n         for (Value value : code.getValue())\n         {\n            UUID uuid = getUUID(value.getUuid(), value.getName());\n            m_activityCodeValues.put(uuid, value.getName());\n            m_activityCodeSequence.put(uuid, Integer.valueOf(++sequence));\n         }\n      }\n   }",
    "reference": "Map from an activity code value UUID to the actual value itself, and its\nsequence number.\n\n@param storepoint storepoint containing current project data",
    "generated": "Process the activity codes.\n\n@param storepoint the storepoint\n@param storepoint the storepoint\n@param code the storepoint"
  },
  {
    "code": "private void processLayouts(Project phoenixProject)\n   {\n      //\n      // Find the active layout\n      //\n      Layout activeLayout = getActiveLayout(phoenixProject);\n\n      //\n      // Create a list of the visible codes in the correct order\n      //\n      for (CodeOption option : activeLayout.getCodeOptions().getCodeOption())\n      {\n         if (option.isShown().booleanValue())\n         {\n            m_codeSequence.add(getUUID(option.getCodeUuid(), option.getCode()));\n         }\n      }\n   }",
    "reference": "Find the current layout and extract the activity code order and visibility.\n\n@param phoenixProject phoenix project data",
    "generated": "Find the active layout\n@param phoenix project\n@param phoenixProject"
  },
  {
    "code": "private Layout getActiveLayout(Project phoenixProject)\n   {\n      //\n      // Start with the first layout we find\n      //\n      Layout activeLayout = phoenixProject.getLayouts().getLayout().get(0);\n\n      //\n      // If this isn't active, find one which is... and if none are,\n      // we'll just use the first.\n      //\n      if (!activeLayout.isActive().booleanValue())\n      {\n         for (Layout layout : phoenixProject.getLayouts().getLayout())\n         {\n            if (layout.isActive().booleanValue())\n            {\n               activeLayout = layout;\n               break;\n            }\n         }\n      }\n\n      return activeLayout;\n   }",
    "reference": "Find the current active layout.\n\n@param phoenixProject phoenix project data\n@return current active layout",
    "generated": "Find the first layout we find\n\n@param phoenixProject"
  },
  {
    "code": "private void processActivities(Storepoint phoenixProject)\n   {\n      final AlphanumComparator comparator = new AlphanumComparator();\n      List<Activity> activities = phoenixProject.getActivities().getActivity();\n      Collections.sort(activities, new Comparator<Activity>()\n      {\n         @Override public int compare(Activity o1, Activity o2)\n         {\n            Map<UUID, UUID> codes1 = getActivityCodes(o1);\n            Map<UUID, UUID> codes2 = getActivityCodes(o2);\n            for (UUID code : m_codeSequence)\n            {\n               UUID codeValue1 = codes1.get(code);\n               UUID codeValue2 = codes2.get(code);\n\n               if (codeValue1 == null || codeValue2 == null)\n               {\n                  if (codeValue1 == null && codeValue2 == null)\n                  {\n                     continue;\n                  }\n\n                  if (codeValue1 == null)\n                  {\n                     return -1;\n                  }\n\n                  if (codeValue2 == null)\n                  {\n                     return 1;\n                  }\n               }\n\n               if (!codeValue1.equals(codeValue2))\n               {\n                  Integer sequence1 = m_activityCodeSequence.get(codeValue1);\n                  Integer sequence2 = m_activityCodeSequence.get(codeValue2);\n\n                  return NumberHelper.compare(sequence1, sequence2);\n               }\n            }\n\n            return comparator.compare(o1.getId(), o2.getId());\n         }\n      });\n\n      for (Activity activity : activities)\n      {\n         processActivity(activity);\n      }\n   }",
    "reference": "Process the set of activities from the Phoenix file.\n\n@param phoenixProject project data",
    "generated": "Process all activities.\n\n@param phoenixProject the phoenix project\n@param phoenixProject the phoenix project\n@param phoenixProject the phoenix project"
  },
  {
    "code": "private void processActivity(Activity activity)\n   {\n      Task task = getParentTask(activity).addTask();\n      task.setText(1, activity.getId());\n\n      task.setActualDuration(activity.getActualDuration());\n      task.setActualFinish(activity.getActualFinish());\n      task.setActualStart(activity.getActualStart());\n      //activity.getBaseunit()\n      //activity.getBilled()\n      //activity.getCalendar()\n      //activity.getCostAccount()\n      task.setCreateDate(activity.getCreationTime());\n      task.setFinish(activity.getCurrentFinish());\n      task.setStart(activity.getCurrentStart());\n      task.setName(activity.getDescription());\n      task.setDuration(activity.getDurationAtCompletion());\n      task.setEarlyFinish(activity.getEarlyFinish());\n      task.setEarlyStart(activity.getEarlyStart());\n      task.setFreeSlack(activity.getFreeFloat());\n      task.setLateFinish(activity.getLateFinish());\n      task.setLateStart(activity.getLateStart());\n      task.setNotes(activity.getNotes());\n      task.setBaselineDuration(activity.getOriginalDuration());\n      //activity.getPathFloat()\n      task.setPhysicalPercentComplete(activity.getPhysicalPercentComplete());\n      task.setRemainingDuration(activity.getRemainingDuration());\n      task.setCost(activity.getTotalCost());\n      task.setTotalSlack(activity.getTotalFloat());\n      task.setMilestone(activityIsMilestone(activity));\n      //activity.getUserDefined()\n      task.setGUID(activity.getUuid());\n\n      if (task.getMilestone())\n      {\n         if (activityIsStartMilestone(activity))\n         {\n            task.setFinish(task.getStart());\n         }\n         else\n         {\n            task.setStart(task.getFinish());\n         }\n      }\n\n      if (task.getActualStart() == null)\n      {\n         task.setPercentageComplete(Integer.valueOf(0));\n      }\n      else\n      {\n         if (task.getActualFinish() != null)\n         {\n            task.setPercentageComplete(Integer.valueOf(100));\n         }\n         else\n         {\n            Duration remaining = activity.getRemainingDuration();\n            Duration total = activity.getDurationAtCompletion();\n            if (remaining != null && total != null && total.getDuration() != 0)\n            {\n               double percentComplete = ((total.getDuration() - remaining.getDuration()) * 100.0) / total.getDuration();\n               task.setPercentageComplete(Double.valueOf(percentComplete));\n            }\n         }\n      }\n\n      m_activityMap.put(activity.getId(), task);\n   }",
    "reference": "Create a Task instance from a Phoenix activity.\n\n@param activity Phoenix activity data",
    "generated": "Process a Activity.\n\n@param activity Activity object\n@return Activity object"
  },
  {
    "code": "private boolean activityIsMilestone(Activity activity)\n   {\n      String type = activity.getType();\n      return type != null && type.indexOf(\"Milestone\") != -1;\n   }",
    "reference": "Returns true if the activity is a milestone.\n\n@param activity Phoenix activity\n@return true if the activity is a milestone",
    "generated": "Check if the activity is a Milestone"
  },
  {
    "code": "private boolean activityIsStartMilestone(Activity activity)\n   {\n      String type = activity.getType();\n      return type != null && type.indexOf(\"StartMilestone\") != -1;\n   }",
    "reference": "Returns true if the activity is a start milestone.\n\n@param activity Phoenix activity\n@return true if the activity is a milestone",
    "generated": "Check if the activity is a start Milestone."
  },
  {
    "code": "private ChildTaskContainer getParentTask(Activity activity)\n   {\n      //\n      // Make a map of activity codes and their values for this activity\n      //\n      Map<UUID, UUID> map = getActivityCodes(activity);\n\n      //\n      // Work through the activity codes in sequence\n      //\n      ChildTaskContainer parent = m_projectFile;\n      StringBuilder uniqueIdentifier = new StringBuilder();\n      for (UUID activityCode : m_codeSequence)\n      {\n         UUID activityCodeValue = map.get(activityCode);\n         String activityCodeText = m_activityCodeValues.get(activityCodeValue);\n         if (activityCodeText != null)\n         {\n            if (uniqueIdentifier.length() != 0)\n            {\n               uniqueIdentifier.append('>');\n            }\n            uniqueIdentifier.append(activityCodeValue.toString());\n            UUID uuid = UUID.nameUUIDFromBytes(uniqueIdentifier.toString().getBytes());\n            Task newParent = findChildTaskByUUID(parent, uuid);\n            if (newParent == null)\n            {\n               newParent = parent.addTask();\n               newParent.setGUID(uuid);\n               newParent.setName(activityCodeText);\n            }\n            parent = newParent;\n         }\n      }\n      return parent;\n   }",
    "reference": "Retrieves the parent task for a Phoenix activity.\n\n@param activity Phoenix activity\n@return parent task",
    "generated": "Make a map of activity codes and their values for this activity\n\n@param activity Activity\n@return the parent task"
  },
  {
    "code": "private Task findChildTaskByUUID(ChildTaskContainer parent, UUID uuid)\n   {\n      Task result = null;\n\n      for (Task task : parent.getChildTasks())\n      {\n         if (uuid.equals(task.getGUID()))\n         {\n            result = task;\n            break;\n         }\n      }\n\n      return result;\n   }",
    "reference": "Locates a task within a child task container which matches the supplied UUID.\n\n@param parent child task container\n@param uuid required UUID\n@return Task instance or null if the task is not found",
    "generated": "Find a child task by UUID.\n\n@param parent child task container\n@return child task"
  },
  {
    "code": "private void readAssignments(Resource mpxjResource, net.sf.mpxj.phoenix.schema.Project.Storepoints.Storepoint.Resources.Resource res)\n   {\n      for (Assignment assignment : res.getAssignment())\n      {\n         readAssignment(mpxjResource, assignment);\n      }\n   }",
    "reference": "Reads Phoenix resource assignments.\n\n@param mpxjResource MPXJ resource\n@param res Phoenix resource",
    "generated": "Read the assignments from the given resource.\n\n@param mpxjResource the resource\n@param res the resource to read"
  },
  {
    "code": "private void readAssignment(Resource resource, Assignment assignment)\n   {\n      Task task = m_activityMap.get(assignment.getActivity());\n      if (task != null)\n      {\n         task.addResourceAssignment(resource);\n      }\n   }",
    "reference": "Read a single resource assignment.\n\n@param resource MPXJ resource\n@param assignment Phoenix assignment",
    "generated": "Read a resource assignment.\n\n@param resource resource"
  },
  {
    "code": "private void readRelationships(Storepoint phoenixProject)\n   {\n      for (Relationship relation : phoenixProject.getRelationships().getRelationship())\n      {\n         readRelation(relation);\n      }\n   }",
    "reference": "Read task relationships from a Phoenix file.\n\n@param phoenixProject Phoenix project data",
    "generated": "Read the relationships of the phoenix project.\n\n@param phoenixProject the phoenix project\n@return phoenix project"
  },
  {
    "code": "private void readRelation(Relationship relation)\n   {\n      Task predecessor = m_activityMap.get(relation.getPredecessor());\n      Task successor = m_activityMap.get(relation.getSuccessor());\n      if (predecessor != null && successor != null)\n      {\n         Duration lag = relation.getLag();\n         RelationType type = relation.getType();\n         successor.addPredecessor(predecessor, type, lag);\n      }\n   }",
    "reference": "Read an individual Phoenix task relationship.\n\n@param relation Phoenix task relationship",
    "generated": "Read a relationship from the activity map.\n\n@param relation relation"
  },
  {
    "code": "Map<UUID, UUID> getActivityCodes(Activity activity)\n   {\n      Map<UUID, UUID> map = m_activityCodeCache.get(activity);\n      if (map == null)\n      {\n         map = new HashMap<UUID, UUID>();\n         m_activityCodeCache.put(activity, map);\n         for (CodeAssignment ca : activity.getCodeAssignment())\n         {\n            UUID code = getUUID(ca.getCodeUuid(), ca.getCode());\n            UUID value = getUUID(ca.getValueUuid(), ca.getValue());\n            map.put(code, value);\n         }\n      }\n      return map;\n   }",
    "reference": "For a given activity, retrieve a map of the activity code values which have been assigned to it.\n\n@param activity target activity\n@return map of activity code value UUIDs",
    "generated": "Get the activity codes."
  },
  {
    "code": "private Storepoint getCurrentStorepoint(Project phoenixProject)\n   {\n      List<Storepoint> storepoints = phoenixProject.getStorepoints().getStorepoint();\n      Collections.sort(storepoints, new Comparator<Storepoint>()\n      {\n         @Override public int compare(Storepoint o1, Storepoint o2)\n         {\n            return DateHelper.compare(o2.getCreationTime(), o1.getCreationTime());\n         }\n      });\n      return storepoints.get(0);\n   }",
    "reference": "Retrieve the most recent storepoint.\n\n@param phoenixProject project data\n@return Storepoint instance",
    "generated": "Get the current storepoint.\n\n@param phoenixProject the phoenix project\n@return the current storepoint"
  },
  {
    "code": "public TableReader read() throws IOException\n   {\n      int tableHeader = m_stream.readInt();\n      if (tableHeader != 0x39AF547A)\n      {\n         throw new IllegalArgumentException(\"Unexpected file format\");\n      }\n\n      int recordCount = m_stream.readInt();\n      for (int loop = 0; loop < recordCount; loop++)\n      {\n         int rowMagicNumber = m_stream.readInt();\n         if (rowMagicNumber != rowMagicNumber())\n         {\n            throw new IllegalArgumentException(\"Unexpected file format\");\n         }\n\n         // We use a LinkedHashMap to preserve insertion order in iteration\n         // Useful when debugging the file format.\n         Map<String, Object> map = new LinkedHashMap<String, Object>();\n\n         if (hasUUID())\n         {\n            readUUID(m_stream, map);\n         }\n\n         readRow(m_stream, map);\n\n         SynchroLogger.log(\"READER\", getClass(), map);\n\n         m_rows.add(new MapRow(map));\n      }\n\n      int tableTrailer = m_stream.readInt();\n      if (tableTrailer != 0x6F99E416)\n      {\n         throw new IllegalArgumentException(\"Unexpected file format\");\n      }\n\n      postTrailer(m_stream);\n\n      return this;\n   }",
    "reference": "Read data from the table. Return a reference to the current\ninstance to allow method chaining.\n\n@return reader instance",
    "generated": "Read a table from the stream.\n\n@param m_stream stream\n@return TableReader instance"
  },
  {
    "code": "protected void readUUID(StreamReader stream, Map<String, Object> map) throws IOException\n   {\n      int unknown0Size = stream.getMajorVersion() > 5 ? 8 : 16;\n      map.put(\"UNKNOWN0\", stream.readBytes(unknown0Size));\n      map.put(\"UUID\", stream.readUUID());   \n   }",
    "reference": "Read the optional row header and UUID.\n\n@param stream input stream\n@param map row map",
    "generated": "Read a UUID from the stream.\n\n@param stream the stream to read\n@param map the map to read\n@return"
  }
]