{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# CodeWhisper Training Pipeline (Kaggle)\n",
                "\n",
                "This notebook is designed to fine-tune the CodeT5 model on CodeSearchNet/CodeXGLUE datasets using a T4 GPU.\n",
                "It is self-contained and does not require cloning the repository."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Setup Environment\n",
                "!nvidia-smi\n",
                "\n",
                "# Install dependencies\n",
                "!pip install -q transformers datasets accelerate peft bitsandbytes radon lizard protobuf==3.20.3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Prepare Datasets\n",
                "import os\n",
                "import json\n",
                "from datasets import load_dataset\n",
                "from tqdm import tqdm\n",
                "\n",
                "def prepare_dataset(output_dir, languages=['python', 'java'], split='train', limit=None):\n",
                "    print(f\"Processing google/code_x_glue_ct_code_to_text for {languages}...\")\n",
                "    \n",
                "    data = []\n",
                "    \n",
                "    for lang in languages:\n",
                "        print(f\"Loading {lang}...\")\n",
                "        try:\n",
                "            # Using 'google/code_x_glue_ct_code_to_text' as requested\n",
                "            # Disable streaming to avoid 429 Too Many Requests\n",
                "            ds = load_dataset(\"google/code_x_glue_ct_code_to_text\", lang, split=split, trust_remote_code=True)\n",
                "        except Exception as e:\n",
                "            print(f\"Error loading {lang}: {e}\")\n",
                "            continue\n",
                "\n",
                "        count = 0\n",
                "        for item in tqdm(ds):\n",
                "            code = item.get('code') or item.get('func_code_string') or ''\n",
                "            doc = item.get('docstring') or item.get('func_documentation_string') or ''\n",
                "            \n",
                "            if code and doc:\n",
                "                entry = {\n",
                "                    \"code\": code,\n",
                "                    \"docstring\": doc,\n",
                "                    \"language\": lang,\n",
                "                    \"source\": \"google/code_x_glue_ct_code_to_text\"\n",
                "                }\n",
                "                data.append(entry)\n",
                "                count += 1\n",
                "                \n",
                "            if limit and count >= limit:\n",
                "                break\n",
                "                \n",
                "    output_file = os.path.join(output_dir, f\"training_data_{split}.jsonl\")\n",
                "    print(f\"Saving {len(data)} records to {output_file}...\")\n",
                "    \n",
                "    os.makedirs(output_dir, exist_ok=True)\n",
                "    with open(output_file, 'w', encoding='utf-8') as f:\n",
                "        for entry in data:\n",
                "            f.write(json.dumps(entry) + '\\n')\n",
                "\n",
                "# Run preparation\n",
                "prepare_dataset(\"processed_data\", limit=10000) # Limit for demo, remove for full run"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Training Logic\n",
                "import torch\n",
                "from transformers import (\n",
                "    AutoModelForSeq2SeqLM,\n",
                "    AutoTokenizer,\n",
                "    Seq2SeqTrainingArguments,\n",
                "    Seq2SeqTrainer,\n",
                "    DataCollatorForSeq2Seq,\n",
                "    BitsAndBytesConfig\n",
                ")\n",
                "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
                "\n",
                "def train(\n",
                "    train_file: str,\n",
                "    output_dir: str,\n",
                "    model_name: str = \"Salesforce/codet5-small\",\n",
                "    batch_size: int = 4,\n",
                "    epochs: int = 3,\n",
                "    learning_rate: float = 2e-5\n",
                "):\n",
                "    print(f\"Loading model: {model_name}\")\n",
                "    \n",
                "    use_cuda = torch.cuda.is_available()\n",
                "    device = \"cuda\" if use_cuda else \"cpu\"\n",
                "    print(f\"Using device: {device}\")\n",
                "\n",
                "    # QLoRA Configuration (Only if CUDA is available)\n",
                "    if use_cuda:\n",
                "        print(\"CUDA detected. Using QLoRA with 4-bit quantization.\")\n",
                "        bnb_config = BitsAndBytesConfig(\n",
                "            load_in_4bit=True,\n",
                "            bnb_4bit_use_double_quant=True,\n",
                "            bnb_4bit_quant_type=\"nf4\",\n",
                "            bnb_4bit_compute_dtype=torch.float16\n",
                "        )\n",
                "        \n",
                "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
                "            model_name,\n",
                "            quantization_config=bnb_config,\n",
                "            device_map=\"auto\",\n",
                "            trust_remote_code=True\n",
                "        )\n",
                "        model.gradient_checkpointing_enable()\n",
                "        model = prepare_model_for_kbit_training(model)\n",
                "    else:\n",
                "        print(\"CUDA NOT detected. Using standard CPU training (No Quantization).\")\n",
                "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
                "            model_name,\n",
                "            trust_remote_code=True\n",
                "        ).to(device)\n",
                "\n",
                "    # Load Tokenizer\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "\n",
                "    # LoRA Configuration\n",
                "    peft_config = LoraConfig(\n",
                "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
                "        inference_mode=False,\n",
                "        r=8,\n",
                "        lora_alpha=32,\n",
                "        lora_dropout=0.1,\n",
                "        target_modules=[\"q\", \"v\"] # Target attention layers\n",
                "    )\n",
                "\n",
                "    model = get_peft_model(model, peft_config)\n",
                "    model.print_trainable_parameters()\n",
                "\n",
                "    # Load Dataset\n",
                "    data_files = {\"train\": train_file}\n",
                "    dataset = load_dataset(\"json\", data_files=data_files)\n",
                "\n",
                "    def preprocess_function(examples):\n",
                "        inputs = [\n",
                "            f\"Generate a documentation string for this function:\\n{lang}: {code}\" \n",
                "            for lang, code in zip(examples[\"language\"], examples[\"code\"])\n",
                "        ]\n",
                "        targets = examples[\"docstring\"]\n",
                "        \n",
                "        model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
                "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
                "        \n",
                "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
                "        return model_inputs\n",
                "\n",
                "    tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
                "\n",
                "    # Training Arguments\n",
                "    training_args = Seq2SeqTrainingArguments(\n",
                "        output_dir=output_dir,\n",
                "        per_device_train_batch_size=batch_size,\n",
                "        gradient_accumulation_steps=4, \n",
                "        learning_rate=learning_rate,\n",
                "        num_train_epochs=epochs,\n",
                "        logging_steps=10,\n",
                "        save_strategy=\"epoch\",\n",
                "        eval_strategy=\"no\", \n",
                "        fp16=use_cuda, \n",
                "        use_cpu=not use_cuda, \n",
                "        optim=\"paged_adamw_8bit\" if use_cuda else \"adamw_torch\", \n",
                "        ddp_find_unused_parameters=False if (use_cuda and torch.cuda.device_count() > 1) else None,\n",
                "        report_to=\"none\"\n",
                "    )\n",
                "\n",
                "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
                "\n",
                "    trainer = Seq2SeqTrainer(\n",
                "        model=model,\n",
                "        args=training_args,\n",
                "        train_dataset=tokenized_dataset[\"train\"],\n",
                "        tokenizer=tokenizer,\n",
                "        data_collator=data_collator,\n",
                "    )\n",
                "\n",
                "    print(\"Starting training...\")\n",
                "    trainer.train()\n",
                "    \n",
                "    print(f\"Saving model to {output_dir}\")\n",
                "    trainer.save_model(output_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Execute Training\n",
                "train(\n",
                "    train_file=\"processed_data/training_data_train.jsonl\",\n",
                "    output_dir=\"results/codet5-finetuned\",\n",
                "    batch_size=4,\n",
                "    epochs=1,\n",
                "    learning_rate=2e-5\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Save/Download Model\n",
                "!zip -r model_output.zip results/codet5-finetuned"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}